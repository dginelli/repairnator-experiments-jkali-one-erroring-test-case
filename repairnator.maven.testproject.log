SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
com.uber.hoodie.exception.CorruptedLogFileException: Found possible corrupted block, cannot read log file in reverse, fallback to forward reading of logfile
	at com.uber.hoodie.common.table.log.HoodieLogFileReader.prev(HoodieLogFileReader.java:375)
	at com.uber.hoodie.common.table.log.HoodieLogFormatTest.testAppendAndReadOnCorruptedLogInReverse(HoodieLogFormatTest.java:1313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
com.uber.hoodie.exception.CorruptedLogFileException: Found possible corrupted block, cannot read log file in reverse, fallback to forward reading of logfile
	at com.uber.hoodie.common.table.log.HoodieLogFileReader.prev(HoodieLogFileReader.java:375)
	at com.uber.hoodie.common.table.log.HoodieLogFormatTest.testAppendAndReadOnCorruptedLogInReverse(HoodieLogFormatTest.java:1313)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-exec/1.1.0-cdh5.7.2/hive-exec-1.1.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-hadoop-bundle/1.5.0-cdh5.7.2/parquet-hadoop-bundle-1.5.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-format/2.1.0-cdh5.7.2/parquet-format-2.1.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

[Stage 3:=============================================================>                                                                                                                         (1 + 1) / 3]
[Stage 3:==========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 4:>                                                                                                                                                                                      (0 + 0) / 3]
[Stage 6:>                                                                                                                                                                                      (0 + 0) / 3]
                                                                                                                                                                                                            

[Stage 3:==========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 20:============================================================>                                                                                                                         (1 + 1) / 3]
[Stage 20:=========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 61:=========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 20:=========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 44:>                                                                                                                                                                                     (0 + 0) / 2]
                                                                                                                                                                                                            

[Stage 50:=========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-hadoop-bundle/1.5.0-cdh5.7.2/parquet-hadoop-bundle-1.5.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-format/2.1.0-cdh5.7.2/parquet-format-2.1.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-exec/1.1.0-cdh5.7.2/hive-exec-1.1.0-cdh5.7.2.jar!/shaded/parquet/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [shaded.parquet.org.slf4j.helpers.NOPLoggerFactory]

[Stage 68:=========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 11:=====================>                                                                                                                                                               (9 + 1) / 76]
[Stage 11:==============================>                                                                                                                                                     (13 + 1) / 76]
[Stage 11:========================================>                                                                                                                                           (17 + 1) / 76]
[Stage 11:===============================================>                                                                                                                                    (20 + 1) / 76]
[Stage 11:========================================================>                                                                                                                           (24 + 1) / 76]
[Stage 11:===============================================================>                                                                                                                    (27 + 1) / 76]
[Stage 11:=======================================================================>                                                                                                            (30 + 1) / 76]
[Stage 11:==============================================================================>                                                                                                     (33 + 1) / 76]
[Stage 11:=======================================================================================>                                                                                            (37 + 1) / 76]
[Stage 11:============================================================================================>                                                                                       (39 + 1) / 76]
[Stage 11:===================================================================================================>                                                                                (42 + 1) / 76]
[Stage 11:============================================================================================================>                                                                       (46 + 1) / 76]
[Stage 11:======================================================================================================================>                                                             (50 + 1) / 76]
[Stage 11:===============================================================================================================================>                                                    (54 + 1) / 76]
[Stage 11:==================================================================================================================================>                                                 (55 + 1) / 76]
[Stage 11:=======================================================================================================================================>                                            (57 + 1) / 76]
[Stage 11:===========================================================================================================================================>                                        (59 + 1) / 76]
[Stage 11:==================================================================================================================================================>                                 (62 + 1) / 76]
[Stage 11:============================================================================================================================================================>                       (66 + 1) / 76]
[Stage 11:=================================================================================================================================================================>                  (68 + 1) / 76]
[Stage 11:==========================================================================================================================================================================>         (72 + 1) / 76]
[Stage 11:=================================================================================================================================================================================>  (75 + 1) / 76]
                                                                                                                                                                                                            

[Stage 16:========================================================================================================================>                                                           (51 + 1) / 76]
[Stage 16:=================================================================================================================================================================>                  (68 + 1) / 76]
                                                                                                                                                                                                            

[Stage 20:=======================================================>                                                                                                                            (23 + 1) / 75]
[Stage 20:===========================================================================================>                                                                                        (38 + 1) / 75]
[Stage 20:===============================================================================================================================>                                                    (53 + 1) / 75]
[Stage 20:===================================================================================================================================================================>                (68 + 1) / 75]
                                                                                                                                                                                                            

[Stage 2:=========>                                                                                                                                                                          (11 + 1) / 200]
[Stage 2:============>                                                                                                                                                                       (14 + 1) / 200]
[Stage 2:=================>                                                                                                                                                                  (19 + 1) / 200]
[Stage 2:====================>                                                                                                                                                               (23 + 1) / 200]
[Stage 2:========================>                                                                                                                                                           (27 + 1) / 200]
[Stage 2:===========================>                                                                                                                                                        (31 + 1) / 200]
[Stage 2:===============================>                                                                                                                                                    (35 + 1) / 200]
[Stage 2:=================================>                                                                                                                                                  (37 + 1) / 200]
[Stage 2:====================================>                                                                                                                                               (41 + 1) / 200]
[Stage 2:=========================================>                                                                                                                                          (46 + 1) / 200]
[Stage 2:=============================================>                                                                                                                                      (50 + 1) / 200]
[Stage 2:===============================================>                                                                                                                                    (53 + 1) / 200]
[Stage 2:====================================================>                                                                                                                               (58 + 1) / 200]
[Stage 2:======================================================>                                                                                                                             (61 + 1) / 200]
[Stage 2:==========================================================>                                                                                                                         (65 + 1) / 200]
[Stage 2:===============================================================>                                                                                                                    (70 + 1) / 200]
[Stage 2:==================================================================>                                                                                                                 (74 + 1) / 200]
[Stage 2:======================================================================>                                                                                                             (78 + 1) / 200]
[Stage 2:==========================================================================>                                                                                                         (83 + 1) / 200]
[Stage 2:==============================================================================>                                                                                                     (87 + 1) / 200]
[Stage 2:=================================================================================>                                                                                                  (91 + 1) / 200]
[Stage 2:====================================================================================>                                                                                               (94 + 1) / 200]
[Stage 2:========================================================================================>                                                                                           (98 + 1) / 200]
[Stage 2:============================================================================================>                                                                                      (103 + 1) / 200]
[Stage 2:===============================================================================================>                                                                                   (107 + 1) / 200]
[Stage 2:==================================================================================================>                                                                                (110 + 1) / 200]
[Stage 2:====================================================================================================>                                                                              (112 + 1) / 200]
[Stage 2:======================================================================================================>                                                                            (114 + 1) / 200]
[Stage 2:=======================================================================================================>                                                                           (116 + 1) / 200]
[Stage 2:=========================================================================================================>                                                                         (118 + 1) / 200]
[Stage 2:===========================================================================================================>                                                                       (120 + 1) / 200]
[Stage 2:============================================================================================================>                                                                      (121 + 1) / 200]
[Stage 2:==============================================================================================================>                                                                    (123 + 1) / 200]
[Stage 2:================================================================================================================>                                                                  (126 + 1) / 200]
[Stage 2:==================================================================================================================>                                                                (128 + 1) / 200]
[Stage 2:====================================================================================================================>                                                              (130 + 1) / 200]
[Stage 2:======================================================================================================================>                                                            (132 + 1) / 200]
[Stage 2:=======================================================================================================================>                                                           (134 + 1) / 200]
[Stage 2:=========================================================================================================================>                                                         (136 + 1) / 200]
[Stage 2:===========================================================================================================================>                                                       (138 + 1) / 200]
[Stage 2:=============================================================================================================================>                                                     (140 + 1) / 200]
[Stage 2:===============================================================================================================================>                                                   (142 + 1) / 200]
[Stage 2:================================================================================================================================>                                                  (144 + 1) / 200]
[Stage 2:==================================================================================================================================>                                                (146 + 1) / 200]
[Stage 2:======================================================================================================================================>                                            (150 + 1) / 200]
[Stage 2:=========================================================================================================================================>                                         (154 + 1) / 200]
[Stage 2:==========================================================================================================================================>                                        (155 + 1) / 200]
[Stage 2:============================================================================================================================================>                                      (157 + 1) / 200]
[Stage 2:================================================================================================================================================>                                  (161 + 1) / 200]
[Stage 2:==================================================================================================================================================>                                (164 + 1) / 200]
[Stage 2:====================================================================================================================================================>                              (166 + 1) / 200]
[Stage 2:======================================================================================================================================================>                            (168 + 1) / 200]
[Stage 2:=========================================================================================================================================================>                         (172 + 1) / 200]
[Stage 2:=============================================================================================================================================================>                     (176 + 1) / 200]
[Stage 2:=================================================================================================================================================================>                 (181 + 1) / 200]
[Stage 2:===================================================================================================================================================================>               (183 + 1) / 200]
[Stage 2:=======================================================================================================================================================================>           (187 + 1) / 200]
[Stage 2:==========================================================================================================================================================================>        (191 + 1) / 200]
[Stage 2:==============================================================================================================================================================================>    (195 + 1) / 200]
[Stage 2:=================================================================================================================================================================================> (198 + 1) / 200]
                                                                                                                                                                                                            

[Stage 10:====================>                                                                                                                                                              (23 + 1) / 200]
[Stage 10:=================================>                                                                                                                                                 (37 + 1) / 200]
[Stage 10:===============================================>                                                                                                                                   (53 + 1) / 200]
[Stage 10:============================================================>                                                                                                                      (68 + 1) / 200]
[Stage 10:==============================================================>                                                                                                                    (70 + 1) / 200]
[Stage 10:============================================================================>                                                                                                      (85 + 1) / 200]
[Stage 10:=========================================================================================>                                                                                        (100 + 1) / 200]
[Stage 10:========================================================================================================>                                                                         (117 + 1) / 200]
[Stage 10:============================================================================================================>                                                                     (122 + 1) / 200]
[Stage 10:========================================================================================================================>                                                         (135 + 1) / 200]
[Stage 10:=====================================================================================================================================>                                            (150 + 1) / 200]
[Stage 10:=================================================================================================================================================>                                (163 + 1) / 200]
[Stage 10:===========================================================================================================================================================>                      (175 + 1) / 200]
[Stage 10:============================================================================================================================================================================>     (194 + 1) / 200]
                                                                                                                                                                                                            

[Stage 17:>                                                                                                                                                                                     (0 + 1) / 2]
[Stage 17:===========================================================================================>                                                                                          (1 + 1) / 2]
                                                                                                                                                                                                            

[Stage 26:=================>                                                                                                                                                                   (8 + 1) / 81]
[Stage 26:========================>                                                                                                                                                           (11 + 1) / 81]
[Stage 26:=================================>                                                                                                                                                  (15 + 1) / 81]
[Stage 26:========================================>                                                                                                                                           (18 + 1) / 81]
[Stage 26:==============================================>                                                                                                                                     (21 + 1) / 81]
[Stage 26:=======================================================>                                                                                                                            (25 + 1) / 81]
[Stage 26:================================================================>                                                                                                                   (29 + 1) / 81]
[Stage 26:====================================================================>                                                                                                               (31 + 1) / 81]
[Stage 26:=============================================================================>                                                                                                      (35 + 1) / 81]
[Stage 26:====================================================================================>                                                                                               (38 + 1) / 81]
[Stage 26:=============================================================================================>                                                                                      (42 + 1) / 81]
[Stage 26:====================================================================================================>                                                                               (45 + 1) / 81]
[Stage 26:==========================================================================================================>                                                                         (48 + 1) / 81]
[Stage 26:===============================================================================================================>                                                                    (50 + 1) / 81]
[Stage 26:========================================================================================================================>                                                           (54 + 1) / 81]
[Stage 26:============================================================================================================================>                                                       (56 + 1) / 81]
[Stage 26:================================================================================================================================>                                                   (58 + 1) / 81]
[Stage 26:=====================================================================================================================================>                                              (60 + 1) / 81]
[Stage 26:==============================================================================================================================================>                                     (64 + 1) / 81]
[Stage 26:==================================================================================================================================================>                                 (66 + 1) / 81]
[Stage 26:=======================================================================================================================================================>                            (68 + 1) / 81]
[Stage 26:=============================================================================================================================================================>                      (71 + 1) / 81]
[Stage 26:==================================================================================================================================================================>                 (73 + 1) / 81]
[Stage 26:======================================================================================================================================================================>             (75 + 1) / 81]
[Stage 26:===========================================================================================================================================================================>        (77 + 1) / 81]
                                                                                                                                                                                                            

[Stage 4:============================================================================================>                                                                                      (103 + 1) / 200]
[Stage 4:====================================================================================================================>                                                              (130 + 1) / 200]
[Stage 4:===========================================================================================================================================>                                       (156 + 1) / 200]
[Stage 4:=====================================================================================================================================================================>             (185 + 1) / 200]
                                                                                                                                                                                                            

[Stage 5:================================================================================>                                                                                                   (89 + 1) / 200]
[Stage 5:========================================================================================================>                                                                          (117 + 1) / 200]
[Stage 5:=================================================================================================================================>                                                 (145 + 1) / 200]
[Stage 5:==========================================================================================================================================================>                        (173 + 1) / 200]
                                                                                                                                                                                                            

[Stage 44:====================================================================================>                                                                                              (94 + 1) / 200]
[Stage 44:=============================================================================================================>                                                                    (123 + 1) / 200]
[Stage 44:=====================================================================================================================================>                                            (150 + 1) / 200]
[Stage 44:=================================================================================================================================================================>                (181 + 1) / 200]
                                                                                                                                                                                                            

[Stage 45:=========================>                                                                                                                                                         (29 + 1) / 200]
[Stage 45:==================================================>                                                                                                                                (56 + 1) / 200]
[Stage 45:==================================================================================>                                                                                                (92 + 1) / 200]
[Stage 45:=========================================================================================================>                                                                        (119 + 1) / 200]
[Stage 45:==================================================================================================================================>                                               (147 + 1) / 200]
[Stage 45:===========================================================================================================================================================>                      (175 + 1) / 200]
                                                                                                                                                                                                            

[Stage 3:==========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            

[Stage 64:>                                                                                                                                                                                     (0 + 0) / 3]
                                                                                                                                                                                                            

[Stage 2:====================================================================================>                                                                                               (94 + 1) / 200]
[Stage 2:================================================================================================================>                                                                  (126 + 1) / 200]
[Stage 2:=================================================================================================================================================>                                 (163 + 1) / 200]
[Stage 2:=============================================================================================================================================================================>     (194 + 1) / 200]
                                                                                                                                                                                                            

[Stage 6:==========================================================================================================================>                                                            (2 + 1) / 3]
                                                                                                                                                                                                            
processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
11606 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741832_1008] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741832_1008
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
11995 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit1442145284422719512/.test-fileid1_100.log.1
12088 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
12127 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
12131 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
12133 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
12134 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
12134 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12134 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7017320056631278980
12138 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7017320056631278980 as 1
12138 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7017320056631278980/.test-fileid1_100.log.1
12140 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7017320056631278980/.test-fileid1_100.log.1} does not exist. Create a new file
12247 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
12247 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7017320056631278980
12248 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7017320056631278980 as 1
12248 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7017320056631278980/.test-fileid1_100.log.1
12249 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7017320056631278980/.test-fileid1_100.log.1} exists. Appending to existing file
12324 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
12327 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
12329 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
12333 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1}
12334 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
12335 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1} has a corrupted block at 9289
13755 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1} starts at 9853
13756 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1}
13756 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
13759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1}
13759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
13759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
13759 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
13759 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
13760 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1}
13760 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit7017320056631278980/.test-fileid1_100.log.1
13760 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
13802 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
13808 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
13808 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
13808 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
13809 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71819
13838 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
13874 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
13877 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
13899 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
13899 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
13938 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
13938 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit6859294748908578916/append_test
13939 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit6859294748908578916/append_test as 1
13940 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit6859294748908578916/append_test/.commits.archive_.archive.1
13940 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit6859294748908578916/append_test/.commits.archive_.archive.1} does not exist. Create a new file
13964 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
13965 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit6859294748908578916/append_test
13966 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit6859294748908578916/append_test as 1
13966 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit6859294748908578916/append_test/.commits.archive_.archive.1
13967 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit6859294748908578916/append_test/.commits.archive_.archive.1} exists. Appending to existing file
13968 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
14008 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14074 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14084 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14086 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14086 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14086 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14086 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6158432219868913606
14087 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6158432219868913606 as 1
14087 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6158432219868913606/.test-fileid1_100.log.1
14090 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6158432219868913606/.test-fileid1_100.log.1} does not exist. Create a new file
14176 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14179 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14184 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1}
14187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1}
14187 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14188 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1}
14188 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14188 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14188 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14188 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
14189 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1}
14189 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6158432219868913606/.test-fileid1_100.log.1
14189 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
14207 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
14208 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
14208 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
14208 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
14208 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71806
14236 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14310 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14314 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14318 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14318 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14318 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14318 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8359797209624300852
14318 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8359797209624300852 as 1
14319 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8359797209624300852/.test-fileid1_100.log.1
14320 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8359797209624300852/.test-fileid1_100.log.1} does not exist. Create a new file
14375 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14375 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8359797209624300852
14376 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8359797209624300852 as 1
14377 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8359797209624300852/.test-fileid1_100.log.1
14378 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8359797209624300852/.test-fileid1_100.log.1} exists. Appending to existing file
14503 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14503 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8359797209624300852
14506 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8359797209624300852 as 1
14506 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8359797209624300852/.test-fileid1_100.log.1
14507 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8359797209624300852/.test-fileid1_100.log.1} exists. Appending to existing file
14679 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14734 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14738 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14740 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14740 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14740 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14740 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5355398955684138135
14742 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5355398955684138135 as 1
14742 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5355398955684138135/.test-fileid1_100.log.1
14743 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5355398955684138135/.test-fileid1_100.log.1} does not exist. Create a new file
14780 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14780 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5355398955684138135
14782 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5355398955684138135 as 1
14782 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5355398955684138135/.test-fileid1_100.log.1
14783 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5355398955684138135/.test-fileid1_100.log.1} exists. Appending to existing file
14852 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14852 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5355398955684138135
14853 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5355398955684138135 as 1
14853 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5355398955684138135/.test-fileid1_100.log.1
14854 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5355398955684138135/.test-fileid1_100.log.1} exists. Appending to existing file
14935 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
14965 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
14969 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
14973 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
14973 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
14973 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
14973 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8697755051910101127
14975 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8697755051910101127 as 1
14975 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8697755051910101127/.test-fileid1_100.log.1
14976 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} does not exist. Create a new file
15059 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} has a corrupted block at 2163
15097 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} starts at 2195
15145 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15145 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8697755051910101127
15146 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8697755051910101127 as 1
15146 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8697755051910101127/.test-fileid1_100.log.1
15147 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} exists. Appending to existing file
15198 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} has a corrupted block at 2163
15245 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} starts at 2195
15246 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} has a corrupted block at 2209
15289 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit8697755051910101127/.test-fileid1_100.log.1} starts at 2246
15296 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15336 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15345 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15355 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15355 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15356 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15356 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8900008165612848682
15357 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8900008165612848682 as 1
15357 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8900008165612848682/.test-fileid1_100.log.1
15357 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8900008165612848682/.test-fileid1_100.log.1} does not exist. Create a new file
15407 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15416 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15417 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15432 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1}
15432 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1}
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1}
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15433 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1}
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit8900008165612848682/.test-fileid1_100.log.1
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
15435 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
15437 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15472 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15475 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15487 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15487 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15487 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15487 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3946019044870555113
15488 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3946019044870555113 as 1
15488 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3946019044870555113/.test-fileid1_100.log.1
15492 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3946019044870555113/.test-fileid1_100.log.1} does not exist. Create a new file
15578 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15584 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15586 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15596 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3946019044870555113/.test-fileid1_100.log.1}
15596 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit3946019044870555113/.test-fileid1_100.log.1
15602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3946019044870555113/.test-fileid1_100.log.1}
15602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit3946019044870555113/.test-fileid1_100.log.1
15602 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:33531/tmp/junit3946019044870555113/.test-fileid1_100.log.1
15602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
15602 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
15609 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
15615 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
15615 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
15615 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
15615 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33802
15628 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15673 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15676 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15682 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15682 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15682 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15682 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit706021968366401621
15683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit706021968366401621 as 1
15683 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit706021968366401621/.test-fileid1_100.log.1
15684 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit706021968366401621/.test-fileid1_100.log.1} does not exist. Create a new file
15721 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15721 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit706021968366401621
15722 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit706021968366401621 as 1
15722 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit706021968366401621/.test-fileid1_100.log.1
15723 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit706021968366401621/.test-fileid1_100.log.1} exists. Appending to existing file
15800 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15800 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit706021968366401621
15803 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit706021968366401621 as 1
15803 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit706021968366401621/.test-fileid1_100.log.1
15803 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit706021968366401621/.test-fileid1_100.log.1} exists. Appending to existing file
15864 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
15982 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
15988 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
15991 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
15991 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
15991 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
15991 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4023287795485983953
15993 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4023287795485983953 as 1
15993 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4023287795485983953/.test-fileid1_100.log.1
15993 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4023287795485983953/.test-fileid1_100.log.1} does not exist. Create a new file
16098 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16098 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4023287795485983953
16099 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4023287795485983953 as 1
16099 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4023287795485983953/.test-fileid1_100.log.1
16100 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4023287795485983953/.test-fileid1_100.log.1} exists. Appending to existing file
16154 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16154 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4023287795485983953
16155 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4023287795485983953 as 1
16155 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4023287795485983953/.test-fileid1_100.log.1
16165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4023287795485983953/.test-fileid1_100.log.1} exists. Appending to existing file
16194 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16196 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16198 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16216 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16216 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16217 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16217 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16218 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16218 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16218 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} has a corrupted block at 10486
16239 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} starts at 10502
16239 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16240 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16240 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} has a corrupted block at 10516
16251 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} starts at 10532
16252 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16252 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16253 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16253 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16253 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} has a corrupted block at 11139
16279 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1} starts at 11155
16279 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16279 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16280 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1}
16280 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16280 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4023287795485983953/.test-fileid1_100.log.1
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
16281 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
16283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
16283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
16283 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
16286 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16302 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16310 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16313 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16313 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16314 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16314 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3658639195516898612
16315 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3658639195516898612 as 1
16315 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3658639195516898612/.test-fileid1_100.log.1
16316 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3658639195516898612/.test-fileid1_100.log.1} does not exist. Create a new file
16383 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16384 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3658639195516898612
16386 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3658639195516898612 as 1
16386 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3658639195516898612/.test-fileid1_100.log.1
16386 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3658639195516898612/.test-fileid1_100.log.1} exists. Appending to existing file
16420 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
16425 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3658639195516898612/.test-fileid1_100.log.2} does not exist. Create a new file
16431 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16442 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16446 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16447 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16447 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16447 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16447 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5115716203511847409
16449 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5115716203511847409 as 1
16449 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5115716203511847409/.test-fileid1_100.log.1
16449 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.1} does not exist. Create a new file
16484 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
16488 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.2} does not exist. Create a new file
16505 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
16510 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.3} does not exist. Create a new file
16527 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
16532 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.4} does not exist. Create a new file
16535 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16542 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16544 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.1}
16546 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit5115716203511847409/.test-fileid1_100.log.1
16565 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.2}
16565 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit5115716203511847409/.test-fileid1_100.log.2
16568 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit5115716203511847409/.test-fileid1_100.log.3}
16568 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit5115716203511847409/.test-fileid1_100.log.3
16568 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
16604 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
16610 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
16610 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
16610 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
16610 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109807
16646 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
16698 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
16700 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
16707 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
16707 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
16707 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16707 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit154298055551247402
16708 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit154298055551247402 as 1
16708 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit154298055551247402/.test-fileid1_100.log.1
16709 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit154298055551247402/.test-fileid1_100.log.1} does not exist. Create a new file
16763 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16763 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit154298055551247402
16764 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit154298055551247402 as 1
16764 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit154298055551247402/.test-fileid1_100.log.1
16765 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit154298055551247402/.test-fileid1_100.log.1} exists. Appending to existing file
16800 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
16800 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit154298055551247402
16806 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit154298055551247402 as 1
16806 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit154298055551247402/.test-fileid1_100.log.1
16808 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit154298055551247402/.test-fileid1_100.log.1} exists. Appending to existing file
16809 [IPC Server handler 2 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit154298055551247402/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
16809 [IPC Server handler 2 on 33531] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:dginelli (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit154298055551247402/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
16810 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit154298055551247402/.test-fileid1_100.log.1
16810 [IPC Server handler 3 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit154298055551247402/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1057 for block blk_1073741859_1056{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-35f06790-9f8b-4538-a7bd-47f66eb63292:NORMAL:127.0.0.1:50010|RBW]]}
17577 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51302 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741859_1055]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51302 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:51302]. 59222 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
17579 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741859_1056] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741859_1056
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
17811 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit154298055551247402/.test-fileid1_100.log.1
17882 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
17902 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
17908 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
17923 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
17923 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
17923 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
17924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1687457123778517441
17925 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1687457123778517441 as 1
17925 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1687457123778517441/.test-fileid1_100.log.1
17926 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1687457123778517441/.test-fileid1_100.log.1} does not exist. Create a new file
17952 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
17961 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1687457123778517441/.test-fileid1_100.log.2} does not exist. Create a new file
17982 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
17995 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1687457123778517441/.test-fileid1_100.log.3} does not exist. Create a new file
18004 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18008 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18010 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18012 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1687457123778517441/.test-fileid1_100.log.1}
18012 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1687457123778517441/.test-fileid1_100.log.1
18016 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1687457123778517441/.test-fileid1_100.log.2}
18016 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1687457123778517441/.test-fileid1_100.log.2
18019 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
18064 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18065 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
18065 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
18066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
18066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71799
18080 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18100 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18103 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18105 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18105 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18106 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18106 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8610947508973712829
18107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8610947508973712829 as 1
18107 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8610947508973712829/.test-fileid1_100.log.1
18108 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8610947508973712829/.test-fileid1_100.log.1} does not exist. Create a new file
18142 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18144 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18147 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18170 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1}
18170 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18171 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1}
18171 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18171 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1}
18172 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1}
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1}
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit8610947508973712829/.test-fileid1_100.log.1
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
18173 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
18178 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18210 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18213 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18215 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18215 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18215 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18215 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6270611841922415389
18216 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6270611841922415389 as 1
18216 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6270611841922415389/.test-fileid1_100.log.1
18217 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6270611841922415389/.test-fileid1_100.log.1} does not exist. Create a new file
18285 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18292 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18294 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1
18317 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18317 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1
18318 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18318 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1
18318 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
18354 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18355 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 4
18355 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 3008
18355 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 146
18355 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71794
18373 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18375 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18380 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18397 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18397 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1
18401 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18401 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1
18402 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6270611841922415389/.test-fileid1_100.log.1}
18402 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
18426 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
18431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
18431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
18431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71794
18463 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18476 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18482 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18496 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18496 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18496 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18496 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1732824016069516227
18501 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1732824016069516227 as 1
18501 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1732824016069516227/.test-fileid1_100.log.1
18501 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1732824016069516227/.test-fileid1_100.log.1} does not exist. Create a new file
18535 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18535 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1732824016069516227
18536 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1732824016069516227 as 1
18536 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1732824016069516227/.test-fileid1_100.log.1
18536 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1732824016069516227/.test-fileid1_100.log.1} exists. Appending to existing file
18576 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18628 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18632 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18652 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18652 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18652 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18652 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3065509760351040467
18653 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3065509760351040467 as 1
18653 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3065509760351040467/.test-fileid1_100.log.1
18654 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3065509760351040467/.test-fileid1_100.log.1} does not exist. Create a new file
18674 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3065509760351040467
18675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3065509760351040467 as 1
18675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3065509760351040467/.test-fileid1_100.log.1
18676 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3065509760351040467/.test-fileid1_100.log.1} exists. Appending to existing file
18708 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18708 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3065509760351040467
18709 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3065509760351040467 as 1
18709 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3065509760351040467/.test-fileid1_100.log.1
18710 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3065509760351040467/.test-fileid1_100.log.1} exists. Appending to existing file
18794 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18853 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18855 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18860 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18860 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18860 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18860 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit2639255077454823026
18864 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit2639255077454823026 as 1
18864 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit2639255077454823026/.test-fileid1_100.log.1
18865 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit2639255077454823026/.test-fileid1_100.log.1} does not exist. Create a new file
18869 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
18898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18900 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18901 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18901 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
18901 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
18901 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4474071298324926356
18902 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4474071298324926356 as 1
18902 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4474071298324926356/.test-fileid1_100.log.1
18902 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4474071298324926356/.test-fileid1_100.log.1} does not exist. Create a new file
18922 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
18927 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
18932 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
18955 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1}
18955 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18956 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1}
18956 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18956 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1}
18956 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1}
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 3
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1}
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit4474071298324926356/.test-fileid1_100.log.1
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
18957 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
18960 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19001 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19003 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19005 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19005 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19005 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19005 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8868163208264295631
19006 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8868163208264295631 as 1
19006 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8868163208264295631/.test-fileid1_100.log.1
19007 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8868163208264295631/.test-fileid1_100.log.1} does not exist. Create a new file
19028 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19035 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19039 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19043 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19043 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19043 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19043 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1099051105868122789
19044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1099051105868122789 as 1
19044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1099051105868122789/.test-fileid1_100.log.1
19044 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1099051105868122789/.test-fileid1_100.log.1} does not exist. Create a new file
19072 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
19092 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
19096 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
19098 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
19098 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
19098 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19098 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8605857371156632801
19101 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8605857371156632801 as 1
19101 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8605857371156632801/.test-fileid1_100.log.1
19102 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8605857371156632801/.test-fileid1_100.log.1} does not exist. Create a new file
19163 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
19163 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8605857371156632801
19164 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8605857371156632801 as 1
19164 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8605857371156632801/.test-fileid1_100.log.1
19165 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8605857371156632801/.test-fileid1_100.log.1} exists. Appending to existing file
19166 [IPC Server handler 4 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit8605857371156632801/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
19166 [IPC Server handler 4 on 33531] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:dginelli (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit8605857371156632801/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
19169 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit8605857371156632801/.test-fileid1_100.log.1
19169 [IPC Server handler 5 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8605857371156632801/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1083 for block blk_1073741879_1082{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-39dc4c8c-b44f-4197-8db6-85016a188332:NORMAL:127.0.0.1:50010|RBW]]}
20171 [IPC Server handler 8 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit8605857371156632801/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1084 for block blk_1073741879_1082{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-39dc4c8c-b44f-4197-8db6-85016a188332:NORMAL:127.0.0.1:50010|RBW]]}
20580 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51388 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741879_1082]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51388 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:51388]. 58576 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
20581 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741879_1082] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741879_1082
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
21173 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit8605857371156632801/.test-fileid1_100.log.1
21267 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
21295 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
21296 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
21299 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
21299 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
21299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21299 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1945280423608419310
21300 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1945280423608419310 as 1
21300 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1945280423608419310/.test-fileid1_100.log.1
21300 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1945280423608419310/.test-fileid1_100.log.1} does not exist. Create a new file
21331 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
21331 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1945280423608419310
21331 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1945280423608419310 as 1
21331 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1945280423608419310/.test-fileid1_100.log.1
21332 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1945280423608419310/.test-fileid1_100.log.1} exists. Appending to existing file
21355 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
21356 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
21361 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
21363 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1}
21363 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21363 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1} has a corrupted block at 9289
21973 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1} starts at 9853
21974 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1}
21974 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21974 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1}
21975 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21975 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21975 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21975 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
21975 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1}
21975 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1945280423608419310/.test-fileid1_100.log.1
21976 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
21998 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
21998 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
21998 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
21998 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
21998 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71803
22027 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22049 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22052 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22055 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22055 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22096 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22096 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit5247303248547360522/append_test
22096 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit5247303248547360522/append_test as 1
22097 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit5247303248547360522/append_test/.commits.archive_.archive.1
22097 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit5247303248547360522/append_test/.commits.archive_.archive.1} does not exist. Create a new file
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for commits.archive in file:/tmp/junit5247303248547360522/append_test
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for commits.archive in file:/tmp/junit5247303248547360522/append_test as 1
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path file:/tmp/junit5247303248547360522/append_test/.commits.archive_.archive.1
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {file:/tmp/junit5247303248547360522/append_test/.commits.archive_.archive.1} exists. Appending to existing file
22142 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Append not supported. Opening a new log file..
22189 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22236 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22238 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22239 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22239 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22239 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22239 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1159802538532533014
22240 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1159802538532533014 as 1
22240 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1159802538532533014/.test-fileid1_100.log.1
22240 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1159802538532533014/.test-fileid1_100.log.1} does not exist. Create a new file
22310 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22312 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22313 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1}
22315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1
22315 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1}
22316 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1
22322 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1}
22322 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1
22322 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1
22322 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 1
22322 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1}
22323 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1159802538532533014/.test-fileid1_100.log.1
22323 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
22327 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
22327 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
22328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
22328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
22328 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71810
22340 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22377 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22381 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22410 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22410 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22411 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22411 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7219385287972537940
22412 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7219385287972537940 as 1
22412 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7219385287972537940/.test-fileid1_100.log.1
22412 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7219385287972537940/.test-fileid1_100.log.1} does not exist. Create a new file
22444 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22444 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7219385287972537940
22445 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7219385287972537940 as 1
22445 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7219385287972537940/.test-fileid1_100.log.1
22445 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7219385287972537940/.test-fileid1_100.log.1} exists. Appending to existing file
22533 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22533 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit7219385287972537940
22534 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit7219385287972537940 as 1
22534 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit7219385287972537940/.test-fileid1_100.log.1
22534 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit7219385287972537940/.test-fileid1_100.log.1} exists. Appending to existing file
22588 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22602 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22606 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22607 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22607 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22607 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22607 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5221228123637323549
22608 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5221228123637323549 as 1
22608 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5221228123637323549/.test-fileid1_100.log.1
22608 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5221228123637323549/.test-fileid1_100.log.1} does not exist. Create a new file
22620 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22620 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5221228123637323549
22621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5221228123637323549 as 1
22621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5221228123637323549/.test-fileid1_100.log.1
22621 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5221228123637323549/.test-fileid1_100.log.1} exists. Appending to existing file
22641 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22641 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5221228123637323549
22642 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5221228123637323549 as 1
22642 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5221228123637323549/.test-fileid1_100.log.1
22642 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5221228123637323549/.test-fileid1_100.log.1} exists. Appending to existing file
22685 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22704 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22708 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22711 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22711 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22711 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22711 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6201918967306858116
22712 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6201918967306858116 as 1
22712 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6201918967306858116/.test-fileid1_100.log.1
22714 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} does not exist. Create a new file
22747 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} has a corrupted block at 2163
22769 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} starts at 2195
22787 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22787 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6201918967306858116
22788 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6201918967306858116 as 1
22788 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6201918967306858116/.test-fileid1_100.log.1
22789 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} exists. Appending to existing file
22815 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} has a corrupted block at 2163
22859 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} starts at 2195
22860 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} has a corrupted block at 2209
22896 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {/tmp/junit6201918967306858116/.test-fileid1_100.log.1} starts at 2246
22900 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
22938 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22944 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22951 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
22951 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
22951 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
22951 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1949583120181183026
22952 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1949583120181183026 as 1
22952 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1949583120181183026/.test-fileid1_100.log.1
22952 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1949583120181183026/.test-fileid1_100.log.1} does not exist. Create a new file
22986 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
22987 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
22989 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23007 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1}
23007 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1}
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1}
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23008 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 2
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1}
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit1949583120181183026/.test-fileid1_100.log.1
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
23009 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
23011 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23029 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23031 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23035 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23035 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23035 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23035 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit645566820549598312
23036 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit645566820549598312 as 1
23036 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit645566820549598312/.test-fileid1_100.log.1
23036 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit645566820549598312/.test-fileid1_100.log.1} does not exist. Create a new file
23049 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23052 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23053 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit645566820549598312/.test-fileid1_100.log.1}
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit645566820549598312/.test-fileid1_100.log.1
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit645566820549598312/.test-fileid1_100.log.1}
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit645566820549598312/.test-fileid1_100.log.1
23066 [main] WARN  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - TargetInstantTime 101 invalid or extra rollback command block in hdfs://localhost:33531/tmp/junit645566820549598312/.test-fileid1_100.log.1
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 0
23066 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
23071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
23071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
23071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
23071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 89
23071 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 33807
23083 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23113 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23116 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23118 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23118 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23118 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4055895458401111133
23119 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4055895458401111133 as 1
23119 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4055895458401111133/.test-fileid1_100.log.1
23120 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4055895458401111133/.test-fileid1_100.log.1} does not exist. Create a new file
23131 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23131 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4055895458401111133
23135 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4055895458401111133 as 1
23135 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4055895458401111133/.test-fileid1_100.log.1
23135 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4055895458401111133/.test-fileid1_100.log.1} exists. Appending to existing file
23151 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23151 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4055895458401111133
23152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4055895458401111133 as 1
23152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4055895458401111133/.test-fileid1_100.log.1
23152 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4055895458401111133/.test-fileid1_100.log.1} exists. Appending to existing file
23183 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23191 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23192 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23206 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23206 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23206 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23206 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6826779030477461800
23206 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6826779030477461800 as 1
23207 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6826779030477461800/.test-fileid1_100.log.1
23207 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6826779030477461800/.test-fileid1_100.log.1} does not exist. Create a new file
23280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23280 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6826779030477461800
23281 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6826779030477461800 as 1
23281 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6826779030477461800/.test-fileid1_100.log.1
23282 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6826779030477461800/.test-fileid1_100.log.1} exists. Appending to existing file
23323 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23323 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6826779030477461800
23324 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6826779030477461800 as 1
23324 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6826779030477461800/.test-fileid1_100.log.1
23328 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6826779030477461800/.test-fileid1_100.log.1} exists. Appending to existing file
23343 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23348 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23349 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23363 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23363 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23370 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23370 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23371 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23371 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23371 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} has a corrupted block at 10486
23384 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} starts at 10502
23384 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23384 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23385 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} has a corrupted block at 10516
23402 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} starts at 10532
23402 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23402 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23403 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23403 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Log HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} has a corrupted block at 11139
23428 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFileReader  - Next available block in HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1} starts at 11155
23429 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23429 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Found a corrupt block in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1}
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last corrupted log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit6826779030477461800/.test-fileid1_100.log.1
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 7
23430 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
23431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
23431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
23431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
23431 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
23433 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23457 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23458 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23474 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23474 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23474 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23474 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit908863031201442465
23475 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit908863031201442465 as 1
23475 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit908863031201442465/.test-fileid1_100.log.1
23476 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit908863031201442465/.test-fileid1_100.log.1} does not exist. Create a new file
23515 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23515 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit908863031201442465
23516 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit908863031201442465 as 1
23517 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit908863031201442465/.test-fileid1_100.log.1
23519 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit908863031201442465/.test-fileid1_100.log.1} exists. Appending to existing file
23545 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 4298 has reached threshold 2148. Rolling over to the next version
23549 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit908863031201442465/.test-fileid1_100.log.2} does not exist. Create a new file
23553 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23561 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23563 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23568 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23568 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23568 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23568 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit4867390398593306666
23569 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit4867390398593306666 as 1
23569 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit4867390398593306666/.test-fileid1_100.log.1
23569 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.1} does not exist. Create a new file
23604 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
23610 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.2} does not exist. Create a new file
23618 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
23624 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.3} does not exist. Create a new file
23652 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 1024. Rolling over to the next version
23659 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.4} does not exist. Create a new file
23661 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23670 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23672 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23674 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.3}
23674 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4867390398593306666/.test-fileid1_100.log.3
23676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.2}
23676 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4867390398593306666/.test-fileid1_100.log.2
23689 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit4867390398593306666/.test-fileid1_100.log.1}
23689 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit4867390398593306666/.test-fileid1_100.log.1
23689 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
23719 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
23721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
23721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8360
23721 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 289
23722 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 109782
23779 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
23800 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
23802 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
23805 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
23805 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
23805 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23805 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5350475878203531552
23806 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5350475878203531552 as 1
23806 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5350475878203531552/.test-fileid1_100.log.1
23807 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5350475878203531552/.test-fileid1_100.log.1} does not exist. Create a new file
23825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5350475878203531552
23825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5350475878203531552 as 1
23825 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5350475878203531552/.test-fileid1_100.log.1
23826 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5350475878203531552/.test-fileid1_100.log.1} exists. Appending to existing file
23842 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
23842 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5350475878203531552
23843 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5350475878203531552 as 1
23843 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5350475878203531552/.test-fileid1_100.log.1
23845 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5350475878203531552/.test-fileid1_100.log.1} exists. Appending to existing file
23846 [IPC Server handler 9 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.append: failed to create file /tmp/junit5350475878203531552/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
23846 [IPC Server handler 9 on 33531] WARN  org.apache.hadoop.security.UserGroupInformation  - PriviledgedActionException as:dginelli (auth:SIMPLE) cause:org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /tmp/junit5350475878203531552/.test-fileid1_100.log.1 for DFSClient_NONMAPREDUCE_1760720046_1 for client 127.0.0.1 because current leaseholder is trying to recreate file.
23847 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Trying to recover log on path /tmp/junit5350475878203531552/.test-fileid1_100.log.1
23847 [IPC Server handler 0 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit5350475878203531552/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1131 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-39dc4c8c-b44f-4197-8db6-85016a188332:NORMAL:127.0.0.1:50010|RBW]]}
24848 [IPC Server handler 1 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit5350475878203531552/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1132 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-39dc4c8c-b44f-4197-8db6-85016a188332:NORMAL:127.0.0.1:50010|RBW]]}
25849 [IPC Server handler 2 on 33531] WARN  org.apache.hadoop.hdfs.StateChange  - DIR* NameSystem.internalReleaseLease: File /tmp/junit5350475878203531552/.test-fileid1_100.log.1 has not been closed. Lease recovery is in progress. RecoveryId = 1133 for block blk_1073741906_1130{blockUCState=UNDER_RECOVERY, primaryNodeIndex=0, replicas=[ReplicaUnderConstruction[[DISK]DS-39dc4c8c-b44f-4197-8db6-85016a188332:NORMAL:127.0.0.1:50010|RBW]]}
26585 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51512 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741906_1129]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51512 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/127.0.0.1:50010 remote=/127.0.0.1:51512]. 57257 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
26586 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741906_1130] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741906_1130
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
26850 [main] WARN  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - Recovered lease on path /tmp/junit5350475878203531552/.test-fileid1_100.log.1
26898 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
26916 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26919 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26922 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26922 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
26923 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
26923 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit9105657298026544389
26923 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit9105657298026544389 as 1
26923 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit9105657298026544389/.test-fileid1_100.log.1
26924 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9105657298026544389/.test-fileid1_100.log.1} does not exist. Create a new file
26935 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
26941 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9105657298026544389/.test-fileid1_100.log.2} does not exist. Create a new file
26965 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - CurrentSize 9275 has reached threshold 500. Rolling over to the next version
26972 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit9105657298026544389/.test-fileid1_100.log.3} does not exist. Create a new file
26975 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
26977 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
26980 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
26990 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit9105657298026544389/.test-fileid1_100.log.1}
26990 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit9105657298026544389/.test-fileid1_100.log.1
27015 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit9105657298026544389/.test-fileid1_100.log.2}
27015 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit9105657298026544389/.test-fileid1_100.log.2
27016 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
27030 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
27031 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
27031 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
27031 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
27037 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71792
27049 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27080 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27082 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27084 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27084 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27084 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27084 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit3110197977501383248
27086 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit3110197977501383248 as 1
27086 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit3110197977501383248/.test-fileid1_100.log.1
27086 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit3110197977501383248/.test-fileid1_100.log.1} does not exist. Create a new file
27102 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27104 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27105 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27119 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1}
27119 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27119 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1}
27119 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27120 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1}
27120 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1}
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1}
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a command block from file hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Rolling back the last log block read in hdfs://localhost:33531/tmp/junit3110197977501383248/.test-fileid1_100.log.1
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of applied rollback blocks 4
27121 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
27122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
27122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
27122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 0
27122 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 0
27124 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27145 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27146 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27152 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27152 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27153 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27153 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit5247769881833483955
27158 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit5247769881833483955 as 1
27158 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit5247769881833483955/.test-fileid1_100.log.1
27159 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit5247769881833483955/.test-fileid1_100.log.1} does not exist. Create a new file
27230 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27232 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27234 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27249 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27249 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1
27250 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27250 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1
27255 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27255 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a delete block from file hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1
27259 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
27264 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
27267 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 5
27267 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 3760
27267 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 145
27267 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71808
27290 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27292 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27297 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27305 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27305 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1
27306 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27306 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1
27326 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {hdfs://localhost:33531/tmp/junit5247769881833483955/.test-fileid1_100.log.1}
27326 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
27330 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 10240
27330 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 11
27330 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 8272
27330 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 189
27330 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 71808
27354 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27397 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27399 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27402 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27402 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27402 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27402 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6271475522847364927
27402 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6271475522847364927 as 1
27402 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6271475522847364927/.test-fileid1_100.log.1
27403 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6271475522847364927/.test-fileid1_100.log.1} does not exist. Create a new file
27545 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27545 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit6271475522847364927
27546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit6271475522847364927 as 1
27546 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6271475522847364927/.test-fileid1_100.log.1
27547 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6271475522847364927/.test-fileid1_100.log.1} exists. Appending to existing file
27642 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27657 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27658 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27660 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27660 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27660 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27660 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1537572657029682553
27661 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1537572657029682553 as 1
27661 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1537572657029682553/.test-fileid1_100.log.1
27661 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1537572657029682553/.test-fileid1_100.log.1} does not exist. Create a new file
27669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1537572657029682553
27670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1537572657029682553 as 1
27670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1537572657029682553/.test-fileid1_100.log.1
27670 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1537572657029682553/.test-fileid1_100.log.1} exists. Appending to existing file
27691 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27691 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit1537572657029682553
27693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit1537572657029682553 as 1
27693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1537572657029682553/.test-fileid1_100.log.1
27693 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1537572657029682553/.test-fileid1_100.log.1} exists. Appending to existing file
27729 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp as hoodie dataset /tmp
27747 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp
27749 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/.hoodie/hoodie.properties
27774 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp
27774 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp
27774 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
27774 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for test-fileid1 in /tmp/junit8804307864524531828
27775 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for test-fileid1 in /tmp/junit8804307864524531828 as 1
27775 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit8804307864524531828/.test-fileid1_100.log.1
27776 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit8804307864524531828/.test-fileid1_100.log.1} does not exist. Create a new file
27779 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
27780 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741866_1065] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741866_1065
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27789 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741845_1033] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741845_1033
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27790 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741894_1109] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741894_1109
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27790 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741826_1002] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741826_1002
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27790 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741851_1046] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741851_1046
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27790 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741873_1076] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741873_1076
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27790 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741864_1063] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741864_1063
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27791 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741911_1139] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741911_1139
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27791 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741892_1107] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741892_1107
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27792 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741898_1120] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741898_1120
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27792 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741847_1035] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741847_1035
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27814 [ResponseProcessor for block BP-380954924-130.239.242.71-1584643854328:blk_1073741913_1141] WARN  org.apache.hadoop.hdfs.DFSClient  - DFSOutputStream ResponseProcessor exception  for block BP-380954924-130.239.242.71-1584643854328:blk_1073741913_1141
java.io.EOFException: Premature EOF: no length prefix available
	at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2241)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:235)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:971)
27913 [263883701@qtp-1357843335-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46067] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
27914 [263883701@qtp-1357843335-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46067] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
28020 [DataNode: [[[DISK]file:/tmp/1584643853081-0/dfs/data/data1/, [DISK]file:/tmp/1584643853081-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33531] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-380954924-130.239.242.71-1584643854328 (Datanode Uuid 1cdd94f4-6306-49ad-b260-051f20df1e11) service to localhost/127.0.0.1:33531 interrupted
28020 [DataNode: [[[DISK]file:/tmp/1584643853081-0/dfs/data/data1/, [DISK]file:/tmp/1584643853081-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:33531] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-380954924-130.239.242.71-1584643854328 (Datanode Uuid 1cdd94f4-6306-49ad-b260-051f20df1e11) service to localhost/127.0.0.1:33531
28021 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51546 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741913_1141]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51546 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59273 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28022 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51538 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741911_1139]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51538 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 59084 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28022 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51482 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741898_1119]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51482 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55325 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28022 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51454 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741894_1109]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51454 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 55033 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51446 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741892_1107]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51446 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 54968 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51272 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741851_1045]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51272 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 48175 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51370 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741873_1076]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51370 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 50905 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51332 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741866_1065]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51332 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 50358 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51244 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741847_1035]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51244 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 47561 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28023 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51324 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741864_1063]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51324 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 50118 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28024 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51236 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741845_1033]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51236 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 47391 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28024 [DataXceiver for client DFSClient_NONMAPREDUCE_1760720046_1 at /127.0.0.1:51156 [Receiving block BP-380954924-130.239.242.71-1584643854328:blk_1073741826_1002]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode  - 127.0.0.1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:51156 dst: /127.0.0.1:50010
java.io.InterruptedIOException: Interruped while waiting for IO on channel java.nio.channels.SocketChannel[closed]. 41305 millis timeout left.
	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:352)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:199)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:500)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:894)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:798)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:246)
	at java.lang.Thread.run(Thread.java:748)
28040 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@4203bb7] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 46, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 24.586 sec - in com.uber.hoodie.common.table.log.HoodieLogFormatTest
Running com.uber.hoodie.common.TestBloomFilter
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in com.uber.hoodie.common.TestBloomFilter
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 44,000
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 22,441B for [_hoodie_record_key] BINARY: 1,000 values, 40,007B raw, 22,339B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 1000 records.
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:50:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 8 ms. row count = 1000
28336 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16450
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28343 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16420
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28349 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16453
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28349 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16391
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28349 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16423
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28350 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16524
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28350 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16429
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28354 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16462
java.io.EOFException: End of File Exception between local host is: "b-cn1011.hpc2n.umu.se/130.239.242.71"; destination host is: "localhost":33531; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1080)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:975)
28358 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16494
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-39dc4c8c-b44f-4197-8db6-85016a188332,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28358 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16527
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-39dc4c8c-b44f-4197-8db6-85016a188332,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28360 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16465
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-35f06790-9f8b-4538-a7bd-47f66eb63292,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28363 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16497
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-39dc4c8c-b44f-4197-8db6-85016a188332,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28363 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16503
java.io.IOException: All datanodes DatanodeInfoWithStorage[127.0.0.1:50010,DS-39dc4c8c-b44f-4197-8db6-85016a188332,DISK] are bad. Aborting...
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1357)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1119)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:622)
28365 [Thread-13] ERROR org.apache.hadoop.hdfs.DFSClient  - Failed to close inode 16536
java.net.ConnectException: Call From b-cn1011.hpc2n.umu.se/130.239.242.71 to localhost:33531 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1475)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy29.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:448)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
	at com.sun.proxy.$Proxy30.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2513)
	at org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:2495)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2458)
	at org.apache.hadoop.hdfs.DFSClient.closeAllFilesBeingWritten(DFSClient.java:986)
	at org.apache.hadoop.hdfs.DFSClient.closeOutputStreams(DFSClient.java:1018)
	at org.apache.hadoop.hdfs.DistributedFileSystem.close(DistributedFileSystem.java:986)
	at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2793)
	at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2810)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1524)
	at org.apache.hadoop.ipc.Client.call(Client.java:1447)
	... 19 more

Results :

Tests run: 82, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:report (post-unit-test) @ hoodie-common ---
[INFO] Loading execution data file /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-common/target/coverage-reports/jacoco-ut.exec
[INFO] Analyzed bundle 'hoodie-common' with 116 classes
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-hadoop-mr 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-hadoop-mr/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.2:compile (default-compile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-hadoop-mr ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.2:testCompile (default-testCompile) @ hoodie-hadoop-mr ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-hadoop-mr ---
[INFO] Surefire report directory: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-hadoop-mr/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
314  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
527  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit8145350658380242560 as hoodie dataset /tmp/junit8145350658380242560
529  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
562  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit8145350658380242560
563  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
563  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit8145350658380242560/.hoodie/hoodie.properties
577  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit8145350658380242560
578  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit8145350658380242560
578  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit1390325842830449703 as hoodie dataset /tmp/junit1390325842830449703
603  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
642  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1390325842830449703
643  [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
643  [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1390325842830449703/.hoodie/hoodie.properties
643  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1390325842830449703
643  [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1390325842830449703
1998 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
1999 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit1390325842830449703/2016/05/01
2027 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit1390325842830449703/2016/05/01 as 1
2027 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit1390325842830449703/2016/05/01/.fileid0_100.log.1
2028 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit1390325842830449703/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
2873 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favoriteIntNumber,favoriteNumber,favoriteFloatNumber,favoriteDoubleNumber,tags,testNestedRecord,stringArray
2899 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit1390325842830449703/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit1390325842830449703/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favoriteIntNumber, favoriteNumber, favoriteFloatNumber, favoriteDoubleNumber, tags, testNestedRecord, stringArray]
2900 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
2900 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit1390325842830449703
2901 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
2902 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit1390325842830449703/.hoodie/hoodie.properties
2903 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1390325842830449703
2905 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/c23d85cc-313b-425f-af2e-e97f702b4220
2908 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit1390325842830449703/2016/05/01/.fileid0_100.log.1}
2915 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit1390325842830449703/2016/05/01/.fileid0_100.log.1
2915 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
3000 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 1640
3056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
3056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
3056 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 1640
3057 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 49
3057 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 21060
3203 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3206 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3763846328148572537 as hoodie dataset /tmp/junit3763846328148572537
3206 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3271 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3763846328148572537
3271 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3271 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3763846328148572537/.hoodie/hoodie.properties
3272 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type MERGE_ON_READ from /tmp/junit3763846328148572537
3272 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type MERGE_ON_READ from /tmp/junit3763846328148572537
3272 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit6744667433766483656 as hoodie dataset /tmp/junit6744667433766483656
3287 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3355 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6744667433766483656
3356 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3356 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6744667433766483656/.hoodie/hoodie.properties
3356 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6744667433766483656
3356 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6744667433766483656
3669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Building HoodieLogFormat Writer
3669 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computing the next log version for fileid0 in /tmp/junit6744667433766483656/2016/05/01
3675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - Computed the next log version for fileid0 in /tmp/junit6744667433766483656/2016/05/01 as 1
3675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder  - HoodieLogFile on path /tmp/junit6744667433766483656/2016/05/01/.fileid0_100.log.1
3675 [main] INFO  com.uber.hoodie.common.table.log.HoodieLogFormatWriter  - HoodieLogFile {/tmp/junit6744667433766483656/2016/05/01/.fileid0_100.log.1} does not exist. Create a new file
3973 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,field1,field2,name,favorite_number,favorite_color,favorite_movie
3991 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [/tmp/junit6744667433766483656/2016/05/01/.fileid0_100.log.1] for base split /tmp/junit6744667433766483656/2016/05/01/fileid0_1_100.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, field1, field2, name, favorite_number, favorite_color, favorite_movie]
3992 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3992 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit6744667433766483656
3992 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
3993 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit6744667433766483656/.hoodie/hoodie.properties
3994 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6744667433766483656
3994 [main] INFO  com.uber.hoodie.common.util.collection.DiskBasedMap  - Spilling to file location /tmp/30b7237e-a88c-4f6d-8308-a652533713ae
3995 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Scanning log file HoodieLogFile {/tmp/junit6744667433766483656/2016/05/01/.fileid0_100.log.1}
3995 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Reading a data block from file /tmp/junit6744667433766483656/2016/05/01/.fileid0_100.log.1
3995 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Merging the final data blocks
4008 [main] INFO  com.uber.hoodie.common.util.collection.ExternalSpillableMap  - Estimated Payload size => 728
4026 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - MaxMemoryInBytes allowed for compaction => 768
4043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in MemoryBasedMap in ExternalSpillableMap => 1
4043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 728
4043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Number of entries in DiskBasedMap in ExternalSpillableMap => 99
4043 [main] INFO  com.uber.hoodie.common.table.log.HoodieCompactedLogRecordScanner  - Size of file spilled to disk => 29036
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.98 sec - in com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReaderTest
Running com.uber.hoodie.hadoop.HoodieInputFormatTest
4119 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2340774831415968832 as hoodie dataset /tmp/junit2340774831415968832
4146 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4203 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2340774831415968832
4203 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4203 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2340774831415968832/.hoodie/hoodie.properties
4203 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2340774831415968832
4203 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2340774831415968832
4531 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit2340774831415968832
4531 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2340774831415968832
4532 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4532 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2340774831415968832/.hoodie/hoodie.properties
4532 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2340774831415968832
4532 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
4547 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
4567 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
4628 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
4629 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
4630 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
4645 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_200.parquet
4646 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_200.parquet
4646 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_200.parquet
4646 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_200.parquet
4646 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_200.parquet
4647 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
4741 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit2340774831415968832
4741 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2340774831415968832
4742 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4742 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2340774831415968832/.hoodie/hoodie.properties
4742 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2340774831415968832
4742 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
4743 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
4743 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
4745 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
4745 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 3
4745 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_400.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_300.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_300.parquet
4746 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
4837 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit2340774831415968832
4837 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2340774831415968832
4837 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4837 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2340774831415968832/.hoodie/hoodie.properties
4838 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2340774831415968832
4838 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
4838 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
4839 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit], [300__commit], [400__commit], [500__commit], [600__commit]]
4840 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 2147483647
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_600.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid0_1_600.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_400.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid2_1_400.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_500.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid1_1_500.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid4_1_200.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Processing incremental hoodie file - file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_300.parquet
4841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit2340774831415968832/2016/05/01/fileid3_1_300.parquet
4842 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 5
4886 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3716428982565728227 as hoodie dataset /tmp/junit3716428982565728227
4914 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4974 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3716428982565728227
4974 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
4975 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3716428982565728227/.hoodie/hoodie.properties
4976 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3716428982565728227
4976 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3716428982565728227
5017 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3716428982565728227
5017 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3716428982565728227
5018 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5018 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3716428982565728227/.hoodie/hoodie.properties
5018 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3716428982565728227
5018 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5018 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
5019 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
5020 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
5020 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid0_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid2_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid1_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid4_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid3_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid6_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid5_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid8_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid7_1_100.parquet
5021 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid9_1_100.parquet
5062 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3716428982565728227
5062 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3716428982565728227
5062 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5062 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3716428982565728227/.hoodie/hoodie.properties
5062 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3716428982565728227
5062 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5063 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
5063 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid0_1_100.parquet
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid2_1_100.parquet
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid1_1_100.parquet
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid4_1_100.parquet
5064 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid3_1_100.parquet
5065 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid6_1_100.parquet
5065 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid5_1_100.parquet
5065 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid8_1_100.parquet
5065 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid7_1_100.parquet
5065 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3716428982565728227/2016/05/01/fileid9_1_100.parquet
5096 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit4771986446119191227 as hoodie dataset /tmp/junit4771986446119191227
5125 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5171 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit4771986446119191227
5171 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5171 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit4771986446119191227/.hoodie/hoodie.properties
5171 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4771986446119191227
5172 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4771986446119191227
5224 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit4771986446119191227
5224 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit4771986446119191227
5224 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5225 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit4771986446119191227/.hoodie/hoodie.properties
5225 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit4771986446119191227
5225 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5225 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: INCREMENTAL
5226 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
5226 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read start commit time - 100
5227 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - Read max commits - 1
5227 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Last Incremental timestamp was set as 100
5227 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie incremental filter 0
5264 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit3962173410218137574 as hoodie dataset /tmp/junit3962173410218137574
5287 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5347 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit3962173410218137574
5347 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5347 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit3962173410218137574/.hoodie/hoodie.properties
5348 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3962173410218137574
5348 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3962173410218137574
5463 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3962173410218137574
5463 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3962173410218137574
5463 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5463 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3962173410218137574/.hoodie/hoodie.properties
5463 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3962173410218137574
5463 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5464 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
5464 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid0_1_100.parquet
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid2_1_100.parquet
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid1_1_100.parquet
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid4_1_100.parquet
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid3_1_100.parquet
5465 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid6_1_100.parquet
5466 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid5_1_100.parquet
5466 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid8_1_100.parquet
5466 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid7_1_100.parquet
5466 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid9_1_100.parquet
5517 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3962173410218137574
5517 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3962173410218137574
5517 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5518 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3962173410218137574/.hoodie/hoodie.properties
5518 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3962173410218137574
5518 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5518 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
5523 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit]]
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid0_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid2_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid1_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid4_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid3_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid6_1_100.parquet
5524 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid5_1_100.parquet
5525 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid8_1_100.parquet
5525 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid7_1_100.parquet
5525 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid9_1_100.parquet
5569 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path file:/tmp/junit3962173410218137574
5569 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit3962173410218137574
5570 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5570 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit3962173410218137574/.hoodie/hoodie.properties
5570 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit3962173410218137574
5570 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
5570 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
5572 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[100__commit], [200__commit]]
5573 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 10
5573 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid0_1_200.parquet
5573 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid2_1_100.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid1_1_100.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid4_1_100.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid3_1_200.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid6_1_100.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid5_1_100.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid8_1_200.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid7_1_200.parquet
5574 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Refreshing file status file:/tmp/junit3962173410218137574/2016/05/01/fileid9_1_200.parquet
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.483 sec - in com.uber.hoodie.hadoop.HoodieInputFormatTest
Running com.uber.hoodie.hadoop.AnnotationTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.hadoop.AnnotationTest
Running com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
5578 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Initializing /tmp/junit2186806733267137006 as hoodie dataset /tmp/junit2186806733267137006
5624 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5662 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from /tmp/junit2186806733267137006
5663 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5663 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from /tmp/junit2186806733267137006/.hoodie/hoodie.properties
5663 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2186806733267137006
5663 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2186806733267137006
5691 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/tmp/junit2186806733267137006
5707 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3a52dba3]
5708 [main] INFO  com.uber.hoodie.common.table.HoodieTableConfig  - Loading dataset properties from file:/tmp/junit2186806733267137006/.hoodie/hoodie.properties
5708 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Finished Loading Table of type COPY_ON_WRITE from file:/tmp/junit2186806733267137006
5710 [main] INFO  com.uber.hoodie.common.table.timeline.HoodieActiveTimeline  - Loaded instants [[001__commit], [002__commit], [==>003__commit]]
5712 [main] INFO  com.uber.hoodie.hadoop.HoodieROTablePathFilter  - Based on hoodie metadata from base path: file:/tmp/junit2186806733267137006, caching 3 files under file:/tmp/junit2186806733267137006/2017/01/01
5817 [main] INFO  com.uber.hoodie.common.table.HoodieTableMetaClient  - Loading HoodieTableMetaClient from file:/
5824 [main] INFO  com.uber.hoodie.common.util.FSUtils  - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@4eed2acf]
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.244 sec - in com.uber.hoodie.hadoop.TestHoodieROTablePathFilter
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 88,387
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 25B raw, 1B comp}
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteIntNumber] INT32: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteNumber] INT64: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 443B for [favoriteFloatNumber] FLOAT: 100 values, 407B raw, 407B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 851B for [favoriteDoubleNumber] DOUBLE: 100 values, 807B raw, 807B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 108B for [tags, map, key] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY], dic { 2 entries, 24B raw, 2B comp}
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,156B for [tags, map, value, item1] BINARY: 200 values, 2,117B raw, 2,117B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 2,963B for [tags, map, value, item2] BINARY: 200 values, 2,917B raw, 2,917B comp, 1 pages, encodings: [RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 48B for [testNestedRecord, isAdmin] BOOLEAN: 100 values, 20B raw, 20B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,647B for [testNestedRecord, userId] BINARY: 100 values, 1,597B raw, 1,597B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:23 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [stringArray, array] BINARY: 200 values, 64B raw, 64B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY], dic { 2 entries, 40B raw, 2B comp}
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8,589
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_commit_time] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_commit_seqno] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 934B for [_hoodie_record_key] BINARY: 100 values, 897B raw, 897B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 42B for [_hoodie_partition_path] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_file_name] BINARY: 100 values, 10B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN_DICTIONARY], dic { 1 entries, 25B raw, 1B comp}
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,138B for [field1] BINARY: 100 values, 1,097B raw, 1,097B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,545B for [field2] BINARY: 100 values, 1,497B raw, 1,497B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,036B for [name] BINARY: 100 values, 997B raw, 997B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_number] INT64: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_color] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [favorite_movie] BINARY: 100 values, 7B raw, 7B comp, 1 pages, encodings: [BIT_PACKED, RLE, PLAIN]
Mar 19, 2020 7:51:24 PM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 19, 2020 7:51:24 PM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 19, 2020 7:51:24 PM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:24 PM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 33 ms. row count = 100
Mar 19, 2020 7:51:25 PM WARNING: parquet.hadoop.ParquetRecordReader: Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
Mar 19, 2020 7:51:25 PM INFO: parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 100 records.
Mar 19, 2020 7:51:25 PM INFO: parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:25 PM INFO: parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 100

Results :

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0

[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building hoodie-client 0.4.2-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.8:prepare-agent (pre-unit-test) @ hoodie-client ---
[INFO] surefireArgLine set to -javaagent:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jacoco/org.jacoco.agent/0.7.8/org.jacoco.agent-0.7.8-runtime.jar=destfile=/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/coverage-reports/jacoco-ut.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 1 resource
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.2:compile (default-compile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hoodie-client ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.2:testCompile (default-testCompile) @ hoodie-client ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ hoodie-client ---
[INFO] Surefire report directory: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.uber.hoodie.func.TestUpdateMapFunction
0    [main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2310 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
2346 [pool-1-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
3069 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
3080 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
4296 [pool-1-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
cc477cf9-5cce-4b19-bf91-ded6015a082c
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.54 sec <<< FAILURE! - in com.uber.hoodie.func.TestUpdateMapFunction
testSchemaEvolutionOnUpdate(com.uber.hoodie.func.TestUpdateMapFunction)  Time elapsed: 5.438 sec  <<< ERROR!
java.lang.NullPointerException
	at com.uber.hoodie.func.TestUpdateMapFunction.testSchemaEvolutionOnUpdate(TestUpdateMapFunction.java:106)

Running com.uber.hoodie.func.TestBufferedIterator
4500 [pool-2-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
4545 [pool-2-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
com.uber.hoodie.exception.HoodieException: operation has failed
	at com.uber.hoodie.func.BufferedIterator.throwExceptionIfFailed(BufferedIterator.java:195)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:163)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$2(TestBufferedIterator.java:155)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Failing it :)
	at com.uber.hoodie.func.TestBufferedIterator.testException(TestBufferedIterator.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
4864 [pool-2-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
4866 [pool-2-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.RuntimeException: failing record reading
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$3(TestBufferedIterator.java:185)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
4910 [pool-3-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
4977 [pool-3-thread-1] ERROR com.uber.hoodie.func.BufferedIterator  - error buffering records
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.uber.hoodie.func.BufferedIterator.insertRecord(BufferedIterator.java:126)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testMemoryLimitForBuffering$1(TestBufferedIterator.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
5032 [pool-4-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
5526 [pool-4-thread-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.192 sec - in com.uber.hoodie.func.TestBufferedIterator
Running com.uber.hoodie.io.TestHoodieCompactor
9072 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 100
11547 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=29, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=39, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=32, numUpdates=0}}}
11674 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
11674 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
11674 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 29, totalInsertBuckets => 1, recordsPerBucket => 500000
11678 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
11678 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
11678 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 39, totalInsertBuckets => 1, recordsPerBucket => 500000
11679 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
11679 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
11679 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 32, totalInsertBuckets => 1, recordsPerBucket => 500000
11679 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
11679 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
11833 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 100
11835 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 100
12216 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12236 [pool-35-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
12293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
12293 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
12529 [pool-35-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
12602 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12602 [pool-36-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
12698 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
12699 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 512
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 80B for [_hoodie_commit_seqno] BINARY: 3 values, 39B raw, 41B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 56B raw, 2B comp}
Mar 19, 2020 7:51:35 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,556
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 140B for [_hoodie_commit_seqno] BINARY: 29 values, 348B raw, 98B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 760B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 661B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,442
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 158B for [_hoodie_commit_seqno] BINARY: 39 values, 474B raw, 115B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 980B for [_hoodie_record_key] BINARY: 39 values, 1,566B raw, 881B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 973B for [_row_key] BINARY: 39 values, 1,560B raw, 874B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:43 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [fare] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PA12848 [pool-36-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
12934 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
12960 [pool-37-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
12996 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
12996 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
13100 [pool-37-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
13235 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
13236 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
13462 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
13462 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
13900 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
13988 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 100 as complete
13989 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 100
14130 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 101
14798 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
14798 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
15286 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit6301705894248412217/2016/03/15/1034030a-7b81-49a2-8c47-49632c6f4848_0_100.parquet
15445 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 29 row keys from /tmp/junit6301705894248412217/2016/03/15/1034030a-7b81-49a2-8c47-49632c6f4848_0_100.parquet
15445 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit6301705894248412217/2016/03/15/1034030a-7b81-49a2-8c47-49632c6f4848_0_100.parquet => [107a3343-00ab-4fc8-aa43-defec4ceee88, 19e48893-026f-48de-ab2c-b597d36a2135, 1c51240b-a4ad-47f1-aeb5-80feddf98e83, 1dc9f7bf-e321-4ad7-aae7-b8eac4f1157d, 3894f102-a3a7-465a-b08e-82f6d09add62, 44d31d35-ca05-455a-837f-67ef6682259c, 51240a96-8d6f-48ca-a4f7-6263c4f9e5fe, 5da513d4-e6a6-46e4-9a9e-70d39f2f6432, 5e0b00b7-87ca-4e61-8bed-0889ecc4ad27, 63f06005-c1e8-45e5-b554-a42d1d7d7ee1, 703cfc8b-9946-456a-9f18-f8eea29e2e6e, 73c0bf69-cbba-4f33-b808-bdafaa033bf7, 770ad746-489b-4ce8-96f1-16caff40feac, 7fb27034-1766-4cb1-b4b1-853555f18bcc, a8253c2d-b180-4e2c-a494-9c534a4f1fd2, adf8ad92-a24e-48c7-ba20-e0d95ce7dacc, b538f471-7b59-4be6-b26f-62a7b23df88d, b84beefc-6191-404b-b98e-c473a38d1078, bdb24271-2685-43e5-a36f-b135c6c0b02e, c262a7b8-426b-407b-9a7f-bd1e73aaf99a, c45d72b8-d5da-4c55-ba0a-fe359312a015, c68e7436-5692-4247-ae90-cf6212dff10a, ce977c8b-2875-4cd3-a102-cbe8e898360e, cee407ab-cf3e-4d94-b452-92bff12d3c83, dae3c448-57f2-43df-9651-f77af27fc7ae, de422298-4673-46fc-984a-c26df0fb184f, e4f5aec5-0fec-4ce3-9559-5a628c098e54, ee1ec39e-36bf-483c-a4c2-054aa0bb9457, ffbc18a5-8871-49ae-8d8e-64dbeb11de3d]
15492 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit6301705894248412217/2015/03/17/1f118036-c7bd-4883-9a38-0083ed0d5049_2_100.parquet
15534 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 32 row keys from /tmp/junit6301705894248412217/2015/03/17/1f118036-c7bd-4883-9a38-0083ed0d5049_2_100.parquet
15534 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit6301705894248412217/2015/03/17/1f118036-c7bd-4883-9a38-0083ed0d5049_2_100.parquet => [17d9654b-9fcd-4311-8669-92aa1a082fc4, 17f82b54-0ef6-45c3-a46e-00f315db9f94, 2658f6b5-2d7f-44e5-bc04-42feabeaac0b, 2862f501-af07-4473-9e48-f503149cb7c7, 2bf280f4-0a04-48dc-8088-7e23ef14c179, 321fe94e-5215-499d-99e3-a11004e88312, 39107ce1-385c-40aa-9df0-8b0ea153973b, 3ac6b967-e5bd-42a2-a1df-8eb343eb3634, 4047cacd-7271-4152-8ced-bf6bdee7327e, 570ec087-52c4-4f44-89e4-88104c21c5ba, 64bc16c6-afeb-4e8c-9813-095df72a3977, 66da81ba-2775-44f2-b6a8-9588cf43c819, 739a3fea-3566-4759-be2a-feb2ec66b238, 9bc6a753-6bd4-426d-b1f1-e8113328284c, a0bc84a3-8ddc-4361-ba84-e215f3f3392b, a2e1e42d-1509-4a6b-88f3-3c21ffbb9045, b3963b24-206d-4d10-9218-5cbd91dcaa1a, bb792f7a-8190-41c9-8328-a3d36724805a, bce9ad2c-c308-468d-9c50-f439699bd3fd, c24e154a-817f-4b26-89ed-9e2a249a1a80, cc8e918c-42f9-422f-9949-138d668f3cda, d67990eb-c1ef-44e4-be14-61310e8c6e5c, e1e59b70-ad92-4d3d-9359-70daa7470729, e22a826d-209e-47e8-a520-b30e82d36c83, e5d9116f-06d5-44f0-9cd9-5fe0fb47c16d, e68b495e-648a-49a6-84d2-a1a3a0392259, e7b466b9-30c5-4123-badb-de75730d8543, e93e08f0-10ff-45cb-a330-7601d7dd0deb, ebef2a5d-2dd7-476f-be8c-4ac04f275860, f3a929bd-1896-44b8-99fa-57aa28226fc0, f77568be-6b42-4705-b23f-09b6afc3a71b, f9469915-ea95-4985-a6c1-dd5481a5790e]
15566 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 39 for /tmp/junit6301705894248412217/2015/03/16/274f97cf-50e1-48f2-8482-8c5b68949505_1_100.parquet
15596 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 39 row keys from /tmp/junit6301705894248412217/2015/03/16/274f97cf-50e1-48f2-8482-8c5b68949505_1_100.parquet
15596 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 39 results, for file /tmp/junit6301705894248412217/2015/03/16/274f97cf-50e1-48f2-8482-8c5b68949505_1_100.parquet => [04dada16-32fa-4860-b0cf-e0a7c74d0b3e, 1b0aad79-8b66-47e3-9f57-23dc738d236b, 1cfeb8bd-b86b-4c9e-93dd-a87a3b28a2fa, 1d2f318c-3ea7-4ee5-86b4-1116712b3f39, 2cf1198c-4753-46c7-8c28-2d9720ac0ad2, 2f96645c-4b39-4280-853d-a3f3decc5d7b, 3060de0d-e8c3-4ca8-b0fe-d9ee6d3370c7, 31eef892-377a-473d-8c13-7d13789dfd5f, 341e719c-4111-4605-a958-f75ee778decb, 48b6a287-77da-4f37-b50e-979443d0b399, 51293425-0a32-4e72-a166-60986082e03e, 53da2eb9-ea07-4cec-acb8-f6ef3a6098d8, 57657813-e4fe-46b6-9147-b84220ffd0d4, 59256c66-4b8f-4f4d-b51c-d48680d0b4d7, 5a1d5d7b-4375-4600-bc87-1379b52a508a, 5b9f7747-779a-4770-8b27-3de1a3b73ed6, 5df01791-60ff-4cfd-b665-51d5e86b65f0, 5e382e64-6eb8-4098-b4ba-212eb1affa91, 5ed93fe8-88a2-4296-94e3-5a2148423737, 69385f69-1d56-4f8d-bded-4f9109469813, 698de7e1-d4ca-47d7-8f65-dd635232ab72, 729e468c-9406-4c36-8649-7d26470614cc, 78820550-4c13-4515-a49d-8406b01467ac, 97619899-c3cb-4583-9801-261e55ed2081, a25870c6-5ef8-445a-8aaf-fc9828ce77a5, a4806645-2d07-4e8a-85ad-dc2ecf2943e9, ac521a0c-f0d6-4714-90dd-ba2a485152ae, b2b31c22-1e61-4e20-90f8-61bb19c4b084, b5dca502-dc90-40a5-b0ac-c8c82e1b849f, b7a6a5c9-0aeb-4160-90cf-1a3649b2b1ea, c25cfd8a-d054-4ed0-b980-fa777476b8b7, c850803d-d609-4a7c-9167-25f2a596875f, d04ee4ce-ce3e-4619-b156-e7ab3a2579b1, d06f34f9-eb74-4586-a21c-9c307b6c064c, d6f6034f-1c62-4f93-9589-4ee287b2847e, e337dcc9-1316-44e8-943b-dc6eb7fcd8df, eafcbd98-d79d-4c7f-b393-0964d3c25445, ee11b87a-9524-4067-9854-58550e86fd74, fa81a52b-3faf-43e5-82c1-693a2ca6506f]
CKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,130
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 149B for [_hoodie_commit_seqno] BINARY: 32 values, 394B raw, 105B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 829B for [_hoodie_record_key] BINARY: 32 values, 1,286B raw, 730B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 821B for [_row_key] BINARY: 32 values, 1,280B raw, 722B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:44 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [fare] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 28 ms. row count = 29
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 32 records.
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 32
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 39 records.
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:46 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 39
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 39 records.
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 39
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,932
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 197B for [_hoodie_commit_seqno] BINARY: 39 values, 942B raw, 130B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 980B for [_hoodie_record_key] BINARY: 39 values, 1,566B raw, 881B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 973B for [_row_key] BINARY: 39 values, 1,560B raw, 874B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:47 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [fare] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 32 records.
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 32
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,532
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 186B for [_hoodie_commit_seqno] BINARY: 32 values, 774B raw, 119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 829B for [_hoodie_record_key] BINARY: 32 values, 1,286B raw, 730B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 821B for [_row_key] BINARY: 32 values, 1,280B raw, 722B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [fare] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 181B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 114B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 760B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 661B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 39 records.
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 39
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,932
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 197B for [_hoodie_commit_seqno] BINARY: 39 values, 942B raw, 130B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 980B for [_hoodie_record_key] BINARY: 39 values, 1,566B raw, 881B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 973B for [_row_key] BINARY: 39 values, 1,560B raw, 874B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [fare] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 32 records.
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 32
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,532
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 186B for [_hoodie_commit_seqno] BINARY: 32 values, 774B raw, 119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 829B for [_hoodie_record_key] BINARY: 32 values, 1,286B raw, 730B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 821B for [_row_key] BINARY: 32 values, 1,280B raw, 722B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [fare] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 29
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 182B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 115B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 760B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 661B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:48 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 39 records.
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 39
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,932
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 197B for [_hoodie_commit_seqno] BINARY: 39 values, 942B raw, 130B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 980B for [_hoodie_record_key] BINARY: 39 values, 1,566B raw, 881B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 39 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 973B for [_row_key] BINARY: 39 values, 1,560B raw, 874B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 39 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [begin_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lat] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [end_lon] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 378B for [fare] DOUBLE: 39 values, 312B raw, 335B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetR18630 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195149
18869 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=37, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=26, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=37, numUpdates=0}}}
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 26, totalInsertBuckets => 1, recordsPerBucket => 500000
18887 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
18888 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
18888 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 37, totalInsertBuckets => 1, recordsPerBucket => 500000
18888 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
18888 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
18919 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195149
18921 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195149
19141 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19154 [pool-77-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
19178 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19198 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
ecordReader: RecordReader initialized will read a total of 32 records.
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 32
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,532
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 186B for [_hoodie_commit_seqno] BINARY: 32 values, 774B raw, 119B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 829B for [_hoodie_record_key] BINARY: 32 values, 1,286B raw, 730B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 32 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 821B for [_row_key] BINARY: 32 values, 1,280B raw, 722B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 32 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [begin_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lat] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [end_lon] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 322B for [fare] DOUBLE: 32 values, 256B raw, 279B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 29 records.
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 0 ms. row count = 29
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,932
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 183B for [_hoodie_commit_seqno] BINARY: 29 values, 702B raw, 116B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 760B for [_hoodie_record_key] BINARY: 29 values, 1,166B raw, 661B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 29 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 755B for [_row_key] BINARY: 29 values, 1,160B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 29 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [begin_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lat] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [end_lon] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:49 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 298B for [fare] DOUBLE: 29 values, 232B raw, 255B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,554
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 193B for [_hoodie_commit_seqno] BINARY: 37 values, 894B raw, 126B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 945B for [_hoodie_record_key] BINARY: 37 values, 1,486B raw, 846B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.h19299 [pool-77-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
19350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19364 [pool-78-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
19382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19391 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
19448 [pool-78-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
19532 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
19555 [pool-79-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
19574 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
19575 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
adoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 935B for [_row_key] BINARY: 37 values, 1,480B raw, 836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [fare] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5,354
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 26 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 172B for [_hoodie_commit_seqno] BINARY: 26 values, 630B raw, 105B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 697B for [_hoodie_record_key] BINARY: 26 values, 1,046B raw, 598B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 26 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 26 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 26 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 692B for [_row_key] BINARY: 26 values, 1,040B raw, 593B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 26 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 26 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 274B for [begin_lat] DOUBLE: 26 values, 208B raw, 231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 274B for [begin_lon] DOUBLE: 26 values, 208B raw, 231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 274B for [end_lat] DOUBLE: 26 values, 208B raw, 231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 274B for [end_lon] DOUBLE: 26 values, 208B raw, 231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 274B for [fare] DOUBLE: 26 values, 208B raw, 231B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7,554
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 199B for [_hoodie_commit_seqno] BINARY: 37 values, 894B raw, 132B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 944B for [_hoodie_record_key] BINARY: 37 values, 1,486B raw, 845B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 37 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 935B for [_row_key] BINARY: 37 values, 1,480B raw, 836B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B for [rider] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 24B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 89B for [driver] BINARY: 37 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 25B raw, 1B comp}
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [begin_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lat] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, enc19730 [pool-79-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
19894 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
19894 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
20203 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
20203 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
20432 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
20468 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195149 as complete
20468 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195149
20866 [main] WARN  com.uber.hoodie.io.compact.HoodieRealtimeTableCompactor  - After filtering, Nothing to compact for /tmp/junit1503571199306414122
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.385 sec - in com.uber.hoodie.io.TestHoodieCompactor
Running com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.031 sec - in com.uber.hoodie.io.strategy.TestHoodieCompactionStrategy
Running com.uber.hoodie.io.TestHoodieCommitArchiveLog
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.184 sec - in com.uber.hoodie.io.TestHoodieCommitArchiveLog
Running com.uber.hoodie.table.TestCopyOnWriteTable
24657 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 0 contains a task of very large size (152 KB). The maximum recommended task size is 100 KB.
24742 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
24743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
24743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
24743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
24743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]
24743 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.5}, WorkloadStat {bucketNumber=1, weight=0.25}, WorkloadStat {bucketNumber=2, weight=0.25}]}, 
UpdateLocations mapped to buckets =>{file1=0}
25243 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 2 contains a task of very large size (752 KB). The maximum recommended task size is 100 KB.
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=file1}, sizeBytes=819200}]
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 200 inserts to existing update bucket 0
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 2200, totalInsertBuckets => 2, recordsPerBucket => 1000
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]
25340 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.08333333333333333}, WorkloadStat {bucketNumber=1, weight=0.4583333333333333}, WorkloadStat {bucketNumber=2, weight=0.4583333333333333}]}, 
UpdateLocations mapped to buckets =>{file1=0}
26045 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
26046 [pool-110-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
odings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [end_lon] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:50 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 362B for [fare] DOUBLE: 37 values, 296B raw, 319B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,240 > 65,536: flushing 280 records to disk.
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,325
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 723B for [_hoodie_commit_seqno] BINARY: 280 values, 6,727B raw, 655B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,335B for [_hoodie_record_key] BINARY: 280 values, 11,207B raw, 6,234B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,325B for [_row_key] BINARY: 280 values, 11,200B raw, 6,224B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 280 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 504B for [number] INT32: 280 values, 1,127B raw, 468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,304 > 65,536: flushing 280 records to disk.
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,389
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 721B for [_hoodie_commit_seqno] BINARY: 280 values, 6,791B raw, 652B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,357B for [_hoodie_record_key] BINARY: 280 values, 11,207B raw, 6,256B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 280 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,348B for [_row_key] BINARY: 280 values, 11,200B raw, 6,247B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 280 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 505B for [number] INT32: 280 values, 1,127B raw, 469B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 726B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,309B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,208B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,299B for [_row_key] BINARY: 279 values, 11,160B raw, 6,198B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 503B for [number] INT32: 279 values, 1,123B raw, 467B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:57 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 726B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 656B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,314B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,213B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,305B for [_row_key] BINARY: 279 values, 11,160B raw, 6,204B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 504B for [number] INT32: 279 values, 1,123B raw, 468B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 720B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 650B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,316B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,215B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,307B for [_row_key] BINARY: 279 values, 11,160B raw, 6,206B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 503B for [number] INT32: 279 values, 1,123B raw, 467B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 731B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 661B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,312B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,211B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,303B for [_row_key] BINARY: 279 values, 11,160B raw, 6,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 502B for [number] INT32: 279 values, 1,123B raw, 466B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: mem size 65,286 > 65,536: flushing 279 records to disk.
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 39,464
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 723B for [_hoodie_commit_seqno] BINARY: 279 values, 6,982B raw, 653B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,313B for [_hoodie_record_key] BINARY: 279 values, 11,167B raw, 6,212B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 279 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 6,303B for [_row_key] BINARY: 279 values, 11,160B raw, 6,202B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 97B for [time] BINARY: 279 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 490B for [number] INT32: 279 values, 1,123B raw, 454B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6,470
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 214B for [_hoodie_commit_seqno] BINARY: 45 values, 1,131B raw, 145B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,115B for [_hoodie_record_key] BINARY: 45 values, 1,806B raw, 1,016B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 en27605 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
27605 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
27636 [pool-110-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
55ddac89-6bbb-4e3a-a29b-67a6ee33c2ce_0_20200319195156.parquet-478595
7ee73b97-c582-4345-98ad-0a0b73c225cf_0_20200319195156.parquet-478489
ee45a6e9-71c3-4aab-ae93-d4c949db88f6_0_20200319195156.parquet-451790
27968 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
27971 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
27971 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
27972 [pool-126-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
28265 [pool-126-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
28275 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
28277 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
28277 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
28292 [pool-127-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
tries, 14B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 45 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,107B for [_row_key] BINARY: 45 values, 1,800B raw, 1,008B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 45 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:58 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 135B for [number] INT32: 45 values, 186B raw, 100B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 143B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 74B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 359B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 260B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 351B for [_row_key] BINARY: 10 values, 400B raw, 252B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 159B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 62B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_row_key] BINARY: 1 values, 40B raw, 57B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 115B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 830
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 131B for [_hoodie_commit_seqno] BINARY: 5 values, 131B raw, 63B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 256B for [_hoodie_record_key] BINARY: 5 values, 206B raw, 157B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 5 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 249B for [_row_key] BINARY: 5 values, 200B raw, 150B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 5 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 71B for [number] INT32: 5 values, 26B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO28440 [pool-127-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
28928 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
28932 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
28932 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
28934 [pool-158-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
29037 [pool-158-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
29038 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
29043 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
29043 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
29064 [pool-159-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 42B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 49B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 37B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 119B for [time] BINARY: 1 values, 28B raw, 46B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:51:59 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 145B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 76B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 361B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 262B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 353B for [_row_key] BINARY: 10 values, 400B raw, 254B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 94B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 41B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 115B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 235B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 86B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1,535
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 145B for [_hoodie_commit_seqno] BINARY: 10 values, 256B raw, 76B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 358B for [_hoodie_record_key] BINARY: 10 values, 406B raw, 259B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 10 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 350B for [_row_key] BINARY: 10 values, 400B raw, 251B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 95B for [time] BINARY: 10 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 28B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [number] INT32: 10 values, 46B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 266
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 94B for [_hoodie_commit_time] BINARY: 1 values, 24B raw, 41B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 115B for [_hoodie_commit_seqno] BINARY: 1 values, 31B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 162B for [_hoodie_record_key] BINARY: 1 values, 46B raw, 64B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 83B for [_hoodie_partition_path] BINARY: 1 values, 20B raw, 38B comp, 1 pages, encodings: [BIT_PA29200 [pool-159-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
29529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
29529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
29529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 200, totalInsertBuckets => 2, recordsPerBucket => 100
29529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]
29529 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=file1}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=1, weight=0.5}, WorkloadStat {bucketNumber=2, weight=0.5}]}, 
UpdateLocations mapped to buckets =>{file1=0}
29733 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
29734 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
29734 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
29739 [pool-190-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
29804 [pool-190-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
29922 [main] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
29923 [main] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
29923 [main] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
29924 [pool-206-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
29997 [pool-206-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
CKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 234B for [_hoodie_file_name] BINARY: 1 values, 71B raw, 85B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 157B for [_row_key] BINARY: 1 values, 40B raw, 60B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 121B for [time] BINARY: 1 values, 28B raw, 48B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [number] INT32: 1 values, 10B raw, 28B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 576
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 122B for [_hoodie_commit_seqno] BINARY: 3 values, 81B raw, 54B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 56B raw, 2B comp}
Mar 19, 2020 7:52:00 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 576
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 81B for [_hoodie_commit_time] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 18B raw, 1B comp}
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 124B for [_hoodie_commit_seqno] BINARY: 3 values, 81B raw, 56B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 178B for [_hoodie_record_key] BINARY: 3 values, 126B raw, 79B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 175B for [_hoodie_file_name] BINARY: 3 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 65B raw, 1B comp}
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 174B for [_row_key] BINARY: 3 values, 120B raw, 75B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 3 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 56B raw, 2B comp}
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 66B for [number] INT32: 3 values, 18B raw, 33B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3 records.
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:01 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 3
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 3 records.
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 3
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 796
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 82B for [_hoodie_commit_time] BINARY: 4 values, 9B raw, 29B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2 entries, 36B raw, 2B comp}
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 129B for [_hoodie_commit_seqno] BINARY: 4 values, 106B raw, 61B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 181B for [_hoodie_record_key] BINARY: 4 values, 166B raw, 82B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 4 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 176B for [_hoodie_file_name] BINARY: 4 values, 9B raw, 29B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2 entries, 130B raw, 2B comp}
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 177B for [_row_key] BINARY: 4 values, 160B raw, 78B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 96B for [time] BINARY: 4 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 56B raw, 2B comp}
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [number] INT32: 4 values, 22B raw, 35B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.172 sec - in com.uber.hoodie.table.TestCopyOnWriteTable
Running com.uber.hoodie.table.TestMergeOnReadTable
Formatting using clusterid: testClusterID
33176 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
33415 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
34609 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
37004 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
37733 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
37733 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
38291 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=73, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=63, numUpdates=0}}}
38386 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
38386 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
38386 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
38387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
38397 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
38397 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
38558 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
38560 [pool-236-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
38707 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
38707 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
38808 [pool-236-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
38837 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
38850 [pool-237-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
38937 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
38938 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
39057 [pool-237-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
39087 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
39113 [pool-238-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
39143 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
39150 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
op.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 4 records.
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:02 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 4
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 13,980
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 223B for [_hoodie_commit_seqno] BINARY: 73 values, 1,029B raw, 175B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,738B for [_hoodie_record_key] BINARY: 73 values, 2,927B raw, 1,638B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 73 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,730B for [_row_key] BINARY: 73 values, 2,920B raw, 1,630B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 73 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [begin_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lat] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [end_lon] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:09 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 651B for [fare] DOUBLE: 73 values, 584B raw, 607B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,270
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [_hoodie_commit_time] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 209B for [_hoodie_commit_seqno] BINARY: 64 values, 903B raw, 161B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,537B for [_hoodie_record_key] BINARY: 64 values, 2,567B raw, 1,437B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 74B for [_hoodie_partition_path] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 154B for [_hoodie_file_name] BINARY: 64 values, 10B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [timestamp] DOUBLE: 64 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,530B for [_row_key] BINARY: 64 values, 2,560B raw, 1,430B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [rider] BINARY: 64 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [driver] BINARY: 64 values, 3B raw, 23B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [begin_lat] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [begin_lon] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [end_lat] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [end_lon] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 579B for [fare] DOUBLE: 64 values, 512B raw, 535B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,080
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 59B for [_hoodie_commit_time] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 7B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 204B for [_hoodie_commit_seqno] BINARY: 63 values, 888B raw, 157B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,508B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,409B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 153B for [_hoodie_file_name] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 54B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [time39308 [pool-238-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
39394 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
39394 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
39414 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
39415 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
39614 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
39677 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
39677 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
39812 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
40335 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 81
40335 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
40630 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit8920228410676156910/2016/03/15/4ee8fd52-1c53-48b9-901d-e783fdd83ca3_0_001.parquet
40693 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit8920228410676156910/2016/03/15/4ee8fd52-1c53-48b9-901d-e783fdd83ca3_0_001.parquet
40693 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit8920228410676156910/2016/03/15/4ee8fd52-1c53-48b9-901d-e783fdd83ca3_0_001.parquet => [025cc8b8-9f1e-44b5-8da3-46037492b7e8, 0aaeb062-fcb8-45ce-9154-dee229c8ec36, 20667c33-4591-41e3-a726-0702815c10c6, 2139257e-6530-4427-8732-0fbd4380f4ed, 2b6c26bb-ea08-4511-8242-8cf582eaae3a, 40e03426-fa42-4380-848a-cec3b6196909, 6aea9589-49a3-4664-b784-73be3b0455f0, 6deaa827-e09c-407f-a9e2-8fde967baa3c, 74b034b4-437d-4d85-ad84-20b665d059cc, 7934e1aa-318f-4c69-9d9e-2efb0bb5ce50, 7ed9338c-0969-4104-a346-5e90b31d04cd, 8dfc391c-6035-449f-ab9f-c0de4f7a3e16, a9af5c57-4010-41af-a29c-75d91e9df7d9, ab71aba4-7884-4345-b272-079384c933d2, ad333b8c-68bc-4e05-8dab-3819d5d34721, b2e0b2a7-9395-4986-b5f1-bbd07a0de3c7, b588a2be-2132-4ed7-a00f-3ae17fbb7e3c, bc5739b4-549e-4f4e-b578-57a1c40dda15, ce50a5ca-a2f7-4256-8c90-a34b9546d403, d49c3314-a726-4f88-92f8-13bcceab60a8, d811f718-6d51-4e56-985a-f91e933f77e7, db0cc0fa-25a3-45c0-8a22-ec08a811f2dc, e1fba30b-25fc-4f4c-9d7c-6b2823c69e20, ef4a34d0-7497-4f19-b8e0-b3f559cdf74a, efaf2b33-df4b-408c-a951-b3f1d4a876be, f0175008-4416-4311-8a5b-e7126f0ed9b8, f291c574-0ad9-47b3-a160-ab1459855a44, f6749f29-c84e-4a75-949c-52c70ce9e15c, fa795e21-7c49-4062-a032-895a9b0eab19, fbbe536a-ecd2-4655-b10c-4607e676bd57]
40726 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet
40777 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet
40777 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet => [02600090-cb42-41b3-a3b9-2410c8306311, 02fc3f4d-eeeb-453b-b350-4ed3de391857, 2c31d38a-b9e6-4fb8-9915-db9d7329a8ed, 4552e7c9-848e-49ba-9acc-da6c139e5e45, 462b8740-967d-48bc-9721-bee30f0a1838, 4f2584e1-02bb-432e-b949-2eb5ad42f4a0, 5b019e7a-a2df-4292-88b8-07b2b9291692, 670b6f19-cebe-428e-89f6-bd9d41013cd7, 694900bc-ece8-4c6e-a968-20f4d9739878, 992ccc80-1f4a-4bc7-a6b7-799d8e7ecb14, a18b1b70-141d-4c5c-8a92-1ca315a7c527]
40824 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet
40868 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet
40868 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit8920228410676156910/2015/03/16/7265a857-134e-4cd8-a382-04fa851713d2_1_001.parquet => [ac4002b4-6be7-4091-856b-806c812aa7fe, af3ac256-9d18-42cd-b8f2-06a45c57e20b, bbebc690-7a97-4576-941d-c10460710659, cd5486af-9adb-4517-83b2-55439c7e206f, d01cabc2-31c7-4089-997a-247093eb716a, d16ea37f-8045-4166-941d-fe1232dd1a97, e1806578-56ca-43f0-b7dd-31aca586b314, f83a9e32-ad4a-4f65-bf89-e90578615a7d, fe83cc20-74e0-49de-ad6c-f907ed1bf6cc]
40912 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit8920228410676156910/2015/03/17/b240cff9-67a5-4755-b31f-7e7422314641_2_001.parquet
40958 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit8920228410676156910/2015/03/17/b240cff9-67a5-4755-b31f-7e7422314641_2_001.parquet
40958 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit8920228410676156910/2015/03/17/b240cff9-67a5-4755-b31f-7e7422314641_2_001.parquet => [0219ff7d-e4d9-483e-94f4-aa2b7b7aef50, 077d7d23-180f-4de9-bb28-d16dab237179, 10788274-4604-4653-bc0a-0b6f3bfdff06, 229fcaea-f631-4c0d-8a42-3b99dd4a83e9, 2e1d6cf5-9c78-4647-b935-eb6ad39ee4b5, 3393cc4b-0177-46bd-83ef-4b53aee91006, 41145425-04c5-4d39-ab35-e577e61cb4a8, 48dd1af1-f7ac-4046-aba0-a66df76d0dd4, 49f80ae8-1f2e-4298-8084-c48d8c7abe46, 65cd67ce-dae3-4078-914a-37b58eb44bad, 65d3455d-5bb5-458a-ade1-1350dbfdb88e, 66522722-4c94-435c-9fc3-946b02cf758e, 763e84f9-1b42-4417-978d-1eced0ade275, 7d8e5274-fcd3-47ba-8bb5-e8572fea25d1, 808ebfb1-d0f7-4bc4-abfc-479e402234cb, 887f8a75-196c-418a-bad8-ac2ac16d7341, 88e767fd-6c4a-47cf-9b09-f24eead3ff8a, 89113684-c059-4e9b-9bf4-6cb527307350, 8d48c1f9-24f4-409f-9d43-5c46644b099b, 9321d51b-6186-48ba-acdf-1d9afb7cbb4e, 954a88cc-374b-4227-88cb-45c9b34744ce, 9742d81f-4861-4e33-8d90-a65c7b88886e, a57b8c5a-884f-432d-817a-a0c358ca930a, b7bfa475-72e9-41ca-a429-26c307537f41, b8c66951-9774-4c82-b370-95a07bc8549a, ba0e4ea1-fc7b-4418-bcc7-063ef239edc9, c6f336cf-f115-4554-8e09-d2a851004a2b, ce0b5100-a729-4287-8049-3b2f37f64cbb, ce9e5b57-502f-4be1-9a44-8fe53e51adc3, e54de410-e328-4c5b-a97d-a5380e88cbe1, f5770f27-f3c6-4c00-b8d3-3c0cf68bd653]
41059 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=81}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=20}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=31}}}
41142 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
41142 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=7265a857-134e-4cd8-a382-04fa851713d2}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b240cff9-67a5-4755-b31f-7e7422314641}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4ee8fd52-1c53-48b9-901d-e783fdd83ca3}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{7265a857-134e-4cd8-a382-04fa851713d2=0, b240cff9-67a5-4755-b31f-7e7422314641=1, 4ee8fd52-1c53-48b9-901d-e783fdd83ca3=2}
41156 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
41157 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
41291 [Executor task launch worker-3] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 7265a857-134e-4cd8-a382-04fa851713d2
41400 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file b240cff9-67a5-4755-b31f-7e7422314641
41506 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 4ee8fd52-1c53-48b9-901d-e783fdd83ca3
41656 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
41656 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
41671 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
41672 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
41866 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
41909 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
41909 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
42042 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195213
42066 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit8920228410676156910
42069 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit8920228410676156910
42192 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195213
42193 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195213
stamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,503B for [_row_key] BINARY: 63 values, 2,520B raw, 1,404B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 65B for [rider] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 13B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 67B for [driver] BINARY: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:10 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [fare] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 73 records.
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 73
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 64 records.
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 1 ms. row count = 64
Mar 19, 2020 7:52:11 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 64 records.
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 64
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:12 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 63
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: reading another 1 footers
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 63 records.
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: at row 0. reading next block
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordReader: block read in memory in 3 ms. row count = 63
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12,531
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 78B for [_hoodie_commit_time] BINARY: 63 values, 16B raw, 36B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2 entries, 25B raw, 2B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 258B for [_hoodie_commit_seqno] BINARY: 63 values, 1,229B raw, 200B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,508B for [_hoodie_record_key] BINARY: 63 values, 2,526B raw, 1,409B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [_hoodie_partition_path] BINARY: 63 values, 8B raw, 28B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 1 entries, 14B raw, 1B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 172B for [_hoodie_file_name] BINARY: 63 values, 16B raw, 36B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED, RLE], dic { 2 entries, 119B raw, 2B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 63B for [timestamp] DOUBLE: 63 values, 2B raw, 22B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 1 entries, 8B raw, 1B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 1,503B for [_row_key] BINARY: 63 values, 2,520B raw, 1,404B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 73B for [rider] BINARY: 63 values, 10B raw, 30B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 26B raw, 2B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 75B for [driver] BINARY: 63 values, 10B raw, 30B comp, 1 pages, encodings: [PLAIN_DICTIONARY, BIT_PACKED], dic { 2 entries, 28B raw, 2B comp}
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [begin_lon] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 570B for [end_lat] DOUBLE: 63 values, 504B raw, 527B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]
Mar 19, 2020 7:52:13 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written42948 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
42948 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
42965 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
42965 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
43128 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
43176 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195213 as complete
43176 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195213
49979 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
50560 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
50560 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
51010 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=63, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=73, numUpdates=0}}}
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 63, totalInsertBuckets => 1, recordsPerBucket => 500000
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 73, totalInsertBuckets => 1, recordsPerBucket => 500000
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
51047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
51081 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
51082 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
51225 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
51246 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
51246 [pool-269-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
51256 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
51441 [pool-269-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
51457 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
51475 [pool-270-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
51496 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
51496 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
51791 [pool-270-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
51809 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
51826 [pool-271-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
51853 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
51854 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
52007 [pool-271-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
52069 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
52069 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
52081 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
52081 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
52236 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
52280 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
52280 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
52367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
52841 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
52841 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
53143 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 63 for /tmp/junit9171435285653445778/2015/03/16/405d4099-3b1e-4d9c-9b65-68e4ff4f20a4_1_001.parquet
53186 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 63 row keys from /tmp/junit9171435285653445778/2015/03/16/405d4099-3b1e-4d9c-9b65-68e4ff4f20a4_1_001.parquet
53186 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 63 results, for file /tmp/junit9171435285653445778/2015/03/16/405d4099-3b1e-4d9c-9b65-68e4ff4f20a4_1_001.parquet => [0a927838-542c-422a-b902-edd4ae69b6d1, 0dedd1c9-3d1b-44e7-9f92-c79b5515f08a, 11996913-7d49-4641-a06b-f87a8f3a8476, 17492261-db66-4363-be97-d72262c55144, 2423f05e-549b-4b5a-819c-d9ab9646d9be, 25b538ca-d5b5-49e0-9915-3272560efba3, 2fa8f533-099a-4841-8a68-0657a35f4451, 3a06f33b-8131-424c-a1c2-38373dce7d3f, 3d92d51f-1707-4095-ab40-1d62f1ca03e6, 3e4d7009-ef6a-450c-88e1-c3c91a2d13bc, 3f9aca40-cdbd-4cea-b4ad-330c526593ab, 3fa28803-91bb-477a-a579-c8096c130983, 4207c719-40df-46af-b961-b630029cf58e, 4215f01e-34bc-4902-a7cd-a39abb0efe64, 4afae946-eb69-42c1-b133-ee41e51e8824, 4db741df-ba08-4fcc-932e-62416a1475b5, 4e044ddb-de94-455f-9cb6-cf23d986c23c, 57cda97b-4eec-4d4f-a729-608669e9e0f6, 58e0c376-fac0-458c-a0df-b928052b29f6, 5d780734-2397-4d39-8504-2d1752e3cba7, 6356754a-df99-49c0-8278-6b27108c07c1, 6562426c-482d-4920-ab15-754b2473309e, 6947e246-d59b-44bd-a661-35917f2796f2, 699e5aa2-8ac8-4787-a434-dc8ff4a0e961, 6c464468-b6fc-47f7-ac7d-54b81a20ac8d, 6fb17061-fb53-470e-82d5-452975d1b0b8, 70b9f540-3249-46dd-8590-210c8d298b71, 73b4b85b-aaf1-44b0-800e-c22d465ea94f, 7b9d9ff0-45ea-4560-846a-b46d6c4f5e65, 83543c67-6bee-4ef8-a28c-3e79e9d36237, 88f86f6b-af9b-4641-ad1f-37a25abc6e24, 88fca2d3-ddd1-4fbe-be51-c7ecfeb17daf, 8de086b8-fd80-4aa4-a693-2f7823038bd1, 8fc14ae6-ef4a-4c0c-845a-14dacfb5f07c, 913227d5-6cb6-41fb-a37d-7936ef34c8e0, 91a25c2e-85d3-470b-aeaf-30d2b4125cad, 95791cb8-d44c-4184-a941-394213b6a930, 9682b680-134d-4099-bbc5-50374fa07c03, 97afed5d-ebd5-4b2e-8c54-bf54e52a626a, 9c87dc1d-2dd8-4be5-b38b-334a431209d6, 9dd71479-e593-4125-8aff-57d883696a0a, ae5144b0-d433-4909-a7d7-dd9f6122494a, afad2568-9fba-475a-ba92-74ee77251676, bfc0c307-4827-4ed9-848d-23fc1796c56e, c0d35169-006c-45c7-874d-4e31c68d0775, c261a008-3790-4729-939e-212867e43883, cb56c460-6242-4b18-9f51-2513b9a89f87, cfc66780-33e3-4618-8492-26d222310c6e, da2890cd-a1bf-4a70-9531-63a63c0e6b93, db6e0b5e-3401-453e-b2c3-25d1cedb2380, dbd8333c-d309-45af-bcad-9f9c7e7e87c2, dcc6dc36-ff3f-4c73-b291-0a70e3f72f20, e27d0e01-8e1c-4165-b045-dbb752bc5d7a, eacd43eb-8fc3-4fbd-aa07-fd9c22b506a9, eaea6579-df4b-4a26-a275-a577bfcbea06, ec921e41-157e-4533-8a33-8ae27f6528d6, ef337195-d52f-47bd-88be-04ad6d68b3b0, f7314160-dcc8-419c-ba27-851006903857, f8d61e20-36b5-4ac8-9906-df16eebe35c3, f927ecc1-5885-4461-9f64-2729c603c1cc, fa97f348-5168-4fa2-95e4-7d28f89def17, fea2c72b-d787-4c99-bf6f-70dace92500a, feff4599-a244-49c9-aaaa-4d80423ed805]
53209 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet
53253 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet
53253 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet => [05f22e01-b219-43b9-9d15-cf4c4b40b931, 0bcd2a76-0614-4d9c-ac22-2690a02beb0c, 0c667db3-0b56-41e9-9425-9155ad24a813, 0e8c72af-0c44-44ba-a16e-2155314a3142, 14708288-82bf-46f0-be1d-aa63617b2e5b, 149087c9-25eb-4e63-91cf-a671aff80cb5, 1bb33535-cc65-4537-94cb-2ee76f5db50b, 202026c9-3337-43ef-8204-e60c3035df56, 217315e5-f8a9-47c3-85a2-e454de2e825e, 25ea0265-e1c0-47a8-9024-c6846808fb5e, 288615f3-6e2e-4901-b973-5c4556dfa8b7, 28b282ce-94a2-4c2a-97ac-f0c763d9617e, 29d76348-ee29-432f-b267-c0c924af2c46, 2a313a6f-8fdc-4540-8d32-fb98970d50fc, 2d31c68a-8cdd-4702-ad9f-ae81b9eec06a, 314090a0-dad8-4a34-a504-084e8c77a517, 353ab768-48e1-451a-a128-611ba25b0eda, 35dbe2dd-0817-443d-8a3b-39489e0a59bd, 362ec5e4-fd85-4c44-a2d8-d8c987fa51a1, 38ac3d78-b40c-491a-8e45-c6cd3e51517a, 3e3b4492-3222-4efe-91c1-fb937804aabf, 468e6a69-bcff-4878-a1f8-005ab8c02719, 4839da8e-3153-44b2-8c15-13858a7c1501, 529f2aea-424a-405b-ae75-d0ce0b7ea5c4, 584e09ed-c347-4959-9206-7f7db6100351, 5e6a0c22-ce67-4560-b1f2-4f5919c66097, 6108737a-4e8d-4c1e-950b-a62ff8f57285, 614f0e86-2be8-4b64-bb8d-ecf7b9f352c8, 66ac98f7-0da1-4d79-a79b-7f1794890541, 72171a71-c2a4-4d1a-ae8e-e6f45aa50eb2, 76c285b4-7d18-480d-a38e-2fbbe7b45d87, 7a00ee17-922f-4b10-8e4f-b7bbd8de220e, 804ab3b7-613a-47ec-9ae5-9c5e06bb0e67, 81b0f2ef-b536-4db2-95ac-10ecf9a95b5f]
53305 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 30 for /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet
53344 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet
53344 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 30 results, for file /tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_001.parquet => [82079c67-4393-42d3-8a2c-0cd6e21ac27e, 8619ea58-4806-4923-a17e-7c3a84344efb, 883d97d5-2d43-4511-8d01-9aa383fc311e, 8c19d019-2e56-4234-bd0d-add2e5848491, 985acb74-51f7-4d17-bbb0-e6df9cf9b22c, 99dcb22f-d61e-455e-a960-c4c2091eacd8, 9c5e2e1f-e98a-46da-953f-f7d167db6531, 9decddd3-1986-447b-8b9c-6933e2cf4111, a5aa76a8-3e02-4cc0-892d-44afbf7778d4, acd328b9-6955-472a-ad8e-c47c3f851649, b2953e7f-b487-46ed-920d-4ea7e4708c0d, b7b9ae3b-2e7b-4acb-bf47-d36a2b62d329, b8bb499b-ac6a-4146-a733-67253496f261, bf518363-2dea-404e-869e-b75dc55f9735, bfc381ef-9167-4cfb-b02c-0b08c4858213, c5dbfece-4454-4866-b7a0-9381bafdfd13, c8bd56c6-cea2-48da-8995-84972d4ddd56, c91789ac-ac13-456f-8abf-f5f545bdd772, d505ca3b-a244-4999-9156-8050ced2466a, d94d2665-4634-488d-bd3c-a796d66eaa77, d997a71a-d78b-4ccc-aabc-d2ae062b1bb8, e0d5f5f9-4d2a-46f0-a7a5-d61ac67223d9, e1d8c37a-d8c7-44bd-92b9-31c17f9d78c6, ea38e959-758e-4786-bc37-a3d9a67c696b, eab72eab-4c6b-4d7c-9948-b4937c1d89ed, ee343e61-ef75-4517-8c61-00480818487a, ef562dad-b601-4ece-8129-a8de4cb1693c, f110868e-2860-4b78-8e4d-4dacc273e6ca, f5c14a33-c5ce-46c5-9063-ad2d41802505, fd0b5833-9140-4665-af6c-fca3f952d1a7]
53378 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 73 for /tmp/junit9171435285653445778/2015/03/17/f1264f05-6c8d-4647-a3e0-5c4706a00637_2_001.parquet
53423 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 73 row keys from /tmp/junit9171435285653445778/2015/03/17/f1264f05-6c8d-4647-a3e0-5c4706a00637_2_001.parquet
53423 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 73 results, for file /tmp/junit9171435285653445778/2015/03/17/f1264f05-6c8d-4647-a3e0-5c4706a00637_2_001.parquet => [091f9926-2adc-4352-9dac-ffd0a20fd9cc, 09a79729-c1d4-4933-b103-789caadcf839, 0d02cc9b-465c-40ee-896d-fe740541a4ba, 10b5d4ee-ee61-4bd7-bda4-3a91104c8ba3, 11db6aed-c385-4806-8f39-e226f3e8e43d, 14affc0c-9c0d-444b-a763-9089662df1a8, 17c4bab5-c390-41d7-a42c-711e1eaf838d, 1c460e29-f492-4d0f-9df0-fdfdf034a2e0, 1d0bb91e-8095-413d-8f49-e9b62cb71915, 1dff6fee-dbba-4acb-a1d6-a3efc3662f13, 23779c19-d93d-4d64-965b-687a4068dec9, 29b81ab6-b561-483a-b6b3-725854281c86, 2f641bd6-1c6f-4f4f-8ac2-3d5c60f712d6, 38d9bb2a-edec-44ce-8523-5733abbaab9b, 3baa9d6d-8773-468c-abde-95cc03091426, 3e30962a-cb3d-4908-8177-623cbae93ccd, 46ee9f28-599a-41be-b911-757db3c9f43f, 492c6835-ceff-439d-996c-4e10d94e4b47, 4c1cfcc2-ec77-43b8-9363-0112aedf7537, 53c89f16-faf8-434a-89a2-49ff907e9919, 56d11c47-c3a8-4e12-8929-812ba2d13056, 57a16410-b391-4cda-830a-914162ce0463, 588c98e1-ab22-4909-986c-5d1ad0517ccb, 5c32458c-2e14-46e5-85a3-8a6556e8d6e3, 667a67cb-b52a-4427-bb37-b8f4205691ef, 708343f8-37a4-4ce1-a360-9e6339e0c2f3, 74d47ce5-504b-4651-9242-f37e9df17847, 7640261a-a1be-4ae3-975c-434301d4d111, 7a73b9de-c02d-407f-84ee-b52230d77606, 7cbde302-7847-4b33-8216-1629a5777aff, 85fb322e-6bda-4952-b06d-85ca7db4d5bf, 8e5bbb36-15eb-4def-9bdd-b8174a55bb7e, 91855a0b-cfab-427b-99c8-18d6aa6d7291, 91eb9dce-9f59-4d75-abcb-e50882c019fd, 9287688c-2b0b-4261-a5d4-74db19ab4462, 9bbe67c8-4b17-45a8-b671-676a026c0714, 9bde2308-b564-4310-9f7a-10d914b3ba30, 9fdee717-985f-49aa-9de0-dfb12bdb06bf, aabf78e4-19c7-4deb-9b0d-2ae832fb448e, ab408641-deb9-4633-972f-187073dd557f, ab5855b2-0898-49e5-bdbd-6ae29625bfde, addf0e46-0445-468c-9af4-4ceef7702da4, b9c93458-de7d-4607-bdb6-e1c5bc1ad933, baff6bb5-4cce-4fd4-9497-60ac5301e0ec, bcb2465d-75b4-4dfd-9e97-05440a82edf6, be509f70-06b5-46f0-ad57-adb58576d10f, beb5743f-6075-4460-8d28-a8b8518d7a90, bf976a9d-5f2c-4e5d-9207-f20de3173462, c0c04d61-5d9d-4249-a34a-5c3904fd9f70, c0f3ed8a-e964-4bbc-b306-8222a7e7006d, c600f70d-31e1-4648-902c-20d95678e7bb, c7bd5027-bf34-4991-a4c6-5bb314dc7c75, c7f338a7-c4b5-44b6-8b28-6f6387ac2b97, c7ff3e51-bf1f-4c94-870e-735caf419b87, c9e494b8-f465-4a93-877e-149cbb9d654a, cb1efcc3-4154-4999-879c-8d5a644f06e7, ce0f6ca0-7e31-4f93-a504-622a1428f65b, cf88784e-9ab8-49ea-ac2a-1ecea71ed228, d6a3f7a9-b31e-475e-9aaf-6b218f8ab56c, d9960bd1-826d-45a0-86e3-0fad3c659ca7, da2ea62b-f9fe-478c-9e35-5a271842646a, dbcba0c8-fb77-47ab-8ea2-5fc2ed739fbe, e481bfe0-b22f-4fcd-9449-aa3bf923f5ab, e7a1676a-b18d-46ef-9979-01d6a8586491, e8464aac-28c0-4d6e-928a-1462883aaf3a, ed9c92cf-2de4-4791-a12e-ee1d17362072, eddfcb2c-5a6f-4388-ac91-8570d1c0fc0b, f41ee086-5ea7-42b6-8399-5149295c4349, f456fe02-fc60-4d03-a992-41c4d7228f1c, f9a1e489-7bad-4cbc-a19e-2c22a4ada4c2, fa695a91-dc36-4f14-a3b3-ca095b8e74a9, fcd5bc10-0e68-4159-a138-15ce2740958b, fd46cc9e-8ce7-4160-b522-3bd46535b6dc]
53565 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=64}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=63}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=73}}}
53619 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
53619 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=4417b7d7-31b4-483d-9aef-3fc5a7faf456}, 1=BucketInfo {bucketType=UPDATE, fileLoc=405d4099-3b1e-4d9c-9b65-68e4ff4f20a4}, 2=BucketInfo {bucketType=UPDATE, fileLoc=f1264f05-6c8d-4647-a3e0-5c4706a00637}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{4417b7d7-31b4-483d-9aef-3fc5a7faf456=0, 405d4099-3b1e-4d9c-9b65-68e4ff4f20a4=1, f1264f05-6c8d-4647-a3e0-5c4706a00637=2}
53629 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
53629 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
54813 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
54813 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
54837 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
54837 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
54976 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
55024 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
55024 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
55137 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
55205 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
55216 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit9171435285653445778/2015/03/16/405d4099-3b1e-4d9c-9b65-68e4ff4f20a4_1_002.parquet	true
55221 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
55234 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit9171435285653445778/2015/03/17/f1264f05-6c8d-4647-a3e0-5c4706a00637_2_002.parquet	true
55234 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
55238 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit9171435285653445778/2016/03/15/4417b7d7-31b4-483d-9aef-3fc5a7faf456_0_002.parquet	true
55251 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
55310 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
55310 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
55579 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
56008 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
56010 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
56259 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=9, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=8, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=3, numUpdates=0}}}
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 9, totalInsertBuckets => 1, recordsPerBucket => 500000
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 8, totalInsertBuckets => 1, recordsPerBucket => 500000
56277 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
56278 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
56278 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 3, totalInsertBuckets => 1, recordsPerBucket => 500000
56278 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
56278 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
56288 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
56288 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
56421 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
56424 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
56424 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
56427 [pool-301-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
56540 [pool-301-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
56559 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
56562 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
56562 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
56562 [pool-302-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
56677 [pool-302-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
56691 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
56692 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
56692 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
56693 [pool-303-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
56813 [pool-303-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
56855 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
56855 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
56867 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
56871 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
57013 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
57053 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
57053 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
57157 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
57507 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
57508 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
57724 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet
57744 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet
57744 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet => [00673bb0-0d36-4b53-ab70-22d86fd52837, 0279cf5a-1a20-45e7-9431-e825fa5cb021, 200883bb-92c6-4d51-87bf-98fd78b7e268, 3011af20-486b-40de-bc04-2c640b88604d, 502a8552-c057-431e-a39d-d8c15cebe387, 74109295-efa7-44aa-8f17-5265a439b79d, 871fa50a-76fc-4037-a596-865d3c161ab8, 8af73f94-baca-46b2-8088-9bcea22ec2bc, 9d3eaf84-9d8e-464a-9705-cae6a8e795aa]
57770 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
57809 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
57809 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet => [16b3615b-e1c8-43d7-b559-eece4dfb1546]
57835 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
57884 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
57884 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet => [2841b827-e573-4b9e-bc25-891f5aa6aa04, 9de6655f-19f3-4023-9149-4ba2fb5be8cb, a269086c-c8a6-4d4f-b1f6-38a1251526a4, a596a511-d8ad-49b6-837f-31bc9b890ff7, bcfcfe3c-683a-4416-9c9a-5aef289291ec, cc27267f-9934-4759-9455-a5dedf969fda, cf798b5f-9b64-495d-b25f-f0863841cbd5]
57909 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet
57957 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet
57957 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet => [c9c1dd22-7add-4dc1-a463-04834fe310ff, cb5dd429-2dcb-46dc-8331-dfd2d068a8a9, e61a3040-37d3-4cde-8949-7486f66a0749]
58017 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=9}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=8}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=3}}}
58049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
58049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ecf54227-4e5b-4422-ba03-e4c1ce3fbc22}, 1=BucketInfo {bucketType=UPDATE, fileLoc=36b45f46-d695-4ff1-9311-9bd371a872a1}, 2=BucketInfo {bucketType=UPDATE, fileLoc=527cb7d5-ad22-4b21-a5a3-aab0ad94edc5}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ecf54227-4e5b-4422-ba03-e4c1ce3fbc22=0, 36b45f46-d695-4ff1-9311-9bd371a872a1=1, 527cb7d5-ad22-4b21-a5a3-aab0ad94edc5=2}
58063 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
58063 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
58169 [Executor task launch worker-3] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file ecf54227-4e5b-4422-ba03-e4c1ce3fbc22
58212 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 36b45f46-d695-4ff1-9311-9bd371a872a1
58299 [Executor task launch worker-3] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 527cb7d5-ad22-4b21-a5a3-aab0ad94edc5
58400 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
58400 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
58410 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
58410 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
58578 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
58627 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
58627 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
58712 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
59058 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 20
59058 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
59252 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet
59287 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet
59287 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet => [00673bb0-0d36-4b53-ab70-22d86fd52837, 0279cf5a-1a20-45e7-9431-e825fa5cb021, 200883bb-92c6-4d51-87bf-98fd78b7e268, 3011af20-486b-40de-bc04-2c640b88604d, 502a8552-c057-431e-a39d-d8c15cebe387, 74109295-efa7-44aa-8f17-5265a439b79d, 871fa50a-76fc-4037-a596-865d3c161ab8, 8af73f94-baca-46b2-8088-9bcea22ec2bc, 9d3eaf84-9d8e-464a-9705-cae6a8e795aa]
59307 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
59337 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
59337 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet => [16b3615b-e1c8-43d7-b559-eece4dfb1546]
59386 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
59419 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 8 row keys from /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet
59419 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet => [2841b827-e573-4b9e-bc25-891f5aa6aa04, 9de6655f-19f3-4023-9149-4ba2fb5be8cb, a269086c-c8a6-4d4f-b1f6-38a1251526a4, a596a511-d8ad-49b6-837f-31bc9b890ff7, bcfcfe3c-683a-4416-9c9a-5aef289291ec, cc27267f-9934-4759-9455-a5dedf969fda, cf798b5f-9b64-495d-b25f-f0863841cbd5]
59456 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 3 for /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet
59488 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 3 row keys from /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet
59488 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 3 results, for file /tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet => [c9c1dd22-7add-4dc1-a463-04834fe310ff, cb5dd429-2dcb-46dc-8331-dfd2d068a8a9, e61a3040-37d3-4cde-8949-7486f66a0749]
59564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=9}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=8}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=3}}}
59601 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
59601 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ecf54227-4e5b-4422-ba03-e4c1ce3fbc22}, 1=BucketInfo {bucketType=UPDATE, fileLoc=36b45f46-d695-4ff1-9311-9bd371a872a1}, 2=BucketInfo {bucketType=UPDATE, fileLoc=527cb7d5-ad22-4b21-a5a3-aab0ad94edc5}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ecf54227-4e5b-4422-ba03-e4c1ce3fbc22=0, 36b45f46-d695-4ff1-9311-9bd371a872a1=1, 527cb7d5-ad22-4b21-a5a3-aab0ad94edc5=2}
59614 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
59614 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
59755 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file ecf54227-4e5b-4422-ba03-e4c1ce3fbc22
59900 [Executor task launch worker-3] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 36b45f46-d695-4ff1-9311-9bd371a872a1
59981 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 004 for file 527cb7d5-ad22-4b21-a5a3-aab0ad94edc5
60089 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
60089 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
60103 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
60103 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
60312 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
60337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
60337 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
60644 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
60650 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
60661 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
60662 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
60672 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
60686 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
60686 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
61429 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
61751 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
61774 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/17/.ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_001.log.1] for base split hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/17/ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
61824 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
61837 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
61837 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
61839 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
61864 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
61870 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
61870 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
61879 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
61891 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
61899 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit6427108733149947416/2016/03/15/.36b45f46-d695-4ff1-9311-9bd371a872a1_001.log.1] for base split hdfs://localhost:41613/tmp/junit6427108733149947416/2016/03/15/36b45f46-d695-4ff1-9311-9bd371a872a1_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
61930 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
61943 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
61943 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
61945 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
61951 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6427108733149947416
61971 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
61971 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
61978 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
61991 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
62000 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/16/.527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_001.log.1] for base split hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/16/527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
62254 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
62613 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
62613 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
62842 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=5, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=6, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=0}}}
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 5, totalInsertBuckets => 1, recordsPerBucket => 500000
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 6, totalInsertBuckets => 1, recordsPerBucket => 500000
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 9, totalInsertBuckets => 1, recordsPerBucket => 500000
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
62868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
62885 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
62885 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
63018 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
63027 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
63027 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
63027 [pool-335-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
63105 [pool-335-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
63120 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
63122 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
63122 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
63127 [pool-336-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
63261 [pool-336-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
63287 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
63290 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
63290 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
63291 [pool-337-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
63364 [pool-337-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
63405 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
63405 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
63427 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
63427 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
63755 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
63778 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
63778 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
63875 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
64209 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 30
64212 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
64409 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6788230914459585319/2016/03/15/8e87fd06-cae5-40cf-ae91-cc4102963fe3_0_001.parquet
64426 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 5 row keys from /tmp/junit6788230914459585319/2016/03/15/8e87fd06-cae5-40cf-ae91-cc4102963fe3_0_001.parquet
64426 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6788230914459585319/2016/03/15/8e87fd06-cae5-40cf-ae91-cc4102963fe3_0_001.parquet => [1c9d10b5-8f4a-4bdb-ae0f-5720ffdb04f7, 44b51922-1bbe-45d3-a1c3-a69db11b5cfa, 882df732-80f4-4388-8ad7-efdfc5d338bc, 89586774-9818-46ce-ac16-a0097463e4f0, b979d85f-8b1c-40d2-8e50-9b441f9863da]
64441 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 4 for /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet
64459 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet
64459 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 4 results, for file /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet => [0f19b773-e0c8-4c37-9d51-4751d544f631, 2fe8a9da-6414-4f9c-acca-d00ea6fa0b5c, 4cd7a9c7-4ad3-4df0-87b2-f19920e4aa07, 59803039-d362-4464-ab4c-48f89e485b76]
64519 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 5 for /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet
64574 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 9 row keys from /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet
64574 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 5 results, for file /tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_001.parquet => [7c349ee4-c618-4c31-b569-8591ba24c72e, 82ba377b-36f5-4e4f-bd6f-ba53ce4c1758, a87df3b9-2476-4321-aa61-ca1d06d1b042, be11c3a7-7495-4f9d-afb1-36c3d39c61dc, d0618243-2dda-4652-9b65-f8ce93269fc6]
64604 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 6 for /tmp/junit6788230914459585319/2015/03/16/b65d8f7a-ca95-44a0-9712-4f9c4f37c519_1_001.parquet
64632 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 6 row keys from /tmp/junit6788230914459585319/2015/03/16/b65d8f7a-ca95-44a0-9712-4f9c4f37c519_1_001.parquet
64633 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 6 results, for file /tmp/junit6788230914459585319/2015/03/16/b65d8f7a-ca95-44a0-9712-4f9c4f37c519_1_001.parquet => [124e323f-fe86-4d87-b82d-12c2168a3e26, 22e3c463-a4b0-462f-96b7-a8d11c89b177, 5a2822af-102c-416b-857a-0cc668a63238, 5bc92636-6db2-447a-a538-144af4c420d3, 6c24dba3-eeff-4567-95bd-428e3578be91, 7a3c2c62-d922-40aa-b25f-a3193a5d676d]
64692 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=20, numUpdates=20}, partitionStat={2016/03/15=WorkloadStat {numInserts=7, numUpdates=5}, 2015/03/16=WorkloadStat {numInserts=4, numUpdates=6}, 2015/03/17=WorkloadStat {numInserts=9, numUpdates=9}}}
64712 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
64715 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=8e87fd06-cae5-40cf-ae91-cc4102963fe3}, sizeBytes=435601}]
64715 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 7 inserts to existing update bucket 0
64715 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
64720 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=b65d8f7a-ca95-44a0-9712-4f9c4f37c519}, sizeBytes=435688}]
64720 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 4 inserts to existing update bucket 1
64721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
64722 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=ab38c651-ebd1-4e61-b85f-4ba6c437ce2e}, sizeBytes=435947}]
64722 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 9 inserts to existing update bucket 2
64722 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
64722 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=8e87fd06-cae5-40cf-ae91-cc4102963fe3}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b65d8f7a-ca95-44a0-9712-4f9c4f37c519}, 2=BucketInfo {bucketType=UPDATE, fileLoc=ab38c651-ebd1-4e61-b85f-4ba6c437ce2e}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{8e87fd06-cae5-40cf-ae91-cc4102963fe3=0, b65d8f7a-ca95-44a0-9712-4f9c4f37c519=1, ab38c651-ebd1-4e61-b85f-4ba6c437ce2e=2}
64737 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
64740 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
64852 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 8e87fd06-cae5-40cf-ae91-cc4102963fe3
64861 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file 8e87fd06-cae5-40cf-ae91-cc4102963fe3
65015 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file b65d8f7a-ca95-44a0-9712-4f9c4f37c519
65015 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file b65d8f7a-ca95-44a0-9712-4f9c4f37c519
65139 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file ab38c651-ebd1-4e61-b85f-4ba6c437ce2e
65139 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Small file corrections for updates for commit 002 for file ab38c651-ebd1-4e61-b85f-4ba6c437ce2e
65326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
65326 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
65336 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
65336 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
65480 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
65519 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
65519 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
65726 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65737 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
65737 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
65738 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
65742 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65754 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
65754 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
65762 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
65783 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
65803 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41613/tmp/junit6788230914459585319/2016/03/15/8e87fd06-cae5-40cf-ae91-cc4102963fe3_0_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
65820 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65838 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
65838 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
65839 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
65841 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65862 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
65862 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
65877 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
65885 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
65899 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41613/tmp/junit6788230914459585319/2015/03/16/b65d8f7a-ca95-44a0-9712-4f9c4f37c519_1_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
65904 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65949 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
65949 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
65952 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
65956 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit6788230914459585319
65968 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
65968 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
65984 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
66000 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
66023 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [] for base split hdfs://localhost:41613/tmp/junit6788230914459585319/2015/03/17/ab38c651-ebd1-4e61-b85f-4ba6c437ce2e_2_002.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
66291 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195237
66767 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
66767 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
67122 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=56, numUpdates=0}}}
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 56, totalInsertBuckets => 1, recordsPerBucket => 500000
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
67136 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
67146 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
67249 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
67268 [pool-367-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
67307 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
67307 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
67397 [pool-367-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
67440 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
67450 [pool-368-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
67465 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
67473 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
67566 [pool-368-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
67580 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
67599 [pool-369-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
67642 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
67642 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
67746 [pool-369-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
67943 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
68457 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
68457 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
68804 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=74, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=67, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=59, numUpdates=0}}}
68818 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
68818 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
68818 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 74, totalInsertBuckets => 1, recordsPerBucket => 500000
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 67, totalInsertBuckets => 1, recordsPerBucket => 500000
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 59, totalInsertBuckets => 1, recordsPerBucket => 500000
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
68819 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
68827 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
68827 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
68939 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
68953 [pool-385-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
68998 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
68998 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
69044 [pool-385-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
69065 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
69095 [pool-386-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
69127 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
69131 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
69189 [pool-386-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
69214 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
69235 [pool-387-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
69279 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
69280 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
69343 [pool-387-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
69378 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
69378 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
69385 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
69385 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
69536 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
69580 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
69580 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
69653 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
70126 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
70126 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
70350 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 59 for /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet
70381 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet
70381 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 59 results, for file /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet => [000e7cef-1564-4773-8e1d-c8101c7ec4aa, 00a37e86-8896-4b3e-b97a-d14b3b155b4c, 0216db9d-1c93-41e8-99ea-afa17fb65e46, 02e0adba-7b81-4b84-b900-72428fee38f9, 0c149700-35cd-4efa-b5b6-4873da99751c, 0e041f8d-344c-4daf-bd87-134fd1db8926, 16abf2c8-36eb-4de9-874e-16c09454294a, 1838d876-2c65-4b11-8285-ad7e8df720a0, 1b367bbf-91e6-4732-8257-2b7c2f3b34c0, 1ba2bb59-1cd5-47c6-9c84-4f30855dc27f, 274feebd-1868-4075-848e-0cf7576f599a, 2cf74abb-25c4-45bc-bd09-e259580ee1d0, 2d2e492e-5bc5-4c03-b096-cdd4200182cb, 306961f4-ff6b-4e35-9e37-f422273244ac, 3bf8f43e-a084-4ef7-a48e-bf0a7197856f, 400824db-5333-4a7e-beef-41bafde5ebee, 46607bfb-b7ca-4167-80b8-13fb3a192c48, 49e31f91-f0fc-4406-84e9-dae892a891b7, 4b3b5ce7-107d-466a-9c15-3758b5579a29, 5468b5a5-8712-4919-a8aa-d9b6f075ba23, 57abd232-1e56-473d-8fe3-a32f74bb63bd, 5c00d241-fdb3-41cd-b5bf-d1231f35d01d, 67cca73a-7b8d-46bc-b34b-94843bba18dc, 698c781b-41ba-465b-a368-ddabd1534653, 70f4fd0a-a49b-4a50-9b7e-5df4dcf64a11, 75c46481-7717-4413-9527-ff06b5355456, 7a6d0c56-256f-4e85-90d0-22487d96200e, 81791e5c-65ba-46bf-a629-8fdd9a5768c0, 848f4cab-3c85-4bc9-8e6d-992e3e2e6808, 8ae26d6b-d63c-42fd-adde-8c12942399c5, 8d32a3de-a799-4e44-b97c-b2be14aec9a9, 95ffdafd-b911-4bab-a167-844834e15020, 9a91ad95-a0c0-43c1-acf7-82d2301aee4b, 9ab606f4-d85d-4c6c-bdb7-80ad174a1870, ad33799d-d2fd-461a-b973-2e41e7de91b0, aed8148e-5e7c-42c2-9348-df1651d9c4ff, b18524d1-b9e3-4525-b48e-10b22022eda0, b3a79a97-7639-4b34-bef9-cbb72560ab6e, b9961195-ab94-4a2f-99cc-31615b56a2bd, bb94d4dc-67e7-4d3f-b3c1-c33693959f65, bcac18ce-d27e-4a27-b84e-6df9fdee80a1, bee7f897-d051-4e1a-9cf5-77a6f4f7bcf4, c0fc3d39-b34c-4c87-adb1-ee72f052e067, c1892233-40a6-49a2-815b-a7049e6cddd0, c62d4332-9f2d-45ce-b9ba-7e11ff0926e0, c8fea043-2e63-478b-9c30-ac37a45aff3e, cdfffe5d-e822-45cb-a537-c9ec722ad12a, d2e0be3d-2dc1-4aef-8223-06db4d354260, d3f63b1a-3cee-44aa-a9de-4d3e51f654fc, d8650e07-8349-45d0-a5ea-56c0c993a6d7, df0eb186-e20f-4866-9340-12df6b7b20af, e18e4c9f-4b2a-4366-96c1-8292b3b453f0, e219635f-aad5-445b-bdfe-45497a5614af, e919ebda-72e1-4b11-802f-4dc7a7c38622, f2d3c5c3-cf3b-4a37-bec6-8d840631541f, f2fa0861-baf7-4681-bfed-5b34baa5de02, f32af2be-e36e-47be-a343-a65a0f1ceae9, f5b3088d-1211-4fa7-8d65-e88ca0d14d01, faba947e-91b6-48fa-9cff-0d1b3b58fc2e]
70409 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
70440 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
70440 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet => [03371721-2213-44af-ac3d-ab6d00e9ecc7, 19aa5b6e-7a5d-4bc7-89e0-0d2db260d2f9, 1b11d494-a27b-4433-a581-1998a4a7d07e, 28e25e83-5524-41ca-a5b2-ee6d1f6eeecb, 2a33ca87-72d3-4c9b-a1a7-818bdd4d5220, 2a85a1f9-b861-46e8-91aa-63c78870a304, 2c36bbd0-9d81-4180-8a7b-4c5029a08ea1, 2d395de6-9fb0-4c4f-87e7-ebfdc6abad87, 2fc12c38-386f-4154-8c93-dfb9250c6358, 336212f8-c4f9-43f6-9bd2-085e77d74866, 33727ce8-cb0c-469e-bd17-5af09762f147, 35132bb1-31d9-4cf9-8977-fbf68986cc9e, 376e46f8-6709-4680-919c-0ef857289813, 380d6438-68e3-46a1-9a8f-dded89f0683c, 39a7774f-5583-455e-be5d-e90ad544e7d9, 3d57dea9-dc3e-4589-b1af-a8886e628229, 3dda50a6-a215-43fa-819b-6a801f5ed5e5, 3efb7501-df86-4de7-bb20-7e9bcf2c70e0, 476e9da8-e14f-4529-b9b1-78c42d945545, 4e5e1974-f494-40ab-b2e6-a92a8edb130b, 5c4a2a77-4bd1-4b81-b92e-3ed37c5dcbcf, 5dd0b7b0-a5dd-47fc-b3a0-362353e557c4, 5f8bd3f7-bcfb-464c-84f6-3e9a947482f1, 603c58bf-9b23-4556-8959-4de4ac6508d6, 62e753f1-482e-4615-9bae-a62053dc1f76, 657bb2d5-7038-40c9-9de9-42534b942448, 68a52114-94ff-4b04-b119-da2a527d952f, 6a0fdaa4-93b5-46bb-a246-dc18ed8273c7, 6bd52823-a7a9-4d1d-a992-dfcb06ecbe62, 6c6cd69d-db48-4677-a60f-e459184ce0b3, 6e18d367-f6b7-46d0-aed1-7442aa950522, 76818be9-9629-4fa7-8883-2d100bce299c, 7b46c942-b723-48f0-b7c3-44d163ba78d4, 865617df-7e49-4e1b-a42f-3324bf9f9ae0, 87138d49-571d-4ffc-9c8d-4e3e5660d425, 873e5353-5ccc-4490-badc-a97b5e6e3012, 877683b3-212b-4975-88c8-c5713a7bc240, 8c44986e-c5a6-474f-93c0-2462f4b517cb]
70493 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
70510 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
70510 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet => [8e110046-fae2-488e-b7dc-a2fe6c70ea49, 91e7269a-abb0-44fa-9f10-1d62bc6149aa, 9959aca2-3694-4439-be23-4a36041a2afd, 9ac4ce9f-e848-4276-89a9-f6e53cb242e5, 9d5bdee9-72a7-4c76-a9a4-ac8192179d69, 9e458db7-eafd-4cbe-994e-cf3c4652dab9, a290c8ce-d4ec-4337-97d1-9a66d1321b3a, a3e17db2-b2a1-4c9d-9d3d-b61dd5bf7160, a3f947a9-8b63-4408-8a90-8f7e218c5140, ad0b3cd9-25e5-4361-a994-09ffd87b8c58, aec88a2c-5389-43e3-9a34-4e4a4ec4ec0e, b0adf2cc-5410-4189-bd99-d91babc09534, b49d60b2-ca84-4acf-abea-94d2055eeb20, b5233ab1-c727-4c8c-a272-fc6e46eb6908, b542b51e-5ffe-450a-a7a0-fd830a621e31, b6065437-a5b2-4c5d-8de0-8f7128400aac, ba359850-12ae-42c4-9f5f-f6ca297c0cba, c20e02be-6865-4896-9268-adf92faebf14, c4daad4c-4636-4cf2-97d7-4aebab112391, c6190e49-e3d8-491c-b959-a1838e6f013f, c68403fb-44a9-47b9-a80b-02e080948da8, ce5bcf53-1f3b-48bc-89d6-4dff634bcc43, cf5482c0-e2fe-4adc-bd20-d6385cc3d00f, d8c8a89c-bb7a-47c3-9aab-74a0bfecb8d9, dff50760-c169-480d-a6f6-d795a2c30f6f, e15b54c3-45ac-40ba-a549-2e49aa5de1c0, ee4ebec3-20f0-4014-a903-4074c2370e05, f33fb28a-72de-47ab-9693-a683a8e6f4eb, f557c132-2730-4bea-a046-7d1da743ea4e]
70530 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 74 for /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet
70558 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet
70558 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 74 results, for file /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet => [02e042b5-0b55-4798-bd7a-f918f18ca3f8, 0c5f8413-4642-41cb-9d1a-acd8fe31c2d8, 0ea251fc-73d0-48b3-8d5e-cfa9b08f9091, 1651c01f-f125-41a4-a8a6-def5f19d56c7, 1bad9297-6de0-4c32-bff1-9e92621cc3e4, 1d54ca7e-7e5d-4126-b524-8e5b2c19d9a1, 241eee04-aaf2-472b-a336-11e9aec322e5, 27e316c5-4fd1-4782-bbb7-edee780862ff, 27f11fd1-2e6e-430c-b9aa-e86742b80a2c, 2be48f35-cf22-469a-a5e8-3c628a894109, 2d728952-4611-46bb-8cca-e998132d344b, 32388f42-f770-41c8-ad2d-07fc4e44f539, 33bbcf6e-5fed-4bce-aa0a-e9bd70a6f49e, 367fdb93-b37a-4fd0-bb78-5b293cd2bf40, 38772d74-f325-4161-8641-e2817374a403, 3892112a-77f8-4cce-b048-5cd7333b8f1c, 3d684060-6342-4d7b-9b19-2290b4d29f98, 408c736f-8934-48c0-bf2c-f0c62849c8fa, 4564d041-565a-440d-bd03-1450553ff99c, 48dc6bfe-790a-4825-bda3-78b29363315d, 4bf95dbf-d116-490d-8ea2-f1942a282527, 4c895e05-24b2-4c83-83dd-a76a4f83ecd9, 4d804853-6828-4b7f-8e4e-3ec7344c282b, 4df1ecd2-fb0d-4333-b5ce-bf9b9c86d114, 4e1483e3-106e-4903-b6c4-c3b585e61978, 51091d59-0ea2-4c65-a50c-3930101dc44f, 53ad6efb-fa03-4b8f-b96f-d98d1c18cf03, 5e3ca3eb-fda7-4cc2-9086-c5b621a49abe, 60641e70-b7a5-4b71-acf5-41ba7e96d571, 67f2acdc-aba5-49ed-9c36-4d3d66e7b977, 6b9293df-b585-432e-a364-f04c6a53682e, 767662ba-2a8f-461c-a323-dd644aada95e, 780fb18f-2582-4bbb-9c1f-0b151972d49b, 7860bd68-5831-4fbb-bb60-f454f3ef4706, 794d89d6-145d-4755-9e74-40fd2d097fca, 7a24abc1-ed68-4a54-b031-e53638299f37, 7e0a5647-bf2c-4739-8220-17175262fc5d, 83f34856-6c18-44f6-97cb-8c21e1b0515e, 894102dd-b5ca-4dfe-b832-f13520e0d2e7, 8ce5fee4-2279-4b0a-bffd-5a6ce333b00e, 8df57a09-d8a4-40e5-b0b7-bbbb9b50a78d, 9276ea58-a840-4a17-9e86-026e82fa20ec, 94e11607-2ab5-4d0d-ba35-0a492f4e0968, 9687c273-8228-498b-9aaf-96e423871f4b, 96a9b6d5-1a10-4138-b2e0-de7afa8d2f16, 9cd55783-31ca-4000-b75c-c0da31cb284c, a7b3fd0b-5957-4e57-af74-ce1932674f62, a9d9809b-4193-41e9-acbb-c2a47f028517, aa3f4539-829a-45d7-8322-8ac378596127, ac54164b-4abb-45f9-aedd-dffba6e6e422, b4bc17d7-01f8-4a3d-8443-9c83768c42db, b4be480f-cdb0-4d15-9ed9-d95c899b1be1, beaf8204-5d23-4909-a32e-163dc1e8c415, c438b71d-7408-4cab-8737-f837174ad35b, c52834aa-983b-45df-8799-6a663e018bdf, c5f50624-7471-423e-bd9d-85bdac95c27f, c6e1283e-bb64-4c94-9af0-02f372f37c7f, cc6d4dd8-9ffa-47dd-9d54-a4652e0b0bd6, d15e930f-3d28-47d2-8ca0-aa6a312b29cb, db907ed5-bdfe-413e-8681-ad811eb0d8b3, dfdfab29-e85a-4f54-8cdb-61a14ac895b8, e0597230-6c48-4d7d-8580-b067bc7b7964, e6d91051-3e77-43cf-9a72-ec3f07be97d2, e8cf8f73-fd80-4af6-9297-4859b705a58c, e8f12f94-b6a6-42f8-9218-c63a8fc69ba2, ee0124c7-a76b-44bf-a76f-cdc9f6a95635, f035382d-9548-4ce3-aec9-3e70601eda99, f077c266-c634-4d95-9851-c6d4d76cdf51, f7de9e81-68f0-4f51-96d8-5685a3a971ae, fceb67ad-77b8-4831-98da-8af2a955f498, fdb32efb-04a5-4947-9f06-c4e67053ec4a, fe1d7252-c175-4d73-bfbb-d3da4dd937f3, ff1dbc32-93b6-41c2-92cf-2602775ce490, ff930e0d-9d94-46b3-93c2-78b460e5555b]
70724 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=74}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=67}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=59}}}
70751 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
70751 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=5a885240-ee0a-4778-975d-a3808817b4f2}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e}, 2=BucketInfo {bucketType=UPDATE, fileLoc=0e87a203-33ec-4ff4-8eba-c78ad81d77fc}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{5a885240-ee0a-4778-975d-a3808817b4f2=0, dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e=1, 0e87a203-33ec-4ff4-8eba-c78ad81d77fc=2}
70764 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
70764 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
70877 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 5a885240-ee0a-4778-975d-a3808817b4f2
70991 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e
71073 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 002 for file 0e87a203-33ec-4ff4-8eba-c78ad81d77fc
71168 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
71168 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
71178 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
71178 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
71334 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
71367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
71367 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
71534 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71551 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
71551 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
71554 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
71564 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71569 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
71569 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71577 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
71594 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71614 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/.5a885240-ee0a-4778-975d-a3808817b4f2_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
71684 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71691 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
71691 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
71692 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
71706 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71713 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
71713 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71719 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
71733 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71756 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/.dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
71821 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71851 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
71851 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
71853 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
71855 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
71859 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
71859 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71872 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
71880 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
71894 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/.0e87a203-33ec-4ff4-8eba-c78ad81d77fc_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
71968 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [002]
72097 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
72216 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
72279 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
72307 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [002]
72329 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [002] rollback is complete
72329 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
72408 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72415 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
72416 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
72422 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
72428 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72443 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
72443 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72476 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
72496 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72520 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/.5a885240-ee0a-4778-975d-a3808817b4f2_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
72557 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72582 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
72583 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
72584 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
72586 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72597 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
72597 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72602 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
72613 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72637 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/.dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
72664 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72667 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Found a total of 1 groups
72667 [main] INFO  com.uber.hoodie.hadoop.HoodieHiveUtil  - hoodie.raw_trips.consume.mode: LATEST
72668 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Total paths to process after hoodie filter 1
72670 [main] INFO  com.uber.hoodie.hadoop.HoodieInputFormat  - Reading hoodie metadata from path hdfs://localhost:41613/tmp/junit3075402958548666369
72683 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Returning a total splits of 1
72683 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat  - Creating record reader with readCols :_hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72691 [main] WARN  parquet.hadoop.ParquetRecordReader  - Can not initialize counter due to context is not a instance of TaskInputOutputContext, but is org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
72698 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - cfg ==> _hoodie_commit_time,_hoodie_commit_seqno,_hoodie_record_key,_hoodie_partition_path,_hoodie_file_name,timestamp,_row_key,rider,driver,begin_lat,begin_lon,end_lat,end_lon,fare
72711 [main] INFO  com.uber.hoodie.hadoop.realtime.HoodieRealtimeRecordReader  - About to read compacted logs [hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/.0e87a203-33ec-4ff4-8eba-c78ad81d77fc_001.log.1] for base split hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet, projecting cols [_hoodie_commit_time, _hoodie_commit_seqno, _hoodie_record_key, _hoodie_partition_path, _hoodie_file_name, timestamp, _row_key, rider, driver, begin_lat, begin_lon, end_lat, end_lon, fare]
72718 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
72818 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 61 contains a task of very large size (118 KB). The maximum recommended task size is 100 KB.
73178 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 180
73179 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
73415 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 51 for /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet
73443 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 59 row keys from /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet
73443 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 51 results, for file /tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_001.parquet => [000e7cef-1564-4773-8e1d-c8101c7ec4aa, 0216db9d-1c93-41e8-99ea-afa17fb65e46, 02e0adba-7b81-4b84-b900-72428fee38f9, 0c149700-35cd-4efa-b5b6-4873da99751c, 0e041f8d-344c-4daf-bd87-134fd1db8926, 16abf2c8-36eb-4de9-874e-16c09454294a, 1b367bbf-91e6-4732-8257-2b7c2f3b34c0, 274feebd-1868-4075-848e-0cf7576f599a, 2cf74abb-25c4-45bc-bd09-e259580ee1d0, 2d2e492e-5bc5-4c03-b096-cdd4200182cb, 306961f4-ff6b-4e35-9e37-f422273244ac, 3bf8f43e-a084-4ef7-a48e-bf0a7197856f, 400824db-5333-4a7e-beef-41bafde5ebee, 46607bfb-b7ca-4167-80b8-13fb3a192c48, 4b3b5ce7-107d-466a-9c15-3758b5579a29, 5468b5a5-8712-4919-a8aa-d9b6f075ba23, 57abd232-1e56-473d-8fe3-a32f74bb63bd, 5c00d241-fdb3-41cd-b5bf-d1231f35d01d, 67cca73a-7b8d-46bc-b34b-94843bba18dc, 698c781b-41ba-465b-a368-ddabd1534653, 70f4fd0a-a49b-4a50-9b7e-5df4dcf64a11, 75c46481-7717-4413-9527-ff06b5355456, 7a6d0c56-256f-4e85-90d0-22487d96200e, 81791e5c-65ba-46bf-a629-8fdd9a5768c0, 848f4cab-3c85-4bc9-8e6d-992e3e2e6808, 8ae26d6b-d63c-42fd-adde-8c12942399c5, 8d32a3de-a799-4e44-b97c-b2be14aec9a9, 95ffdafd-b911-4bab-a167-844834e15020, 9ab606f4-d85d-4c6c-bdb7-80ad174a1870, ad33799d-d2fd-461a-b973-2e41e7de91b0, aed8148e-5e7c-42c2-9348-df1651d9c4ff, b18524d1-b9e3-4525-b48e-10b22022eda0, b3a79a97-7639-4b34-bef9-cbb72560ab6e, b9961195-ab94-4a2f-99cc-31615b56a2bd, bb94d4dc-67e7-4d3f-b3c1-c33693959f65, bcac18ce-d27e-4a27-b84e-6df9fdee80a1, c1892233-40a6-49a2-815b-a7049e6cddd0, c62d4332-9f2d-45ce-b9ba-7e11ff0926e0, c8fea043-2e63-478b-9c30-ac37a45aff3e, cdfffe5d-e822-45cb-a537-c9ec722ad12a, d2e0be3d-2dc1-4aef-8223-06db4d354260, d3f63b1a-3cee-44aa-a9de-4d3e51f654fc, d8650e07-8349-45d0-a5ea-56c0c993a6d7, df0eb186-e20f-4866-9340-12df6b7b20af, e18e4c9f-4b2a-4366-96c1-8292b3b453f0, e219635f-aad5-445b-bdfe-45497a5614af, e919ebda-72e1-4b11-802f-4dc7a7c38622, f2d3c5c3-cf3b-4a37-bec6-8d840631541f, f32af2be-e36e-47be-a343-a65a0f1ceae9, f5b3088d-1211-4fa7-8d65-e88ca0d14d01, faba947e-91b6-48fa-9cff-0d1b3b58fc2e]
73460 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
73483 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
73483 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet => [03371721-2213-44af-ac3d-ab6d00e9ecc7, 19aa5b6e-7a5d-4bc7-89e0-0d2db260d2f9, 1b11d494-a27b-4433-a581-1998a4a7d07e, 28e25e83-5524-41ca-a5b2-ee6d1f6eeecb, 2a33ca87-72d3-4c9b-a1a7-818bdd4d5220, 2a85a1f9-b861-46e8-91aa-63c78870a304, 2c36bbd0-9d81-4180-8a7b-4c5029a08ea1, 2d395de6-9fb0-4c4f-87e7-ebfdc6abad87, 2fc12c38-386f-4154-8c93-dfb9250c6358, 336212f8-c4f9-43f6-9bd2-085e77d74866, 33727ce8-cb0c-469e-bd17-5af09762f147, 376e46f8-6709-4680-919c-0ef857289813, 380d6438-68e3-46a1-9a8f-dded89f0683c, 39a7774f-5583-455e-be5d-e90ad544e7d9, 3d57dea9-dc3e-4589-b1af-a8886e628229, 3dda50a6-a215-43fa-819b-6a801f5ed5e5, 3efb7501-df86-4de7-bb20-7e9bcf2c70e0, 476e9da8-e14f-4529-b9b1-78c42d945545, 4e5e1974-f494-40ab-b2e6-a92a8edb130b, 5c4a2a77-4bd1-4b81-b92e-3ed37c5dcbcf, 5dd0b7b0-a5dd-47fc-b3a0-362353e557c4, 5f8bd3f7-bcfb-464c-84f6-3e9a947482f1, 603c58bf-9b23-4556-8959-4de4ac6508d6, 62e753f1-482e-4615-9bae-a62053dc1f76, 657bb2d5-7038-40c9-9de9-42534b942448, 68a52114-94ff-4b04-b119-da2a527d952f, 6a0fdaa4-93b5-46bb-a246-dc18ed8273c7, 6bd52823-a7a9-4d1d-a992-dfcb06ecbe62, 6c6cd69d-db48-4677-a60f-e459184ce0b3, 6e18d367-f6b7-46d0-aed1-7442aa950522, 76818be9-9629-4fa7-8883-2d100bce299c]
73552 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
73582 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet
73582 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_001.parquet => [7b46c942-b723-48f0-b7c3-44d163ba78d4, 865617df-7e49-4e1b-a42f-3324bf9f9ae0, 87138d49-571d-4ffc-9c8d-4e3e5660d425, 873e5353-5ccc-4490-badc-a97b5e6e3012, 877683b3-212b-4975-88c8-c5713a7bc240, 8c44986e-c5a6-474f-93c0-2462f4b517cb, 8e110046-fae2-488e-b7dc-a2fe6c70ea49, 91e7269a-abb0-44fa-9f10-1d62bc6149aa, 9ac4ce9f-e848-4276-89a9-f6e53cb242e5, 9d5bdee9-72a7-4c76-a9a4-ac8192179d69, 9e458db7-eafd-4cbe-994e-cf3c4652dab9, a290c8ce-d4ec-4337-97d1-9a66d1321b3a, a3f947a9-8b63-4408-8a90-8f7e218c5140, ad0b3cd9-25e5-4361-a994-09ffd87b8c58, aec88a2c-5389-43e3-9a34-4e4a4ec4ec0e, b0adf2cc-5410-4189-bd99-d91babc09534, b49d60b2-ca84-4acf-abea-94d2055eeb20, b5233ab1-c727-4c8c-a272-fc6e46eb6908, b542b51e-5ffe-450a-a7a0-fd830a621e31, ba359850-12ae-42c4-9f5f-f6ca297c0cba, c20e02be-6865-4896-9268-adf92faebf14, c6190e49-e3d8-491c-b959-a1838e6f013f, c68403fb-44a9-47b9-a80b-02e080948da8, ce5bcf53-1f3b-48bc-89d6-4dff634bcc43, cf5482c0-e2fe-4adc-bd20-d6385cc3d00f, d8c8a89c-bb7a-47c3-9aab-74a0bfecb8d9, dff50760-c169-480d-a6f6-d795a2c30f6f, e15b54c3-45ac-40ba-a549-2e49aa5de1c0, ee4ebec3-20f0-4014-a903-4074c2370e05, f33fb28a-72de-47ab-9693-a683a8e6f4eb, f557c132-2730-4bea-a046-7d1da743ea4e]
73607 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 67 for /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet
73638 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet
73638 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 67 results, for file /tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_001.parquet => [02e042b5-0b55-4798-bd7a-f918f18ca3f8, 1651c01f-f125-41a4-a8a6-def5f19d56c7, 1bad9297-6de0-4c32-bff1-9e92621cc3e4, 1d54ca7e-7e5d-4126-b524-8e5b2c19d9a1, 241eee04-aaf2-472b-a336-11e9aec322e5, 27e316c5-4fd1-4782-bbb7-edee780862ff, 27f11fd1-2e6e-430c-b9aa-e86742b80a2c, 2be48f35-cf22-469a-a5e8-3c628a894109, 2d728952-4611-46bb-8cca-e998132d344b, 32388f42-f770-41c8-ad2d-07fc4e44f539, 33bbcf6e-5fed-4bce-aa0a-e9bd70a6f49e, 367fdb93-b37a-4fd0-bb78-5b293cd2bf40, 38772d74-f325-4161-8641-e2817374a403, 3892112a-77f8-4cce-b048-5cd7333b8f1c, 3d684060-6342-4d7b-9b19-2290b4d29f98, 408c736f-8934-48c0-bf2c-f0c62849c8fa, 4564d041-565a-440d-bd03-1450553ff99c, 48dc6bfe-790a-4825-bda3-78b29363315d, 4bf95dbf-d116-490d-8ea2-f1942a282527, 4c895e05-24b2-4c83-83dd-a76a4f83ecd9, 4d804853-6828-4b7f-8e4e-3ec7344c282b, 4df1ecd2-fb0d-4333-b5ce-bf9b9c86d114, 4e1483e3-106e-4903-b6c4-c3b585e61978, 51091d59-0ea2-4c65-a50c-3930101dc44f, 53ad6efb-fa03-4b8f-b96f-d98d1c18cf03, 5e3ca3eb-fda7-4cc2-9086-c5b621a49abe, 60641e70-b7a5-4b71-acf5-41ba7e96d571, 67f2acdc-aba5-49ed-9c36-4d3d66e7b977, 6b9293df-b585-432e-a364-f04c6a53682e, 767662ba-2a8f-461c-a323-dd644aada95e, 7860bd68-5831-4fbb-bb60-f454f3ef4706, 794d89d6-145d-4755-9e74-40fd2d097fca, 7a24abc1-ed68-4a54-b031-e53638299f37, 7e0a5647-bf2c-4739-8220-17175262fc5d, 83f34856-6c18-44f6-97cb-8c21e1b0515e, 894102dd-b5ca-4dfe-b832-f13520e0d2e7, 8ce5fee4-2279-4b0a-bffd-5a6ce333b00e, 8df57a09-d8a4-40e5-b0b7-bbbb9b50a78d, 9276ea58-a840-4a17-9e86-026e82fa20ec, 94e11607-2ab5-4d0d-ba35-0a492f4e0968, 9687c273-8228-498b-9aaf-96e423871f4b, 96a9b6d5-1a10-4138-b2e0-de7afa8d2f16, 9cd55783-31ca-4000-b75c-c0da31cb284c, a9d9809b-4193-41e9-acbb-c2a47f028517, aa3f4539-829a-45d7-8322-8ac378596127, ac54164b-4abb-45f9-aedd-dffba6e6e422, b4bc17d7-01f8-4a3d-8443-9c83768c42db, b4be480f-cdb0-4d15-9ed9-d95c899b1be1, beaf8204-5d23-4909-a32e-163dc1e8c415, c438b71d-7408-4cab-8737-f837174ad35b, c52834aa-983b-45df-8799-6a663e018bdf, c5f50624-7471-423e-bd9d-85bdac95c27f, c6e1283e-bb64-4c94-9af0-02f372f37c7f, cc6d4dd8-9ffa-47dd-9d54-a4652e0b0bd6, d15e930f-3d28-47d2-8ca0-aa6a312b29cb, db907ed5-bdfe-413e-8681-ad811eb0d8b3, dfdfab29-e85a-4f54-8cdb-61a14ac895b8, e0597230-6c48-4d7d-8580-b067bc7b7964, e6d91051-3e77-43cf-9a72-ec3f07be97d2, e8cf8f73-fd80-4af6-9297-4859b705a58c, e8f12f94-b6a6-42f8-9218-c63a8fc69ba2, ee0124c7-a76b-44bf-a76f-cdc9f6a95635, f035382d-9548-4ce3-aec9-3e70601eda99, f077c266-c634-4d95-9851-c6d4d76cdf51, f7de9e81-68f0-4f51-96d8-5685a3a971ae, fdb32efb-04a5-4947-9f06-c4e67053ec4a, ff930e0d-9d94-46b3-93c2-78b460e5555b]
73776 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=180}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=67}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=62}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=51}}}
73821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
73821 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=5a885240-ee0a-4778-975d-a3808817b4f2}, 1=BucketInfo {bucketType=UPDATE, fileLoc=dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e}, 2=BucketInfo {bucketType=UPDATE, fileLoc=0e87a203-33ec-4ff4-8eba-c78ad81d77fc}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{5a885240-ee0a-4778-975d-a3808817b4f2=0, dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e=1, 0e87a203-33ec-4ff4-8eba-c78ad81d77fc=2}
73840 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
73841 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
73940 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 5a885240-ee0a-4778-975d-a3808817b4f2
74090 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e
74155 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Merging updates for commit 003 for file 0e87a203-33ec-4ff4-8eba-c78ad81d77fc
74309 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
74309 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
74315 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
74315 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
74476 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
74494 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
74494 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
74556 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195245
74566 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Checking if compaction needs to be run on /tmp/junit3075402958548666369
74566 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Compacting merge on read table /tmp/junit3075402958548666369
74688 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195245
74692 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195245
75210 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
75210 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
75231 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
75231 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
75364 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
75376 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195245 as complete
75376 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195245
75402 [main] INFO  com.uber.hoodie.table.HoodieMergeOnReadTable  - Unpublished [20200319195245]
75463 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
75467 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/5a885240-ee0a-4778-975d-a3808817b4f2_1_20200319195245.parquet	true
75467 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
75470 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/0e87a203-33ec-4ff4-8eba-c78ad81d77fc_2_20200319195245.parquet	true
75470 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
75471 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_0_20200319195245.parquet	true
75476 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20200319195245]
75486 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20200319195245] rollback is complete
75486 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
75535 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
75647 [550285686@qtp-1409952627-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35049] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
75647 [550285686@qtp-1409952627-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35049] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
75754 [DataNode: [[[DISK]file:/tmp/1584643922518-0/dfs/data/data1/, [DISK]file:/tmp/1584643922518-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41613] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1090522349-130.239.242.71-1584643923866 (Datanode Uuid c6d148ce-c44b-44d8-a693-0907c88e4f10) service to localhost/127.0.0.1:41613 interrupted
75754 [DataNode: [[[DISK]file:/tmp/1584643922518-0/dfs/data/data1/, [DISK]file:/tmp/1584643922518-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:41613] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1090522349-130.239.242.71-1584643923866 (Datanode Uuid c6d148ce-c44b-44d8-a693-0907c88e4f10) service to localhost/127.0.0.1:41613
75768 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@1816047e] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
Tests run: 7, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 44.496 sec - in com.uber.hoodie.table.TestMergeOnReadTable
Running com.uber.hoodie.config.HoodieWriteConfigTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.uber.hoodie.config.HoodieWriteConfigTest
Running com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
75988 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
76073 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
76087 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
76087 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
76087 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
76087 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
76087 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
76096 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
76096 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
76167 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
76185 [pool-425-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
76249 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
76260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
76348 [pool-425-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
76412 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
76412 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
76550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
76550 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
76646 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
76654 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
76654 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
76721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
76774 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=0}}}
76787 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4438
76788 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04}, sizeBytes=443773}]
76789 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to new update bucket 0
76789 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
76789 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04=0}
76811 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
76812 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
77129 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
77129 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
77367 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
77367 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
77457 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
77470 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
77470 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
77562 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
77672 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=200, numUpdates=0}}}
77720 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 3195
77721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=002, fileId=98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04}, sizeBytes=447195}]
77721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 180 inserts to new update bucket 0
77721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 20, totalInsertBuckets => 1, recordsPerBucket => 100
77721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]
77721 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :2, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]}, 
UpdateLocations mapped to buckets =>{98dcf3f7-ebf4-4eed-bb5b-62776f4a3b04=0}
77739 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
77748 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
78239 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
78241 [pool-436-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
78246 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
78248 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
78291 [pool-436-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
78313 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
78313 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
78377 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
78377 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
78481 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
78492 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
78492 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
78699 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195249
78972 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=175, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=157, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=168, numUpdates=0}}}
78991 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 175, totalInsertBuckets => 1, recordsPerBucket => 500000
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 157, totalInsertBuckets => 1, recordsPerBucket => 500000
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 168, totalInsertBuckets => 1, recordsPerBucket => 500000
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
78992 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
79002 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195249
79002 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195249
79177 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
79204 [pool-456-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
79289 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
79289 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
79337 [pool-456-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
79365 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
79396 [pool-457-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
79490 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
79490 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
79562 [pool-457-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
79580 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
79593 [pool-458-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
79669 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
79669 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
79716 [pool-458-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
79744 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
79744 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
79833 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
79833 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
79975 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
80001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195249 as complete
80001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195249
80108 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
80340 [dispatcher-event-loop-0] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
80352 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
80352 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
80427 [dispatcher-event-loop-0] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
80441 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 16 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
80559 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 175 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet
80588 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet
80588 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 175 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet => [03bd632b-c594-4ae5-a56b-7b67e1985cc3, 062248f7-1122-4364-b861-bf4a6aab7877, 070bf235-03fd-4423-9406-c5bc1531178c, 07156029-0e04-4968-818b-2d222135e0a9, 0ad3e1ba-0a6b-42fa-8bd8-7cdc98b41533, 0ee52ff8-1b24-4c6f-a869-8eec986e9053, 0f0cf2a4-bfcf-422d-842e-fbbd86d27abc, 11374b07-6595-47ca-a958-d1060808a8bd, 129be2ed-dc5e-42ae-9be7-f036e19c799b, 12cb3d70-b179-41c4-b6d6-5fbb2d547a6d, 13f3e120-785f-47bc-a923-cb3df3fc8f42, 147468fb-c9d6-4ef5-9e5a-ce50c51b674c, 15da68c2-ae2f-4e37-a981-3e44664a51ed, 17373b30-b428-421a-a1fe-aa991e3c9c38, 17c45908-c223-4593-b166-d720e36e4719, 187fc44c-4a37-4475-b0fb-d1fb5e84a149, 1c19d75e-3b81-41e1-8756-4a94f67276e1, 1d0e8a8b-7bd2-4ef0-98fc-0f0c12de3b15, 1d832a92-381b-4b58-9aab-128713c9e7c9, 1da70b92-f7ca-4b0e-b257-68f92cbb0105, 2122117f-c102-465c-a4c4-7f011f27a4de, 213aff1f-db05-46d4-9267-01b0f8e7a07f, 21553c7d-6836-427d-aa77-c89e36d9431f, 21b318ce-f19f-4cad-8932-b08f4d7acd6e, 22109158-b25c-457c-b3b4-2f3a6dfb1cee, 232c9e16-b138-45ca-8271-ddd571d4ec5e, 238b92b4-e925-4414-80bf-210ac380b9ec, 2463545a-b57b-4156-9683-c4ec93a321b9, 24ff1716-a84d-464a-a433-27f17ac360d7, 2592bbd2-e81e-4000-abe4-b3a519bf9022, 26ba0f07-9ef2-4ea6-9535-243a8f6cd126, 27fac0f6-6093-4e94-9e98-2be473287609, 293ff62a-c92b-436d-b162-d85e96c4bdad, 2c089b8c-9692-48d0-85e8-0b217ea482ce, 2dcda651-f564-4734-b920-e354cc7dce46, 2e6b48e5-f2d2-4e6e-a820-ea4d3ff9c1cb, 3118f32d-895a-4bf1-a408-da968bf56fa6, 330ee1ac-a7b0-4189-84bf-139d3ee17416, 33a824e3-e930-41a8-bf4b-e280e2ef08eb, 3692ed00-3b6d-4d6f-aac9-9329459953ae, 375d6489-0568-4d17-85f2-3d60db6ee7a6, 38db06dd-8011-4e02-8723-26d03757524e, 3e942e1a-b476-4b3d-979a-d78c628fd18e, 3f7e194a-bc06-42d5-9fce-f57a9630eee3, 42614694-f4e3-4e3f-9c34-ea3b63d6a5e5, 47d745cf-1524-4034-875b-07c1719fa3a9, 4a706413-6c00-4e6b-8629-43cade892efa, 4a85e18b-e0c6-4bf8-9b30-bd4f386cee92, 4a9c5504-01b3-4153-91ce-ca9e74369447, 4b879c20-52e3-401c-9f4f-a0403e70943e, 4c69fa64-3a56-4aa4-9fc8-dd71edf3636b, 4cfd7357-7945-46df-af1b-78e44c3d82c4, 4e83ffdb-ff1c-4333-bd2d-794877affdeb, 4fdcdae3-e455-448e-a903-8d8a8b727ce2, 511c9b62-5ed3-4905-adbd-49bda4c45d58, 51afd4d2-c612-463e-81ca-e9416173d57a, 54883d8b-16d0-4d30-977f-759a7dceef5b, 5568a6df-8803-420e-b438-b76bca80873e, 55a6f703-623e-49ff-b537-1b790a4060c9, 583c9fb1-42ae-4446-be2f-7d0e20c64cb8, 59045f07-dfd0-4e5c-a716-b2d9e5997f53, 5aa168ac-40dd-436d-88a7-92f195560cb6, 5bd77f51-ab84-4897-bc85-a9cabbda8e29, 5d670d8d-b722-44b3-bd20-ed7c7b84702a, 5e2b1950-29fb-442c-a5ab-8dcad99da699, 5f0fe5eb-7e4f-4812-aeeb-ac8344ccec5d, 5f354e32-ee6e-43c5-8083-2ffc89d1831c, 63274066-2ffd-4286-81e5-de948a0d58ee, 632fd36d-bef5-48d0-b5c4-25eb88c3ab20, 6422caf3-342a-49ed-aa47-54f4583a8c5f, 65b6d11f-2ac8-4e14-b991-3a018dc05533, 6626e36c-2b84-451a-b947-6395e1182469, 6636a3c2-b67c-4388-a602-eacc1b6f38ab, 67ef1e69-42a1-4d47-a5e0-347426c88cb7, 680982b1-8daf-4093-9388-44dac51680cb, 6ca9792a-d758-4770-901e-200f914434c5, 6d0c62a7-fad2-4391-8d71-408952cdb162, 6e4c3027-016b-45b3-90ad-ca41fc195649, 707f06ba-f64b-49f6-9020-7fcd4c47a290, 727e9086-d1fe-4eb1-9e25-99f1ccda7bcf, 733328b5-9f28-4178-943d-f450d2a1c1ab, 745605cf-c553-45c9-ae1b-9411d440ce27, 76074cdc-6793-45ee-a084-529d0917ad80, 7d4c22fd-55cd-49f6-9e60-40b1f6118d30, 82d87afd-4ae8-4ecc-bc93-06e5045937b1, 834e3daf-8f65-487a-b11b-ae0b07d47453, 86d3c624-eda7-42bc-a4c9-b58885c4efae, 8709a4eb-9987-47e0-bcb3-a0f7dd92b161, 8a27bac0-aac4-4cc6-9d89-6d807f1205dc, 8a50d5bd-c92c-46bc-9af5-8a3ed70ca7b2, 8c9be4e8-dfc8-43b0-a526-1a2cb502d913, 8cf5bdfb-74cb-4272-bf24-2b448dd03882, 8d5dea60-ac45-4928-b695-5c297d7d1ad8, 8e2b2fe7-3000-4496-84d3-fb50505e0a19, 90d7c934-61b3-4b34-aa58-850da3da6aa2, 93cd7617-1c00-4388-855b-e7d9366de93c, 94792c90-048c-4db2-bcc9-769895df09e9, 96eb2d26-1cb6-44d6-83f2-8492d77bb7fa, 97f12504-3105-4d0c-a881-9e59c572da7c, 984f6f11-ee43-4f57-9413-9f04669aca04, 9b6f4285-d265-4e4d-81f9-c946baa4c7f4, 9cd057dd-eba5-49c6-9b28-9c8f072fd9ad, 9cdb6bfa-c2a1-4589-99e2-b10d925ebb50, 9ec6010b-8d40-49be-ab0a-0e8249944080, 9fae89d8-8709-422d-9e58-90088d5d8280, 9fc68fdd-e4ab-4278-ac6d-75ab620cadf7, a033335d-ea0e-4fb4-9ce5-ee648c1fe2e8, a15f5ad2-9658-4ad1-baec-caa252f1bb1d, a2fc3989-18d7-4acf-8f28-c17153b29114, a4b46046-5891-4856-88af-bed0e12ebd89, a572f520-e872-45fd-8623-eb5bf71d7974, a6d68d08-545c-4967-a386-07a60caa1281, a74f9b5e-12cb-49d5-891f-0ca42a028c54, a7a32f06-0fef-45ac-8f77-0628aebe25ed, ab61c768-c3c1-43f0-9e35-9a669240ef88, ac00da06-cd5c-432f-95d3-88bb12b2ef12, ae448a51-d25e-40c6-9e52-618e8251ecb7, afa94199-221b-4e7f-9ee5-ab6ce1a3fa26, b0ca91dd-defd-4d74-9925-2dbb99859890, b159840e-a5b6-46f2-83fe-c79ec3768b38, b260bdfa-66f3-4de5-a022-ac87a0c88f41, b4e4ec75-5bb3-407e-b50f-74c5a61b259a, b5b0c3ff-c080-4454-a066-7b10b8ce6ecc, b65c7e62-8ca1-4310-9ea9-4fe2a4eafd0b, b77e74ca-6bff-46fe-87d1-cd67b2d7312f, b965fd4e-61ad-4494-93f9-a42dbddb1ac6, b9d6540d-3098-4daa-b136-e15b1a07c0f4, baab27b7-e665-4647-aecb-5651f8715137, bb4d3928-c6a8-4f33-b29c-b4aa53e17214, bd3fcd70-9787-4b5c-ade6-9de4eb9724d4, c1b2b935-d57c-45c0-9ef1-399c641e003b, c3031ebf-7a25-4ac8-9b63-5bd7f8c21b4d, c601ba0e-cc7d-4d13-858a-4f4b3ea5a009, c7b281de-becf-445d-a3f8-b2bc0579f6aa, c7d5895d-27e9-4fc0-97bd-d0261cd09a7e, c99edb66-f3d5-4dc6-874f-9e2eb436943c, cb49a7cc-c25e-4dd1-915b-0afecf67b57f, cc063f76-9453-499f-8202-a7f82aab5e50, cd0d9e67-5d71-4a60-884d-17f85bc3d881, d1844d91-9ec3-4cfe-9c29-c8bf74707044, d1d7480e-617c-44b8-b78d-f3ea5cb5635e, d55aef78-17dc-4165-b926-48d78d303bac, d5793d4b-c900-4e5d-b25c-f9a762523af6, d86581de-d9bd-4ea2-82eb-ce9746fc5cc5, d928d3c7-546e-47f0-b506-6e413eaf2c72, d9f14ac4-68c4-4d82-96a2-f81956b08558, dbee6f38-72e6-4af8-b2bf-0639308695d9, dbf47c1f-2e65-4f58-97d3-23318ddf44fd, dd207213-51cd-497f-b15b-6e6dc110ebd0, df23c7c8-18e8-4116-9f74-6836432bb65e, e08ded2d-c7a6-4d3a-bc1e-fa17d961040b, e0af7011-7ed4-4927-92c1-cd7df58eeedd, e1a712db-8c9a-4bb5-8449-15930ac8eb19, e3e76bf8-83ce-427a-86b9-ea6a793c8b31, e4662d9a-1722-4e2e-8b56-1790527ceee4, e67c86de-c3e1-48f0-9326-395a62832e63, e72755ab-ce05-4231-a419-02c66c5d10fe, e8636d1f-8427-4d21-9911-a657abd0704d, ea3c8bc4-224b-4967-bdcf-121a133f71f1, ea64a681-cf0b-4aa8-9d67-c0ff624b7c0b, eb73aef1-a189-4e72-bb16-3901ee0b2a03, ede2f789-1bfc-4001-a84f-9f6be4f20a6a, ee6396f9-a133-4f89-a274-016cafe4f353, ef414f0c-e7b4-4928-a035-876249ecb5ad, ef435952-c1d8-4a98-864e-f2035c007a09, f2abd64b-8f3d-4487-9d6f-7b905d780b6e, f370dd4e-fb0e-4eda-bf17-99e4b22d9d0c, f6cea0b8-3a7d-4984-a0c2-d90b49198a7d, fbeb5bb0-2260-4671-80c1-9c20190334be, fc8f42db-128d-4f46-abcd-236939c9b093, fdaad461-3da2-48a9-9b02-873e355d3c18, fdbec0e2-4866-4864-9505-5a3e5b5d1944, fe9b1a3a-b184-4bb1-b514-cb785822e634, fec2c2d9-49aa-4c78-a3bb-9a77fb962056, fffee8b6-939b-4188-a59f-c99e5f363ed5]
80601 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 157 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
80623 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
80623 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 157 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet => [00cc4de3-b585-434f-bda5-dfdd90684431, 014ac1f2-a343-4c78-8cb8-a25edd0f7640, 015b81e4-5788-499b-9850-4bf89ff07689, 027c4553-7771-4566-80c4-1aec459ba259, 02f93ecb-226b-47cd-8c99-40ce61d003b7, 05958526-0c68-4924-aa47-6818bc90162d, 064460c4-dd4c-4f07-b096-c33afb1a8a94, 0655162b-3c2a-44ec-a21e-226022becca1, 09662aea-bca8-494b-bb12-579ce723864a, 096e32ac-d042-4386-9379-5b071fb2eebf, 0a469bcf-561b-4309-8489-f3bf70308610, 0b476ad2-c2ba-4ce7-8d2c-c7bb5757be20, 0d25450c-98f0-4ac9-aba8-150be0c67267, 0df76c16-ae21-45fa-a30e-97abc80e7137, 0e3ce39c-c3d9-4cc5-b0e4-50090ba1ba90, 0fe48614-2bf7-44bc-ac81-51d7b4338d02, 1327424f-1eee-4d41-a338-d6dc6a67b598, 1393a1f7-bbb7-4ae0-9817-396631db2fe9, 155e3781-e02c-484d-9cd2-6f543d67924c, 16bd85c2-fa89-49b2-b472-20eff1b2b211, 180355d1-6f8a-4014-8204-04febd030ef1, 1915da38-3f79-4599-93a2-62926a16e243, 194148c9-3a08-4b15-b293-44216f1b754f, 19a1fb2b-8877-4f05-9f8a-0d97df6fe1a6, 19c5b51c-f50a-4294-b684-35e5a3e67fd2, 1bc41e22-89fd-4ed4-8e6b-53f7b929e953, 1d12cf6f-6cab-4577-83e7-3f6b184b9123, 228cff4e-ec4b-45ad-914b-03a5cfcf36f5, 22e1da0c-d6a2-49e0-95dc-f12dd8489bce, 24721080-74a6-4197-8b6a-35e838790a8e, 251653e2-5e72-4fee-8060-91fb8fd071cc, 288ce413-1fbb-4835-80d2-be3c3a6f32dd, 28cf81f5-a30e-429d-a0fe-d2250a41f4ed, 29c5a107-400f-47c7-8529-15f60b3fd1cb, 31d1bae3-927e-48f5-a5eb-67e1281acc5b, 332b642b-71fd-4d57-a42a-f7791d26e519, 336aff68-2045-40e3-bd69-3fac6f196ef3, 36f9ae01-f65d-4fb6-b8ed-4ac73dc8e464, 3727f9a5-7500-4d20-98bf-dd1fa6fd33ee, 37526e44-424c-4abf-8255-9b565ae2a416, 37f7be06-47d1-4e63-b4cd-88eaed32f429, 3db79d9f-665e-43eb-9782-8d96dfedac05, 4002543e-283f-48a3-b3d8-4f2a5a43968c, 41ae204b-f511-4211-853a-33b7ff434633, 4295f631-4113-49ce-8fb5-2e2ffc75fadf, 43037ad9-970b-4157-bd66-79a1fc715f03, 444d51d5-954c-4e1f-bdab-fdd13dad7fa1, 468d6d52-acb5-4743-a085-c456795552e7, 4917bb00-f48b-4ab4-908c-f5b734c3d521, 49fd0400-ca36-4ec4-9e70-c257e31a0738, 4a93190f-3bc1-46ed-93c7-487609781dfa, 4c05c7cb-67a6-402c-8590-61522413b350, 4f16fdb0-962f-4ad5-ac49-ec95b788f972, 50bca798-95c5-4763-8e5d-9d537a7d46bf, 513c01ea-fc9e-4d8e-847a-4b5065950151, 52425553-dafe-49d1-9cd4-01c8143eb1a6, 56170900-6412-4587-86a6-e00143f7e5df, 581a863b-a205-4922-ac38-7a278087e8a4, 5997537e-a2c2-40c4-ad14-7f6e6f0d88f3, 59d6d7bf-ff7c-4c4b-a7dc-8f60076a57e4, 5d43c8fd-e9a9-4c34-9737-e52ad062b495, 5dceb8e5-03f2-4510-91c4-5f6785e9b874, 5e127d00-e21a-4543-96b2-a899dc37478c, 5e8ec758-39e5-412b-a974-002d170fcb1d, 5ed018a8-e2ff-4247-ac1b-01f453433b3b, 5ee8c41c-ed34-4423-8b71-ba608046fbe9, 616a5bf6-0cfc-4f52-bac6-ca09838350a6, 618a3475-46b3-4a68-aa5b-ef53fbe4deab, 62a73f7f-58ff-4560-b16b-b65844f9c9da, 67b58fc2-2108-49e6-9991-4de16c3367c8, 68bf712d-7e8b-4861-b4f9-a06378031580, 69a11a03-b2e9-4766-9e5b-274d44625826, 6acc5657-6c45-41bf-aaca-c6e85c07f66d, 6b4847b0-fa26-4dd6-9bf6-356c7be66a73, 6cc43006-9d8b-46cb-8bcf-81f3e555afac, 6d168d47-5f1d-49c3-b62d-fc9a45900966, 6db18f3e-82d2-4946-82d8-2a9f8bcd12b0, 78387be6-ffb3-4b59-a792-80b011963582, 7b5282ff-6dfa-437d-b2eb-45e2736bf556, 7c62e33f-395f-47cf-b368-8d42b0169182, 7cb4efcd-ff00-415e-8e33-5c73e1fdcf23, 7eee934c-1908-4179-ae41-19bd419b2bb5, 7ef184c5-4c4e-4214-94e3-905e538ba3b9, 7f6e9639-984a-45fb-b203-8c4bfd89638b, 837d1a07-398c-47ef-90ce-5be6d4098818, 841d8935-d707-4e6e-84b3-449d9d4a7674, 85b8eccb-b13c-4714-964e-9a36b9f3b3a9, 85cac447-ecd1-4648-b55b-e3a0f969a1a4, 86e8b4a3-f4cb-4a85-b61f-196a76af64e4, 89ad623f-e864-4f05-8ca8-08e003e6bea4, 8c826be0-6061-44fc-8edb-34d317693e7a, 8db81187-4722-4558-bf0f-76a9428dc2f7, 8f13d585-917d-44dc-bdaa-89f00cd26435, 8f3362d5-cf2d-4640-9c1e-d289437fb07f, 8fa8f4a5-9ead-4303-902b-76752b0968d2, 94805164-8084-4526-96b4-c03c984caf7b, 96454905-3def-4fff-82f6-a60904247aa1, 98390fe9-fadb-4d01-a653-90b2a70d9fad, 9b3792f1-e31f-4dfe-89f5-425f01950aa9, 9e840aad-f192-408e-88d8-0b31f1db8317, 9f2f6127-65b5-40bb-8c21-358cfdd9485e, a1c72868-5ee7-4e79-a53e-e112d6f74293, a33fbaac-40e8-4404-9f39-7f9b6beb0222, a690f3c5-55b3-4e6f-bc7d-d9964fcea328, a90728ab-b804-447b-b6cd-79cdcb35a80e, a98900ac-2c09-4818-9d49-584d5a8924dc, abc8a1b5-d691-465b-ae7f-99038d8dcc32, ac58eafe-a47a-4bdd-b64a-de28e66dd4cf, ad9e34d2-e5f9-4570-8789-5bc4cf714980, adf02844-0360-43ae-b582-c5e4db13605c, af56b2ec-03ec-4715-9f3c-fd0570ba91f6, b0905422-5a36-4ad7-8dfe-a10e7f08c6a4, b3b4b330-a785-410c-a1cf-54f428400e53, b8052dc4-cd02-42ec-a6c2-a5d952657432, b8d20e89-d9d4-4408-92eb-b75c5584cab3, bcbcc2cd-19a0-45ae-ad1d-b0349711e071, c0eb2145-7beb-4825-83ff-a41e5b983c70, c3d6c966-d7e5-47c7-a6c6-983390c391e2, c43bfc4c-c573-4498-9431-aa7d881c37b9, c4566ce1-99d8-4cea-91e1-6d015df6c100, c50de1a0-a4d7-4c26-a6d6-f31f7f0350f2, c84db2c1-5147-4f6d-8543-82285d3a0e1a, c88becfb-96a2-44ac-a978-31e511773ca5, cad43dab-8d7b-46a8-92c7-1236670c0fc0, cdd7e864-db24-41e6-ae6c-bd5fc1d50674, ce4b3619-4a1f-4b04-b580-20e11f52a9cc, d3417891-0188-4d1f-ba30-db2daa93edde, d37a5d70-950e-4b80-8525-64b79a054d0a, d383c92d-c7ff-42ec-80bc-c51117656958, d6bf157d-4bcb-4346-ba75-0795274fa762, d7a4d753-541e-4a00-b453-2074185fcd2b, d8f05e32-63b3-45e0-8c06-8581d0233f57, d9067e8b-8899-4057-8598-7e2c0869f20d, d98c0871-98a9-4521-ac10-d054cc43d0b1, d9f371fe-e2e9-4e53-916e-691a42dba80e, dabcb779-6cef-47e2-8c1f-ede6e605c7a5, dbd42ab7-d44f-47db-a5fc-7d924c7ae5b0, df7810cf-3491-4c00-9df6-c572a2e4f20a, e09c2309-825b-4add-986b-2381bdea85ac, e0b5bae7-59d5-433f-9a1f-78b6d4d39b7f, e0ce1063-cd94-4e8a-b5cb-5eb992318ad0, e2a43b4d-e03a-49d0-9881-2f113f21e015, e3031f0c-8c13-4296-8162-7f0abbebb853, e3511bad-bde6-4d3e-99d7-01d584b94b5c, e72be06e-6161-4183-bd90-db2c7974dfe2, e75d3e7f-9073-4a8b-af38-5de09a03b65d, e8dc936b-a559-4fde-ae37-47268dff7b52, e908edae-036f-45d7-a8f6-01173245cea6, ebc50645-6f07-47ed-b694-52a15258295b, ed3f700f-9a65-49de-9392-429598ee6630, f1b9b496-324e-40b3-a417-610471847363, f1c26591-9fed-4f07-8df4-5ed8a6b47aba, f701bef1-febb-4e95-8d13-4fd72dd640ed, fb7f2648-be71-4d99-90a7-270aed3256af, fd2d8374-190a-4a10-a44e-46db0b19aeb3, fe2a4bf2-6b61-4949-8c76-4addcbf385d4, ff038b2d-3c24-4222-8b24-4753e549a0fa]
80634 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 168 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet
80656 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet
80657 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 168 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet => [017f36e2-785c-41e8-a539-b8f7e2b32c19, 01e37408-9b23-4ce1-8e08-3307385b471e, 0358bb05-f69e-4d8d-9cca-43be2c4ce83a, 0372f083-7964-4448-9f45-5c97e88cfcd4, 04391b18-982d-44df-aec4-9bd0118084f3, 04e3128e-06b1-4086-8c84-c5c54e30359f, 062909f6-6ab2-4400-a8fb-aa7ba5f0fce9, 0669e2d0-c87a-489a-8c8a-fc00a6fc4427, 0704789b-1c71-4009-be36-f92fd7f78801, 0718d90e-b166-4da2-9fcc-a6483f44914b, 08ab014d-a5cf-475d-97e0-d4f01b8336b7, 09ed5001-1bcf-4745-8ac8-2ddfe7f49772, 0d76948f-5404-4884-b3e7-fe17279d78fe, 0f7b4829-87f3-4fd7-bc77-268741466e3f, 105d070d-c03d-48eb-86e7-ac6143fb6abe, 11bcab4d-a781-4995-9292-c7a61c16a81b, 11f4ad5d-3cf5-4399-a48e-8f92a5881616, 1395a091-2082-45ee-8b50-9148f0401e9f, 1406bd31-75b7-48e9-8a04-e7e9b6f625a6, 14414995-ca72-4905-b11f-4e83303963f5, 14c9681c-92a9-4ad4-8f99-b7e760469835, 15f371c7-2dcb-4557-b70e-ecd8cf1fc477, 15fda3b8-e5ef-4f31-ab8c-2427e7b6059f, 178c69a9-bc0c-4dfa-b017-a3392dfc64ae, 192e53cd-43c9-4ac9-bdd1-e2bcbf074b99, 1a054602-42f3-48ae-9e4e-1d3804016f69, 1e995a7b-2e99-4ef0-95a0-49ff2b0ff534, 1f57af2b-6050-423c-a6e4-77d4bc9cd693, 1f7f5115-7803-480b-bc5e-4d93c5a4c48b, 21ba62a6-18d1-4c96-a0d8-cae90532cf25, 233d727e-5cc4-437f-9f03-127b4e031589, 23f1f5fc-cf75-47c7-9b22-9d5b0a0fc4f7, 24d2b681-4e70-4bd1-8375-51ead3d11c22, 27243f0d-c566-44a8-a173-78b2a1600282, 281602aa-4998-4f14-96fb-b71778864fc1, 28898d9d-1307-4070-8b19-1a94b7273407, 29904eb1-bb90-4d16-8769-d684c318c585, 2b495672-fa93-44d3-8171-a59168a36071, 2ec389e6-f9d8-469d-a064-44152ee07724, 2f066a74-bb0e-43a3-9c0d-1650d3c96934, 2f8d4ecf-ab3c-4f7d-abf5-a2ebfceac423, 2fe23727-557e-485c-a7e5-ad103b783022, 301cfb8c-df20-4803-95de-ece1877ab5b0, 33fb412f-8f68-4874-99cb-ada771bd5c15, 3606fb39-4c87-432f-bfb9-163464d71630, 365a538a-3cdc-4ee9-a40e-a12c4f0dadf3, 372c765f-f9bc-46a0-b81c-eab26a704c19, 37b00082-379f-4d7f-9a74-24ea0d2c901c, 37db7ac3-0f52-489e-9f84-39397254e93b, 3ac7ebf1-a5fd-4dd3-96f4-1f1de50de0f9, 3ca7b4c5-d11b-4405-941e-65667a085992, 3cbb411c-7a8b-4c03-865f-4d7ac697f974, 3e50efb3-57ef-4ad6-a818-289010d01457, 3efc15e4-6ec4-4277-ba54-94fbdae8b477, 3fa289fb-8da3-4674-872c-47a2131e14ee, 40446c3c-5bf9-49a8-b85a-2c0a45d8dede, 405b9054-f2c9-45bc-9a6c-917d9951ec0f, 429805a8-c615-494c-a09c-8a8f4eb06a15, 46232136-3edf-4ce8-8b2a-9f5f3f45a4a2, 466eca40-4559-43ff-925e-eaa4a830da02, 4a107100-a345-4657-a936-02a70a2d0b2a, 4aab791b-efe5-403d-81f7-c4ef99cfb59a, 4ab67f7d-e650-4c1d-b07c-0144d157890f, 4ba91581-3435-4f4b-a678-230966341ba6, 4cdda29f-ffb5-4075-a81a-8131ecccb7f9, 4f38dc16-6c48-4086-a069-7497a2b1e832, 4ffbb8fc-2de2-4c15-8ff6-71b62cc41fee, 50a87258-b035-4d89-bf37-5ed2e8933fb0, 528008b3-7042-4e10-9100-d9334a352927, 53242c5c-c1da-4baa-b237-4fcbb9c0964f, 56045c11-1fae-4f9c-bc1d-0b8abcbccb8e, 561a00ad-3f8e-481a-b3a6-c4e5d427ad09, 5701b0cb-4760-4192-a2e9-39eb6f923fa7, 5740ddfe-fc24-412b-80c8-881fa119703c, 58b67771-200c-4cf1-b65c-ef8edca4cfa1, 59195cce-24ff-40cd-8753-89ba4e3af00b, 5c4e4d5e-3c7b-47ef-8b0a-c87efab20a71, 5d3c9fbd-9dbf-4db8-b007-5f0594d2e775, 5dfc10c5-6060-4eb4-a970-33bbf99486c2, 60c3b516-8bb3-41e2-91e2-fb6e73df80c9, 6182f99f-71ab-409a-b656-b8766bb865ae, 61b68c14-bc8f-42e7-8eec-6564f13f7640, 632eac04-fae7-456d-8ad6-969fab55239a, 641c4428-dafa-4d3e-a3b0-ccbe570c7e89, 65a03896-6f79-4e0c-9c87-2de140062757, 682e7197-edbb-43c6-a2a1-f3e898538b8f, 683462d3-df26-46d9-ad44-ec302fead189, 6960df46-8087-4265-b23a-d79c148cc26d, 6d50476b-b09e-4b47-8cc7-889042e28c2a, 6fb8a618-1823-46ba-aa4e-eb0e78c28c1d, 73dea131-24aa-41d3-9093-f052c469710b, 73fda1d7-b54d-45f8-b328-4f74402253b1, 743ea730-c8b1-4e54-a743-68a21459900c, 7ce60b40-9616-46b6-b6dd-34355cf1664a, 81ed840d-028d-458a-90dc-c39d15b1b78e, 82e68f38-f8d1-44e2-ad9a-8593b69deccf, 85720d19-7f24-49f4-8c6b-389dedb22767, 86b88aa9-c905-4d79-91bc-38ea0a8e818e, 88807718-3f02-4f91-8772-8781ba0a2598, 8a03b5a9-214a-4441-a344-f929fb00a8fb, 8a34e100-664e-470f-8c47-35f675e040f8, 8dce87f7-75ea-4fa5-abd3-f33e8c2c66b7, 8e3459d2-dedd-4f2e-a91b-45af5c117550, 90feb056-4741-4843-b5b1-be18dfc79073, 93c16a9a-116c-4fc4-a317-8ac6142f69ed, 9582c9ed-7c1b-4a5d-8925-15562e72ba91, 96372425-816d-4bcb-b551-f97a79f4a6e8, 989e43ad-061b-47cf-8961-43f60554b98a, 9903efa6-7ede-468b-af43-5d47c4ad19c9, 99d57e02-852f-40f6-b808-141bd04bad30, 9ac614a6-5ad7-4db0-b547-b4cda8f33a1e, 9addbeba-b5e0-4c29-81c3-6bacc74e4dd3, 9af53088-1f8e-475e-8bb9-a3ecde0372ec, 9c94e67c-cd5b-49f6-9271-1e7f4dd1e378, 9ce1c500-903e-477d-9150-5ccc5d160b2f, a10d0058-8cfb-4869-8174-03cd01d3b804, a1cf4309-3d5e-4d67-8c1e-ed7fa2e05b65, a3170519-7e44-4d6c-972f-a924dc5f4f51, a6464121-e7e2-494d-95a6-3a159fc6279d, a69f75bd-d2a7-4251-9776-c0d7592a59b8, a7ed562c-4ec3-4668-a070-cc64ae76dabd, aae55288-5596-4f71-be59-bd9808b9ada0, acb3c005-3aea-451d-9e1b-c5acbe007aa5, ad3c3607-a55a-40f9-b832-5a4deec5f8f2, ad5393c6-9495-479e-bbaf-38cd6d2233a8, adb6849d-e275-4cdf-8bb1-cbea0f47ccac, aefcc5a8-2d06-4802-b015-9da30e50fbb1, af829356-c3c6-4ffa-a96c-3303f1afeb96, b0ffe05f-b8f8-41a2-99fb-0d0a3bd9ab69, b1667950-f6f7-498b-89f9-7fdc136485b3, b2f514a0-d7a9-42ca-81bd-08f728e37fc1, b47ce91b-2e0a-47ed-861b-572eb78e5515, b65debcc-ab6a-48cf-9a9e-04e4019b22d0, b6b53320-9cb9-45f8-9a81-66692d322bee, b850556e-39e2-4ab3-b8c4-80299f5e4779, b85af041-6853-4af7-980b-876721cfba2a, c221730d-cf83-4524-afd3-2e06e9da87c9, c71b55da-f347-48cd-ac87-0f31fbb01fe7, cb0aa5fe-fc8f-4044-b1e0-4aff3b274d09, cdeadda3-18b0-410b-a97f-d070593d50ce, cfea85cd-b0c1-4862-b0ab-09fe9e029037, d0121e13-d9e2-4678-9205-1bb993879076, d0d501e2-20ba-4759-8f30-65195d209d81, d226d605-0a2e-4a85-bc47-29cbea4d7a35, d4c6954b-11fe-43be-bd62-a579fa905d1a, d4df1523-f8a7-455b-9a4c-466fa39aef11, d5011e7b-a936-41b3-887d-9225b3c982ed, d7def30c-809d-4fd8-9a7a-5e21d23ab679, db528f5c-0dcf-46d4-bb59-56c984ee976b, dbd6ab66-c2ba-42c5-a9b9-d4841bfe2f21, dbe143fb-39a1-408c-836d-4f626bd6a89d, dc4095be-6378-408c-aa0a-24960ec0d64c, dca4224b-f803-4290-9735-f7e96f027f76, dcb2df37-21ae-41b3-a4c7-05b7d9474c01, dedaabfb-16ec-44a4-9aaa-1cae77c3e0f7, e42b9c42-b80f-4145-be84-84a785c6b1ad, e9fa38c4-90b9-4edc-a59f-f9d7dc9598e0, ebba3cea-cba3-44ca-8c77-95b7d95aeeba, ee57401d-386e-45f9-8e9a-bdcaeff22ed9, ef7ce1cc-b299-4348-b26e-3c6ec4541c8c, f3bc0e3c-a1ad-4995-a6b3-1a216e8fe03a, f649f17b-0438-4880-8182-b0033c36d93e, f71a9e83-0972-4322-885d-eca975aaa113, f8efc934-134b-441c-b00d-1ba36683d158, fa5d5521-101f-448c-9209-ee03838a1fe4, fb4bffad-ac3d-4bd6-bb57-6ab7b643f48d, fc04652c-f22e-404a-adce-5e90c37e3134, fd9daca3-458c-4ca7-913c-ca10dba367de]
81889 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195253
82208 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 96
82208 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
82404 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet
82427 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet
82427 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet => [03bd632b-c594-4ae5-a56b-7b67e1985cc3, 062248f7-1122-4364-b861-bf4a6aab7877, 13f3e120-785f-47bc-a923-cb3df3fc8f42, 15da68c2-ae2f-4e37-a981-3e44664a51ed, 187fc44c-4a37-4475-b0fb-d1fb5e84a149, 1d0e8a8b-7bd2-4ef0-98fc-0f0c12de3b15, 2592bbd2-e81e-4000-abe4-b3a519bf9022, 3118f32d-895a-4bf1-a408-da968bf56fa6, 330ee1ac-a7b0-4189-84bf-139d3ee17416, 33a824e3-e930-41a8-bf4b-e280e2ef08eb, 4a85e18b-e0c6-4bf8-9b30-bd4f386cee92, 4e83ffdb-ff1c-4333-bd2d-794877affdeb, 583c9fb1-42ae-4446-be2f-7d0e20c64cb8, 6626e36c-2b84-451a-b947-6395e1182469, 727e9086-d1fe-4eb1-9e25-99f1ccda7bcf, 834e3daf-8f65-487a-b11b-ae0b07d47453, 8c9be4e8-dfc8-43b0-a526-1a2cb502d913, 93cd7617-1c00-4388-855b-e7d9366de93c, 96eb2d26-1cb6-44d6-83f2-8492d77bb7fa, 97f12504-3105-4d0c-a881-9e59c572da7c, 9cd057dd-eba5-49c6-9b28-9c8f072fd9ad, a033335d-ea0e-4fb4-9ce5-ee648c1fe2e8, a572f520-e872-45fd-8623-eb5bf71d7974, a74f9b5e-12cb-49d5-891f-0ca42a028c54, b65c7e62-8ca1-4310-9ea9-4fe2a4eafd0b, b77e74ca-6bff-46fe-87d1-cd67b2d7312f, c601ba0e-cc7d-4d13-858a-4f4b3ea5a009, c7d5895d-27e9-4fc0-97bd-d0261cd09a7e, d1844d91-9ec3-4cfe-9c29-c8bf74707044, d9f14ac4-68c4-4d82-96a2-f81956b08558, e72755ab-ce05-4231-a419-02c66c5d10fe, ef435952-c1d8-4a98-864e-f2035c007a09, f6cea0b8-3a7d-4984-a0c2-d90b49198a7d, fbeb5bb0-2260-4671-80c1-9c20190334be]
82438 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
82463 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
82463 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet => [02f93ecb-226b-47cd-8c99-40ce61d003b7, 0a469bcf-561b-4309-8489-f3bf70308610, 0e3ce39c-c3d9-4cc5-b0e4-50090ba1ba90, 0fe48614-2bf7-44bc-ac81-51d7b4338d02, 180355d1-6f8a-4014-8204-04febd030ef1, 1915da38-3f79-4599-93a2-62926a16e243, 24721080-74a6-4197-8b6a-35e838790a8e, 251653e2-5e72-4fee-8060-91fb8fd071cc, 31d1bae3-927e-48f5-a5eb-67e1281acc5b, 4002543e-283f-48a3-b3d8-4f2a5a43968c, 59d6d7bf-ff7c-4c4b-a7dc-8f60076a57e4, 5d43c8fd-e9a9-4c34-9737-e52ad062b495, 5e8ec758-39e5-412b-a974-002d170fcb1d, 62a73f7f-58ff-4560-b16b-b65844f9c9da]
82494 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
82515 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet
82515 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet => [7cb4efcd-ff00-415e-8e33-5c73e1fdcf23, 85cac447-ecd1-4648-b55b-e3a0f969a1a4, 98390fe9-fadb-4d01-a653-90b2a70d9fad, a1c72868-5ee7-4e79-a53e-e112d6f74293, ac58eafe-a47a-4bdd-b64a-de28e66dd4cf, ad9e34d2-e5f9-4570-8789-5bc4cf714980, cdd7e864-db24-41e6-ae6c-bd5fc1d50674, ce4b3619-4a1f-4b04-b580-20e11f52a9cc, dbd42ab7-d44f-47db-a5fc-7d924c7ae5b0, df7810cf-3491-4c00-9df6-c572a2e4f20a, e09c2309-825b-4add-986b-2381bdea85ac, ebc50645-6f07-47ed-b694-52a15258295b, fb7f2648-be71-4d99-90a7-270aed3256af, fd2d8374-190a-4a10-a44e-46db0b19aeb3]
82538 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet
82555 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet
82555 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet => [01e37408-9b23-4ce1-8e08-3307385b471e, 062909f6-6ab2-4400-a8fb-aa7ba5f0fce9, 0718d90e-b166-4da2-9fcc-a6483f44914b, 09ed5001-1bcf-4745-8ac8-2ddfe7f49772, 1395a091-2082-45ee-8b50-9148f0401e9f, 1406bd31-75b7-48e9-8a04-e7e9b6f625a6, 1a054602-42f3-48ae-9e4e-1d3804016f69, 1f7f5115-7803-480b-bc5e-4d93c5a4c48b, 24d2b681-4e70-4bd1-8375-51ead3d11c22, 281602aa-4998-4f14-96fb-b71778864fc1, 28898d9d-1307-4070-8b19-1a94b7273407, 301cfb8c-df20-4803-95de-ece1877ab5b0, 365a538a-3cdc-4ee9-a40e-a12c4f0dadf3, 37b00082-379f-4d7f-9a74-24ea0d2c901c, 3efc15e4-6ec4-4277-ba54-94fbdae8b477, 405b9054-f2c9-45bc-9a6c-917d9951ec0f, 4a107100-a345-4657-a936-02a70a2d0b2a, 4ba91581-3435-4f4b-a678-230966341ba6, 53242c5c-c1da-4baa-b237-4fcbb9c0964f, 59195cce-24ff-40cd-8753-89ba4e3af00b, 5dfc10c5-6060-4eb4-a970-33bbf99486c2, 6fb8a618-1823-46ba-aa4e-eb0e78c28c1d, 73fda1d7-b54d-45f8-b328-4f74402253b1, 81ed840d-028d-458a-90dc-c39d15b1b78e, 85720d19-7f24-49f4-8c6b-389dedb22767, 88807718-3f02-4f91-8772-8781ba0a2598, 90feb056-4741-4843-b5b1-be18dfc79073, b0ffe05f-b8f8-41a2-99fb-0d0a3bd9ab69, d4c6954b-11fe-43be-bd62-a579fa905d1a, d7def30c-809d-4fd8-9a7a-5e21d23ab679, dedaabfb-16ec-44a4-9aaa-1cae77c3e0f7, e42b9c42-b80f-4145-be84-84a785c6b1ad, ebba3cea-cba3-44ca-8c77-95b7d95aeeba, fb4bffad-ac3d-4bd6-bb57-6ab7b643f48d]
82659 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=96}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=34}}}
82711 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
82711 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
82721 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195253
82722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195253
83308 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
83308 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
83415 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
83415 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
83548 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
83558 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195253 as complete
83558 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195253
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet; isDirectory=false; length=450618; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet; isDirectory=false; length=450424; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet; isDirectory=false; length=449032; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet; isDirectory=false; length=448860; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet; isDirectory=false; length=450010; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet; isDirectory=false; length=449830; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
84734 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195255
85091 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 95
85091 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
85288 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet
85298 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet
85298 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet => [062248f7-1122-4364-b861-bf4a6aab7877, 0ad3e1ba-0a6b-42fa-8bd8-7cdc98b41533, 0ee52ff8-1b24-4c6f-a869-8eec986e9053, 129be2ed-dc5e-42ae-9be7-f036e19c799b, 1d0e8a8b-7bd2-4ef0-98fc-0f0c12de3b15, 22109158-b25c-457c-b3b4-2f3a6dfb1cee, 232c9e16-b138-45ca-8271-ddd571d4ec5e, 26ba0f07-9ef2-4ea6-9535-243a8f6cd126, 3118f32d-895a-4bf1-a408-da968bf56fa6, 47d745cf-1524-4034-875b-07c1719fa3a9, 4a85e18b-e0c6-4bf8-9b30-bd4f386cee92, 4fdcdae3-e455-448e-a903-8d8a8b727ce2, 511c9b62-5ed3-4905-adbd-49bda4c45d58, 583c9fb1-42ae-4446-be2f-7d0e20c64cb8, 5aa168ac-40dd-436d-88a7-92f195560cb6, 5bd77f51-ab84-4897-bc85-a9cabbda8e29, 6e4c3027-016b-45b3-90ad-ca41fc195649, 707f06ba-f64b-49f6-9020-7fcd4c47a290, 7d4c22fd-55cd-49f6-9e60-40b1f6118d30, 8cf5bdfb-74cb-4272-bf24-2b448dd03882, 8d5dea60-ac45-4928-b695-5c297d7d1ad8, 97f12504-3105-4d0c-a881-9e59c572da7c, a7a32f06-0fef-45ac-8f77-0628aebe25ed, ac00da06-cd5c-432f-95d3-88bb12b2ef12, afa94199-221b-4e7f-9ee5-ab6ce1a3fa26, b159840e-a5b6-46f2-83fe-c79ec3768b38, b4e4ec75-5bb3-407e-b50f-74c5a61b259a, baab27b7-e665-4647-aecb-5651f8715137, c601ba0e-cc7d-4d13-858a-4f4b3ea5a009, c99edb66-f3d5-4dc6-874f-9e2eb436943c, cc063f76-9453-499f-8202-a7f82aab5e50, dbee6f38-72e6-4af8-b2bf-0639308695d9, e72755ab-ce05-4231-a419-02c66c5d10fe, ef414f0c-e7b4-4928-a035-876249ecb5ad, ef435952-c1d8-4a98-864e-f2035c007a09, f2abd64b-8f3d-4487-9d6f-7b905d780b6e, f370dd4e-fb0e-4eda-bf17-99e4b22d9d0c]
85311 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet
85323 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet
85323 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet => [00cc4de3-b585-434f-bda5-dfdd90684431, 09662aea-bca8-494b-bb12-579ce723864a, 0b476ad2-c2ba-4ce7-8d2c-c7bb5757be20, 0e3ce39c-c3d9-4cc5-b0e4-50090ba1ba90, 28cf81f5-a30e-429d-a0fe-d2250a41f4ed, 29c5a107-400f-47c7-8529-15f60b3fd1cb, 336aff68-2045-40e3-bd69-3fac6f196ef3, 4917bb00-f48b-4ab4-908c-f5b734c3d521, 513c01ea-fc9e-4d8e-847a-4b5065950151, 5ee8c41c-ed34-4423-8b71-ba608046fbe9, 62a73f7f-58ff-4560-b16b-b65844f9c9da]
85352 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet
85370 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet
85370 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet => [6cc43006-9d8b-46cb-8bcf-81f3e555afac, 7eee934c-1908-4179-ae41-19bd419b2bb5, 7ef184c5-4c4e-4214-94e3-905e538ba3b9, 837d1a07-398c-47ef-90ce-5be6d4098818, 94805164-8084-4526-96b4-c03c984caf7b, 98390fe9-fadb-4d01-a653-90b2a70d9fad, a1c72868-5ee7-4e79-a53e-e112d6f74293, bcbcc2cd-19a0-45ae-ad1d-b0349711e071, c4566ce1-99d8-4cea-91e1-6d015df6c100, e0ce1063-cd94-4e8a-b5cb-5eb992318ad0, e2a43b4d-e03a-49d0-9881-2f113f21e015, e3031f0c-8c13-4296-8162-7f0abbebb853, ebc50645-6f07-47ed-b694-52a15258295b, ed3f700f-9a65-49de-9392-429598ee6630]
85384 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet
85390 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet
85390 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet => [0358bb05-f69e-4d8d-9cca-43be2c4ce83a, 04e3128e-06b1-4086-8c84-c5c54e30359f, 09ed5001-1bcf-4745-8ac8-2ddfe7f49772, 105d070d-c03d-48eb-86e7-ac6143fb6abe, 14414995-ca72-4905-b11f-4e83303963f5, 21ba62a6-18d1-4c96-a0d8-cae90532cf25, 27243f0d-c566-44a8-a173-78b2a1600282, 281602aa-4998-4f14-96fb-b71778864fc1, 2b495672-fa93-44d3-8171-a59168a36071, 2ec389e6-f9d8-469d-a064-44152ee07724, 466eca40-4559-43ff-925e-eaa4a830da02, 4aab791b-efe5-403d-81f7-c4ef99cfb59a, 4ab67f7d-e650-4c1d-b07c-0144d157890f, 4ba91581-3435-4f4b-a678-230966341ba6, 528008b3-7042-4e10-9100-d9334a352927, 561a00ad-3f8e-481a-b3a6-c4e5d427ad09, 5dfc10c5-6060-4eb4-a970-33bbf99486c2, 65a03896-6f79-4e0c-9c87-2de140062757, 683462d3-df26-46d9-ad44-ec302fead189, 73dea131-24aa-41d3-9093-f052c469710b, 81ed840d-028d-458a-90dc-c39d15b1b78e, a10d0058-8cfb-4869-8174-03cd01d3b804, a3170519-7e44-4d6c-972f-a924dc5f4f51, ad5393c6-9495-479e-bbaf-38cd6d2233a8, af829356-c3c6-4ffa-a96c-3303f1afeb96, cdeadda3-18b0-410b-a97f-d070593d50ce, cfea85cd-b0c1-4862-b0ab-09fe9e029037, db528f5c-0dcf-46d4-bb59-56c984ee976b, dbe143fb-39a1-408c-836d-4f626bd6a89d, e42b9c42-b80f-4145-be84-84a785c6b1ad, ee57401d-386e-45f9-8e9a-bdcaeff22ed9, fb4bffad-ac3d-4bd6-bb57-6ab7b643f48d, fd9daca3-458c-4ca7-913c-ca10dba367de]
85499 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=95}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=37}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=33}}}
85547 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2700
85547 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
85556 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195255
85558 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195255
86185 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
86185 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
86295 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
86295 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
86490 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
86540 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195255 as complete
86540 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195255
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet; isDirectory=false; length=450739; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet; isDirectory=false; length=450618; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_0_20200319195249.parquet; isDirectory=false; length=450424; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1584643976000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet; isDirectory=false; length=449032; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_1_20200319195249.parquet; isDirectory=false; length=448860; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet; isDirectory=false; length=450137; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet; isDirectory=false; length=450010; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195249.parquet; isDirectory=false; length=449830; replication=1; blocksize=33554432; modification_time=1584643970000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
87694 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195258
88005 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 87
88006 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
88183 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 27 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet
88216 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet
88216 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 27 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet => [0ee52ff8-1b24-4c6f-a869-8eec986e9053, 13f3e120-785f-47bc-a923-cb3df3fc8f42, 15da68c2-ae2f-4e37-a981-3e44664a51ed, 187fc44c-4a37-4475-b0fb-d1fb5e84a149, 213aff1f-db05-46d4-9267-01b0f8e7a07f, 238b92b4-e925-4414-80bf-210ac380b9ec, 2463545a-b57b-4156-9683-c4ec93a321b9, 375d6489-0568-4d17-85f2-3d60db6ee7a6, 47d745cf-1524-4034-875b-07c1719fa3a9, 51afd4d2-c612-463e-81ca-e9416173d57a, 5aa168ac-40dd-436d-88a7-92f195560cb6, 5f0fe5eb-7e4f-4812-aeeb-ac8344ccec5d, 63274066-2ffd-4286-81e5-de948a0d58ee, 632fd36d-bef5-48d0-b5c4-25eb88c3ab20, 6422caf3-342a-49ed-aa47-54f4583a8c5f, 834e3daf-8f65-487a-b11b-ae0b07d47453, 86d3c624-eda7-42bc-a4c9-b58885c4efae, 8e2b2fe7-3000-4496-84d3-fb50505e0a19, a033335d-ea0e-4fb4-9ce5-ee648c1fe2e8, ab61c768-c3c1-43f0-9e35-9a669240ef88, c7d5895d-27e9-4fc0-97bd-d0261cd09a7e, cd0d9e67-5d71-4a60-884d-17f85bc3d881, e08ded2d-c7a6-4d3a-bc1e-fa17d961040b, e0af7011-7ed4-4927-92c1-cd7df58eeedd, e3e76bf8-83ce-427a-86b9-ea6a793c8b31, e67c86de-c3e1-48f0-9326-395a62832e63, ea64a681-cf0b-4aa8-9d67-c0ff624b7c0b]
88227 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet
88248 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet
88248 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet => [02f93ecb-226b-47cd-8c99-40ce61d003b7, 0655162b-3c2a-44ec-a21e-226022becca1, 155e3781-e02c-484d-9cd2-6f543d67924c, 180355d1-6f8a-4014-8204-04febd030ef1, 22e1da0c-d6a2-49e0-95dc-f12dd8489bce, 29c5a107-400f-47c7-8529-15f60b3fd1cb, 4295f631-4113-49ce-8fb5-2e2ffc75fadf, 468d6d52-acb5-4743-a085-c456795552e7, 616a5bf6-0cfc-4f52-bac6-ca09838350a6, 67b58fc2-2108-49e6-9991-4de16c3367c8, 6cc43006-9d8b-46cb-8bcf-81f3e555afac, 6db18f3e-82d2-4946-82d8-2a9f8bcd12b0, 7c62e33f-395f-47cf-b368-8d42b0169182, 7eee934c-1908-4179-ae41-19bd419b2bb5, 8f3362d5-cf2d-4640-9c1e-d289437fb07f, 94805164-8084-4526-96b4-c03c984caf7b, 96454905-3def-4fff-82f6-a60904247aa1]
88570 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet
88582 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet
88582 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet => [ac58eafe-a47a-4bdd-b64a-de28e66dd4cf, b8d20e89-d9d4-4408-92eb-b75c5584cab3, d98c0871-98a9-4521-ac10-d054cc43d0b1, e09c2309-825b-4add-986b-2381bdea85ac, e75d3e7f-9073-4a8b-af38-5de09a03b65d, fb7f2648-be71-4d99-90a7-270aed3256af, ff038b2d-3c24-4222-8b24-4753e549a0fa]
88604 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 36 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet
88624 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet
88624 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 36 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet => [0f7b4829-87f3-4fd7-bc77-268741466e3f, 11bcab4d-a781-4995-9292-c7a61c16a81b, 233d727e-5cc4-437f-9f03-127b4e031589, 23f1f5fc-cf75-47c7-9b22-9d5b0a0fc4f7, 27243f0d-c566-44a8-a173-78b2a1600282, 28898d9d-1307-4070-8b19-1a94b7273407, 33fb412f-8f68-4874-99cb-ada771bd5c15, 372c765f-f9bc-46a0-b81c-eab26a704c19, 3ac7ebf1-a5fd-4dd3-96f4-1f1de50de0f9, 3ca7b4c5-d11b-4405-941e-65667a085992, 3cbb411c-7a8b-4c03-865f-4d7ac697f974, 429805a8-c615-494c-a09c-8a8f4eb06a15, 528008b3-7042-4e10-9100-d9334a352927, 53242c5c-c1da-4baa-b237-4fcbb9c0964f, 561a00ad-3f8e-481a-b3a6-c4e5d427ad09, 59195cce-24ff-40cd-8753-89ba4e3af00b, 5d3c9fbd-9dbf-4db8-b007-5f0594d2e775, 60c3b516-8bb3-41e2-91e2-fb6e73df80c9, 6182f99f-71ab-409a-b656-b8766bb865ae, 61b68c14-bc8f-42e7-8eec-6564f13f7640, 632eac04-fae7-456d-8ad6-969fab55239a, 683462d3-df26-46d9-ad44-ec302fead189, 99d57e02-852f-40f6-b808-141bd04bad30, acb3c005-3aea-451d-9e1b-c5acbe007aa5, adb6849d-e275-4cdf-8bb1-cbea0f47ccac, b2f514a0-d7a9-42ca-81bd-08f728e37fc1, b85af041-6853-4af7-980b-876721cfba2a, c221730d-cf83-4524-afd3-2e06e9da87c9, d7def30c-809d-4fd8-9a7a-5e21d23ab679, dbd6ab66-c2ba-42c5-a9b9-d4841bfe2f21, dc4095be-6378-408c-aa0a-24960ec0d64c, dcb2df37-21ae-41b3-a4c7-05b7d9474c01, f649f17b-0438-4880-8182-b0033c36d93e, f71a9e83-0972-4322-885d-eca975aaa113, f8efc934-134b-441c-b00d-1ba36683d158, fc04652c-f22e-404a-adce-5e90c37e3134]
88709 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=87}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=27}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=24}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=36}}}
88760 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
88760 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
88777 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195258
88777 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195258
89317 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
89317 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
89446 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
89446 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
89592 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
89644 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195258 as complete
89644 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195258
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet; isDirectory=false; length=450761; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet; isDirectory=false; length=450739; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195253.parquet; isDirectory=false; length=450618; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet; isDirectory=false; length=449178; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1584643976000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195253.parquet; isDirectory=false; length=449032; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet; isDirectory=false; length=450187; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet; isDirectory=false; length=450137; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195253.parquet; isDirectory=false; length=450010; replication=1; blocksize=33554432; modification_time=1584643974000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
90828 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195301
91136 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 95
91137 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
91367 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 25 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet
91405 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet
91405 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 25 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet => [070bf235-03fd-4423-9406-c5bc1531178c, 07156029-0e04-4968-818b-2d222135e0a9, 129be2ed-dc5e-42ae-9be7-f036e19c799b, 13f3e120-785f-47bc-a923-cb3df3fc8f42, 1d832a92-381b-4b58-9aab-128713c9e7c9, 26ba0f07-9ef2-4ea6-9535-243a8f6cd126, 4a85e18b-e0c6-4bf8-9b30-bd4f386cee92, 511c9b62-5ed3-4905-adbd-49bda4c45d58, 583c9fb1-42ae-4446-be2f-7d0e20c64cb8, 6422caf3-342a-49ed-aa47-54f4583a8c5f, 6636a3c2-b67c-4388-a602-eacc1b6f38ab, 76074cdc-6793-45ee-a084-529d0917ad80, 7d4c22fd-55cd-49f6-9e60-40b1f6118d30, 90d7c934-61b3-4b34-aa58-850da3da6aa2, 9cd057dd-eba5-49c6-9b28-9c8f072fd9ad, a15f5ad2-9658-4ad1-baec-caa252f1bb1d, a2fc3989-18d7-4acf-8f28-c17153b29114, cb49a7cc-c25e-4dd1-915b-0afecf67b57f, cd0d9e67-5d71-4a60-884d-17f85bc3d881, df23c7c8-18e8-4116-9f74-6836432bb65e, e3e76bf8-83ce-427a-86b9-ea6a793c8b31, e67c86de-c3e1-48f0-9326-395a62832e63, ea64a681-cf0b-4aa8-9d67-c0ff624b7c0b, f370dd4e-fb0e-4eda-bf17-99e4b22d9d0c, fdbec0e2-4866-4864-9505-5a3e5b5d1944]
91416 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 23 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet
91437 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet
91437 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 23 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet => [027c4553-7771-4566-80c4-1aec459ba259, 02f93ecb-226b-47cd-8c99-40ce61d003b7, 096e32ac-d042-4386-9379-5b071fb2eebf, 0a469bcf-561b-4309-8489-f3bf70308610, 0df76c16-ae21-45fa-a30e-97abc80e7137, 0fe48614-2bf7-44bc-ac81-51d7b4338d02, 1327424f-1eee-4d41-a338-d6dc6a67b598, 155e3781-e02c-484d-9cd2-6f543d67924c, 194148c9-3a08-4b15-b293-44216f1b754f, 1bc41e22-89fd-4ed4-8e6b-53f7b929e953, 336aff68-2045-40e3-bd69-3fac6f196ef3, 36f9ae01-f65d-4fb6-b8ed-4ac73dc8e464, 37526e44-424c-4abf-8255-9b565ae2a416, 3db79d9f-665e-43eb-9782-8d96dfedac05, 4002543e-283f-48a3-b3d8-4f2a5a43968c, 4a93190f-3bc1-46ed-93c7-487609781dfa, 5e8ec758-39e5-412b-a974-002d170fcb1d, 5ed018a8-e2ff-4247-ac1b-01f453433b3b, 7c62e33f-395f-47cf-b368-8d42b0169182, 7cb4efcd-ff00-415e-8e33-5c73e1fdcf23, 7eee934c-1908-4179-ae41-19bd419b2bb5, 85b8eccb-b13c-4714-964e-9a36b9f3b3a9, 8fa8f4a5-9ead-4303-902b-76752b0968d2]
91458 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet
91490 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet
91490 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet => [96454905-3def-4fff-82f6-a60904247aa1, 9b3792f1-e31f-4dfe-89f5-425f01950aa9, 9e840aad-f192-408e-88d8-0b31f1db8317, a690f3c5-55b3-4e6f-bc7d-d9964fcea328, a90728ab-b804-447b-b6cd-79cdcb35a80e, b3b4b330-a785-410c-a1cf-54f428400e53, bcbcc2cd-19a0-45ae-ad1d-b0349711e071, c43bfc4c-c573-4498-9431-aa7d881c37b9, c4566ce1-99d8-4cea-91e1-6d015df6c100, cdd7e864-db24-41e6-ae6c-bd5fc1d50674, d3417891-0188-4d1f-ba30-db2daa93edde, d9067e8b-8899-4057-8598-7e2c0869f20d]
91502 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet
91513 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet
91513 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet => [017f36e2-785c-41e8-a539-b8f7e2b32c19, 0358bb05-f69e-4d8d-9cca-43be2c4ce83a, 0704789b-1c71-4009-be36-f92fd7f78801, 0718d90e-b166-4da2-9fcc-a6483f44914b, 0d76948f-5404-4884-b3e7-fe17279d78fe, 11bcab4d-a781-4995-9292-c7a61c16a81b, 1395a091-2082-45ee-8b50-9148f0401e9f, 1406bd31-75b7-48e9-8a04-e7e9b6f625a6, 15f371c7-2dcb-4557-b70e-ecd8cf1fc477, 15fda3b8-e5ef-4f31-ab8c-2427e7b6059f, 2ec389e6-f9d8-469d-a064-44152ee07724, 3ac7ebf1-a5fd-4dd3-96f4-1f1de50de0f9, 405b9054-f2c9-45bc-9a6c-917d9951ec0f, 466eca40-4559-43ff-925e-eaa4a830da02, 4ba91581-3435-4f4b-a678-230966341ba6, 4f38dc16-6c48-4086-a069-7497a2b1e832, 56045c11-1fae-4f9c-bc1d-0b8abcbccb8e, 59195cce-24ff-40cd-8753-89ba4e3af00b, 5d3c9fbd-9dbf-4db8-b007-5f0594d2e775, 60c3b516-8bb3-41e2-91e2-fb6e73df80c9, 6182f99f-71ab-409a-b656-b8766bb865ae, 632eac04-fae7-456d-8ad6-969fab55239a, 682e7197-edbb-43c6-a2a1-f3e898538b8f, 73dea131-24aa-41d3-9093-f052c469710b, 73fda1d7-b54d-45f8-b328-4f74402253b1, 8dce87f7-75ea-4fa5-abd3-f33e8c2c66b7, 9ac614a6-5ad7-4db0-b547-b4cda8f33a1e, a3170519-7e44-4d6c-972f-a924dc5f4f51, b47ce91b-2e0a-47ed-861b-572eb78e5515, b85af041-6853-4af7-980b-876721cfba2a, cdeadda3-18b0-410b-a97f-d070593d50ce, d5011e7b-a936-41b3-887d-9225b3c982ed, d7def30c-809d-4fd8-9a7a-5e21d23ab679, dbe143fb-39a1-408c-836d-4f626bd6a89d, dcb2df37-21ae-41b3-a4c7-05b7d9474c01]
91587 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=95}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=35}}}
91623 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
91623 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
91634 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195301
91634 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195301
92153 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
92153 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
92283 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
92283 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
92446 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
92516 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195301 as complete
92516 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195301
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet; isDirectory=false; length=450916; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet; isDirectory=false; length=450761; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195255.parquet; isDirectory=false; length=450739; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet; isDirectory=false; length=449300; replication=1; blocksize=33554432; modification_time=1584643982000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet; isDirectory=false; length=449178; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195255.parquet; isDirectory=false; length=449154; replication=1; blocksize=33554432; modification_time=1584643976000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet; isDirectory=false; length=450317; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet; isDirectory=false; length=450187; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195255.parquet; isDirectory=false; length=450137; replication=1; blocksize=33554432; modification_time=1584643977000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
93669 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195304
93935 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 94
93935 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
94112 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet
94119 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet
94119 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet => [07156029-0e04-4968-818b-2d222135e0a9, 12cb3d70-b179-41c4-b6d6-5fbb2d547a6d, 238b92b4-e925-4414-80bf-210ac380b9ec, 2c089b8c-9692-48d0-85e8-0b217ea482ce, 2dcda651-f564-4734-b920-e354cc7dce46, 5568a6df-8803-420e-b438-b76bca80873e, 63274066-2ffd-4286-81e5-de948a0d58ee, 632fd36d-bef5-48d0-b5c4-25eb88c3ab20, 6d0c62a7-fad2-4391-8d71-408952cdb162, 733328b5-9f28-4178-943d-f450d2a1c1ab, 834e3daf-8f65-487a-b11b-ae0b07d47453, 86d3c624-eda7-42bc-a4c9-b58885c4efae, 8a27bac0-aac4-4cc6-9d89-6d807f1205dc, 8c9be4e8-dfc8-43b0-a526-1a2cb502d913, 93cd7617-1c00-4388-855b-e7d9366de93c, a4b46046-5891-4856-88af-bed0e12ebd89, b260bdfa-66f3-4de5-a022-ac87a0c88f41, c7d5895d-27e9-4fc0-97bd-d0261cd09a7e, d1844d91-9ec3-4cfe-9c29-c8bf74707044, e72755ab-ce05-4231-a419-02c66c5d10fe, fe9b1a3a-b184-4bb1-b514-cb785822e634]
94137 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet
94157 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet
94157 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet => [02f93ecb-226b-47cd-8c99-40ce61d003b7, 05958526-0c68-4924-aa47-6818bc90162d, 09662aea-bca8-494b-bb12-579ce723864a, 096e32ac-d042-4386-9379-5b071fb2eebf, 0a469bcf-561b-4309-8489-f3bf70308610, 1393a1f7-bbb7-4ae0-9817-396631db2fe9, 228cff4e-ec4b-45ad-914b-03a5cfcf36f5, 29c5a107-400f-47c7-8529-15f60b3fd1cb, 336aff68-2045-40e3-bd69-3fac6f196ef3, 468d6d52-acb5-4743-a085-c456795552e7, 581a863b-a205-4922-ac38-7a278087e8a4, 59d6d7bf-ff7c-4c4b-a7dc-8f60076a57e4, 5dceb8e5-03f2-4510-91c4-5f6785e9b874, 5e8ec758-39e5-412b-a974-002d170fcb1d, 67b58fc2-2108-49e6-9991-4de16c3367c8, 6cc43006-9d8b-46cb-8bcf-81f3e555afac, 6d168d47-5f1d-49c3-b62d-fc9a45900966, 7ef184c5-4c4e-4214-94e3-905e538ba3b9, 85cac447-ecd1-4648-b55b-e3a0f969a1a4, 86e8b4a3-f4cb-4a85-b61f-196a76af64e4, 94805164-8084-4526-96b4-c03c984caf7b, 98390fe9-fadb-4d01-a653-90b2a70d9fad, 9f2f6127-65b5-40bb-8c21-358cfdd9485e, a690f3c5-55b3-4e6f-bc7d-d9964fcea328, a98900ac-2c09-4818-9d49-584d5a8924dc, abc8a1b5-d691-465b-ae7f-99038d8dcc32]
94191 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 9 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet
94214 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet
94214 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 9 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet => [b8d20e89-d9d4-4408-92eb-b75c5584cab3, c88becfb-96a2-44ac-a978-31e511773ca5, cad43dab-8d7b-46a8-92c7-1236670c0fc0, d98c0871-98a9-4521-ac10-d054cc43d0b1, e908edae-036f-45d7-a8f6-01173245cea6, f1c26591-9fed-4f07-8df4-5ed8a6b47aba, f701bef1-febb-4e95-8d13-4fd72dd640ed, fe2a4bf2-6b61-4949-8c76-4addcbf385d4, ff038b2d-3c24-4222-8b24-4753e549a0fa]
94226 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet
94238 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet
94238 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet => [01e37408-9b23-4ce1-8e08-3307385b471e, 04e3128e-06b1-4086-8c84-c5c54e30359f, 0f7b4829-87f3-4fd7-bc77-268741466e3f, 1406bd31-75b7-48e9-8a04-e7e9b6f625a6, 14c9681c-92a9-4ad4-8f99-b7e760469835, 15fda3b8-e5ef-4f31-ab8c-2427e7b6059f, 27243f0d-c566-44a8-a173-78b2a1600282, 29904eb1-bb90-4d16-8769-d684c318c585, 3ac7ebf1-a5fd-4dd3-96f4-1f1de50de0f9, 3ca7b4c5-d11b-4405-941e-65667a085992, 4ab67f7d-e650-4c1d-b07c-0144d157890f, 4f38dc16-6c48-4086-a069-7497a2b1e832, 4ffbb8fc-2de2-4c15-8ff6-71b62cc41fee, 528008b3-7042-4e10-9100-d9334a352927, 5dfc10c5-6060-4eb4-a970-33bbf99486c2, 6182f99f-71ab-409a-b656-b8766bb865ae, 61b68c14-bc8f-42e7-8eec-6564f13f7640, 632eac04-fae7-456d-8ad6-969fab55239a, 641c4428-dafa-4d3e-a3b0-ccbe570c7e89, 682e7197-edbb-43c6-a2a1-f3e898538b8f, 6d50476b-b09e-4b47-8cc7-889042e28c2a, 7ce60b40-9616-46b6-b6dd-34355cf1664a, 85720d19-7f24-49f4-8c6b-389dedb22767, 8a03b5a9-214a-4441-a344-f929fb00a8fb, 8a34e100-664e-470f-8c47-35f675e040f8, 93c16a9a-116c-4fc4-a317-8ac6142f69ed, 9903efa6-7ede-468b-af43-5d47c4ad19c9, aae55288-5596-4f71-be59-bd9808b9ada0, aefcc5a8-2d06-4802-b015-9da30e50fbb1, b850556e-39e2-4ab3-b8c4-80299f5e4779, c221730d-cf83-4524-afd3-2e06e9da87c9, c71b55da-f347-48cd-ac87-0f31fbb01fe7, d0d501e2-20ba-4759-8f30-65195d209d81, dbe143fb-39a1-408c-836d-4f626bd6a89d, dca4224b-f803-4290-9735-f7e96f027f76, dedaabfb-16ec-44a4-9aaa-1cae77c3e0f7, ee57401d-386e-45f9-8e9a-bdcaeff22ed9, f649f17b-0438-4880-8182-b0033c36d93e]
94330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=94}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=21}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=38}}}
94360 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
94360 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
94386 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195304
94387 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195304
94905 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
94905 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
95166 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
95166 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
95297 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
95321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195304 as complete
95321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195304
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet; isDirectory=false; length=450946; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet; isDirectory=false; length=450916; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195258.parquet; isDirectory=false; length=450761; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet; isDirectory=false; length=449351; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet; isDirectory=false; length=449300; replication=1; blocksize=33554432; modification_time=1584643982000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195258.parquet; isDirectory=false; length=449178; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet; isDirectory=false; length=450352; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet; isDirectory=false; length=450317; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195258.parquet; isDirectory=false; length=450187; replication=1; blocksize=33554432; modification_time=1584643980000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
96482 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195307
96778 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 95
96778 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
96953 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet
96987 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet
96987 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet => [070bf235-03fd-4423-9406-c5bc1531178c, 0ee52ff8-1b24-4c6f-a869-8eec986e9053, 1da70b92-f7ca-4b0e-b257-68f92cbb0105, 232c9e16-b138-45ca-8271-ddd571d4ec5e, 2592bbd2-e81e-4000-abe4-b3a519bf9022, 2c089b8c-9692-48d0-85e8-0b217ea482ce, 375d6489-0568-4d17-85f2-3d60db6ee7a6, 4a9c5504-01b3-4153-91ce-ca9e74369447, 4e83ffdb-ff1c-4333-bd2d-794877affdeb, 511c9b62-5ed3-4905-adbd-49bda4c45d58, 5d670d8d-b722-44b3-bd20-ed7c7b84702a, 5f0fe5eb-7e4f-4812-aeeb-ac8344ccec5d, 5f354e32-ee6e-43c5-8083-2ffc89d1831c, 63274066-2ffd-4286-81e5-de948a0d58ee, 65b6d11f-2ac8-4e14-b991-3a018dc05533, 6e4c3027-016b-45b3-90ad-ca41fc195649, 76074cdc-6793-45ee-a084-529d0917ad80, 82d87afd-4ae8-4ecc-bc93-06e5045937b1, 8c9be4e8-dfc8-43b0-a526-1a2cb502d913, 8cf5bdfb-74cb-4272-bf24-2b448dd03882, 8e2b2fe7-3000-4496-84d3-fb50505e0a19, 93cd7617-1c00-4388-855b-e7d9366de93c, 94792c90-048c-4db2-bcc9-769895df09e9, 97f12504-3105-4d0c-a881-9e59c572da7c, a033335d-ea0e-4fb4-9ce5-ee648c1fe2e8, a2fc3989-18d7-4acf-8f28-c17153b29114, a74f9b5e-12cb-49d5-891f-0ca42a028c54, afa94199-221b-4e7f-9ee5-ab6ce1a3fa26, b0ca91dd-defd-4d74-9925-2dbb99859890, b260bdfa-66f3-4de5-a022-ac87a0c88f41, baab27b7-e665-4647-aecb-5651f8715137, c7b281de-becf-445d-a3f8-b2bc0579f6aa, d928d3c7-546e-47f0-b506-6e413eaf2c72, e4662d9a-1722-4e2e-8b56-1790527ceee4, ef435952-c1d8-4a98-864e-f2035c007a09, f370dd4e-fb0e-4eda-bf17-99e4b22d9d0c, fec2c2d9-49aa-4c78-a3bb-9a77fb962056]
96997 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet
97018 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet
97018 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet => [027c4553-7771-4566-80c4-1aec459ba259, 096e32ac-d042-4386-9379-5b071fb2eebf, 251653e2-5e72-4fee-8060-91fb8fd071cc, 31d1bae3-927e-48f5-a5eb-67e1281acc5b, 41ae204b-f511-4211-853a-33b7ff434633, 4295f631-4113-49ce-8fb5-2e2ffc75fadf, 43037ad9-970b-4157-bd66-79a1fc715f03, 49fd0400-ca36-4ec4-9e70-c257e31a0738, 4f16fdb0-962f-4ad5-ac49-ec95b788f972, 513c01ea-fc9e-4d8e-847a-4b5065950151, 5997537e-a2c2-40c4-ad14-7f6e6f0d88f3]
97041 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 15 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet
97066 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet
97066 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 15 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet => [5d43c8fd-e9a9-4c34-9737-e52ad062b495, 5dceb8e5-03f2-4510-91c4-5f6785e9b874, 78387be6-ffb3-4b59-a792-80b011963582, 841d8935-d707-4e6e-84b3-449d9d4a7674, 96454905-3def-4fff-82f6-a60904247aa1, 9e840aad-f192-408e-88d8-0b31f1db8317, 9f2f6127-65b5-40bb-8c21-358cfdd9485e, adf02844-0360-43ae-b582-c5e4db13605c, b0905422-5a36-4ad7-8dfe-a10e7f08c6a4, c4566ce1-99d8-4cea-91e1-6d015df6c100, dabcb779-6cef-47e2-8c1f-ede6e605c7a5, e09c2309-825b-4add-986b-2381bdea85ac, e75d3e7f-9073-4a8b-af38-5de09a03b65d, ed3f700f-9a65-49de-9392-429598ee6630, ff038b2d-3c24-4222-8b24-4753e549a0fa]
97077 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet
97105 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet
97105 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet => [0372f083-7964-4448-9f45-5c97e88cfcd4, 04e3128e-06b1-4086-8c84-c5c54e30359f, 062909f6-6ab2-4400-a8fb-aa7ba5f0fce9, 0669e2d0-c87a-489a-8c8a-fc00a6fc4427, 0718d90e-b166-4da2-9fcc-a6483f44914b, 14c9681c-92a9-4ad4-8f99-b7e760469835, 15fda3b8-e5ef-4f31-ab8c-2427e7b6059f, 178c69a9-bc0c-4dfa-b017-a3392dfc64ae, 192e53cd-43c9-4ac9-bdd1-e2bcbf074b99, 1f57af2b-6050-423c-a6e4-77d4bc9cd693, 3606fb39-4c87-432f-bfb9-163464d71630, 40446c3c-5bf9-49a8-b85a-2c0a45d8dede, 405b9054-f2c9-45bc-9a6c-917d9951ec0f, 50a87258-b035-4d89-bf37-5ed2e8933fb0, 59195cce-24ff-40cd-8753-89ba4e3af00b, 5dfc10c5-6060-4eb4-a970-33bbf99486c2, 632eac04-fae7-456d-8ad6-969fab55239a, 9582c9ed-7c1b-4a5d-8925-15562e72ba91, 9ac614a6-5ad7-4db0-b547-b4cda8f33a1e, a10d0058-8cfb-4869-8174-03cd01d3b804, a3170519-7e44-4d6c-972f-a924dc5f4f51, a7ed562c-4ec3-4668-a070-cc64ae76dabd, adb6849d-e275-4cdf-8bb1-cbea0f47ccac, b1667950-f6f7-498b-89f9-7fdc136485b3, b65debcc-ab6a-48cf-9a9e-04e4019b22d0, d0121e13-d9e2-4678-9205-1bb993879076, d4c6954b-11fe-43be-bd62-a579fa905d1a, dbd6ab66-c2ba-42c5-a9b9-d4841bfe2f21, dcb2df37-21ae-41b3-a4c7-05b7d9474c01, f8efc934-134b-441c-b00d-1ba36683d158, fb4bffad-ac3d-4bd6-bb57-6ab7b643f48d, fd9daca3-458c-4ca7-913c-ca10dba367de]
97171 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=95}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=37}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=32}}}
97270 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
97270 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
97281 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195307
97281 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195307
97902 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
97902 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
98086 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
98086 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
98223 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
98231 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195307 as complete
98231 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195307
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet; isDirectory=false; length=450951; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet; isDirectory=false; length=450946; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195301.parquet; isDirectory=false; length=450916; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet; isDirectory=false; length=449401; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet; isDirectory=false; length=449351; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195301.parquet; isDirectory=false; length=449300; replication=1; blocksize=33554432; modification_time=1584643982000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet; isDirectory=false; length=450379; replication=1; blocksize=33554432; modification_time=1584643989000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet; isDirectory=false; length=450352; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195301.parquet; isDirectory=false; length=450317; replication=1; blocksize=33554432; modification_time=1584643983000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
99398 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195310
99689 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 92
99689 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
99849 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 40 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet
99878 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet
99878 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 40 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet => [129be2ed-dc5e-42ae-9be7-f036e19c799b, 147468fb-c9d6-4ef5-9e5a-ce50c51b674c, 1d832a92-381b-4b58-9aab-128713c9e7c9, 21b318ce-f19f-4cad-8932-b08f4d7acd6e, 2463545a-b57b-4156-9683-c4ec93a321b9, 26ba0f07-9ef2-4ea6-9535-243a8f6cd126, 293ff62a-c92b-436d-b162-d85e96c4bdad, 3692ed00-3b6d-4d6f-aac9-9329459953ae, 38db06dd-8011-4e02-8723-26d03757524e, 4a706413-6c00-4e6b-8629-43cade892efa, 4b879c20-52e3-401c-9f4f-a0403e70943e, 4c69fa64-3a56-4aa4-9fc8-dd71edf3636b, 55a6f703-623e-49ff-b537-1b790a4060c9, 59045f07-dfd0-4e5c-a716-b2d9e5997f53, 5aa168ac-40dd-436d-88a7-92f195560cb6, 632fd36d-bef5-48d0-b5c4-25eb88c3ab20, 680982b1-8daf-4093-9388-44dac51680cb, 707f06ba-f64b-49f6-9020-7fcd4c47a290, 76074cdc-6793-45ee-a084-529d0917ad80, 7d4c22fd-55cd-49f6-9e60-40b1f6118d30, 94792c90-048c-4db2-bcc9-769895df09e9, 96eb2d26-1cb6-44d6-83f2-8492d77bb7fa, 97f12504-3105-4d0c-a881-9e59c572da7c, a2fc3989-18d7-4acf-8f28-c17153b29114, a74f9b5e-12cb-49d5-891f-0ca42a028c54, a7a32f06-0fef-45ac-8f77-0628aebe25ed, b159840e-a5b6-46f2-83fe-c79ec3768b38, b77e74ca-6bff-46fe-87d1-cd67b2d7312f, bb4d3928-c6a8-4f33-b29c-b4aa53e17214, c7d5895d-27e9-4fc0-97bd-d0261cd09a7e, dbee6f38-72e6-4af8-b2bf-0639308695d9, dd207213-51cd-497f-b15b-6e6dc110ebd0, e08ded2d-c7a6-4d3a-bc1e-fa17d961040b, e72755ab-ce05-4231-a419-02c66c5d10fe, ea3c8bc4-224b-4967-bdcf-121a133f71f1, ee6396f9-a133-4f89-a274-016cafe4f353, ef414f0c-e7b4-4928-a035-876249ecb5ad, f370dd4e-fb0e-4eda-bf17-99e4b22d9d0c, fc8f42db-128d-4f46-abcd-236939c9b093, fdaad461-3da2-48a9-9b02-873e355d3c18]
99894 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 6 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet
99941 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet
99941 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 6 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet => [00cc4de3-b585-434f-bda5-dfdd90684431, 015b81e4-5788-499b-9850-4bf89ff07689, 064460c4-dd4c-4f07-b096-c33afb1a8a94, 0b476ad2-c2ba-4ce7-8d2c-c7bb5757be20, 0d25450c-98f0-4ac9-aba8-150be0c67267, 0fe48614-2bf7-44bc-ac81-51d7b4338d02]
99963 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet
100000 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet
100000 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet => [1bc41e22-89fd-4ed4-8e6b-53f7b929e953, 31d1bae3-927e-48f5-a5eb-67e1281acc5b, 332b642b-71fd-4d57-a42a-f7791d26e519, 3db79d9f-665e-43eb-9782-8d96dfedac05, 41ae204b-f511-4211-853a-33b7ff434633, 5e8ec758-39e5-412b-a974-002d170fcb1d, 841d8935-d707-4e6e-84b3-449d9d4a7674, 8c826be0-6061-44fc-8edb-34d317693e7a, 96454905-3def-4fff-82f6-a60904247aa1, a33fbaac-40e8-4404-9f39-7f9b6beb0222, b8052dc4-cd02-42ec-a6c2-a5d952657432, c3d6c966-d7e5-47c7-a6c6-983390c391e2, c84db2c1-5147-4f6d-8543-82285d3a0e1a, cad43dab-8d7b-46a8-92c7-1236670c0fc0, d7a4d753-541e-4a00-b453-2074185fcd2b, dabcb779-6cef-47e2-8c1f-ede6e605c7a5, e0b5bae7-59d5-433f-9a1f-78b6d4d39b7f, e0ce1063-cd94-4e8a-b5cb-5eb992318ad0, f1c26591-9fed-4f07-8df4-5ed8a6b47aba]
100012 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 27 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet
100019 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet
100019 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 27 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet => [0358bb05-f69e-4d8d-9cca-43be2c4ce83a, 08ab014d-a5cf-475d-97e0-d4f01b8336b7, 21ba62a6-18d1-4c96-a0d8-cae90532cf25, 24d2b681-4e70-4bd1-8375-51ead3d11c22, 33fb412f-8f68-4874-99cb-ada771bd5c15, 3e50efb3-57ef-4ad6-a818-289010d01457, 466eca40-4559-43ff-925e-eaa4a830da02, 53242c5c-c1da-4baa-b237-4fcbb9c0964f, 59195cce-24ff-40cd-8753-89ba4e3af00b, 641c4428-dafa-4d3e-a3b0-ccbe570c7e89, 85720d19-7f24-49f4-8c6b-389dedb22767, 8a34e100-664e-470f-8c47-35f675e040f8, 8dce87f7-75ea-4fa5-abd3-f33e8c2c66b7, 93c16a9a-116c-4fc4-a317-8ac6142f69ed, 9903efa6-7ede-468b-af43-5d47c4ad19c9, 9ce1c500-903e-477d-9150-5ccc5d160b2f, a10d0058-8cfb-4869-8174-03cd01d3b804, a3170519-7e44-4d6c-972f-a924dc5f4f51, a6464121-e7e2-494d-95a6-3a159fc6279d, acb3c005-3aea-451d-9e1b-c5acbe007aa5, b0ffe05f-b8f8-41a2-99fb-0d0a3bd9ab69, b2f514a0-d7a9-42ca-81bd-08f728e37fc1, cdeadda3-18b0-410b-a97f-d070593d50ce, e42b9c42-b80f-4145-be84-84a785c6b1ad, e9fa38c4-90b9-4edc-a59f-f9d7dc9598e0, fb4bffad-ac3d-4bd6-bb57-6ab7b643f48d, fd9daca3-458c-4ca7-913c-ca10dba367de]
100111 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=92}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=40}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=25}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
100125 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
100125 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
100134 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195310
100134 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195310
100564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
100564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
100724 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
100725 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
100865 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
100876 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195310 as complete
100876 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195310
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195310.parquet; isDirectory=false; length=450985; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet; isDirectory=false; length=450951; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195304.parquet; isDirectory=false; length=450946; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet; isDirectory=false; length=449418; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet; isDirectory=false; length=449401; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195304.parquet; isDirectory=false; length=449351; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195310.parquet; isDirectory=false; length=450397; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet; isDirectory=false; length=450379; replication=1; blocksize=33554432; modification_time=1584643989000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195304.parquet; isDirectory=false; length=450352; replication=1; blocksize=33554432; modification_time=1584643985000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
102052 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195313
102340 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 95
102340 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
102504 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195310.parquet
102509 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 175 row keys from /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195310.parquet
102509 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195310.parquet => [070bf235-03fd-4423-9406-c5bc1531178c, 0ee52ff8-1b24-4c6f-a869-8eec986e9053, 11374b07-6595-47ca-a958-d1060808a8bd, 1c19d75e-3b81-41e1-8756-4a94f67276e1, 2463545a-b57b-4156-9683-c4ec93a321b9, 24ff1716-a84d-464a-a433-27f17ac360d7, 27fac0f6-6093-4e94-9e98-2be473287609, 3f7e194a-bc06-42d5-9fce-f57a9630eee3, 47d745cf-1524-4034-875b-07c1719fa3a9, 511c9b62-5ed3-4905-adbd-49bda4c45d58, 51afd4d2-c612-463e-81ca-e9416173d57a, 59045f07-dfd0-4e5c-a716-b2d9e5997f53, 5bd77f51-ab84-4897-bc85-a9cabbda8e29, 6636a3c2-b67c-4388-a602-eacc1b6f38ab, 6e4c3027-016b-45b3-90ad-ca41fc195649, 733328b5-9f28-4178-943d-f450d2a1c1ab, 93cd7617-1c00-4388-855b-e7d9366de93c, 96eb2d26-1cb6-44d6-83f2-8492d77bb7fa, 9ec6010b-8d40-49be-ab0a-0e8249944080, a7a32f06-0fef-45ac-8f77-0628aebe25ed, b260bdfa-66f3-4de5-a022-ac87a0c88f41, b4e4ec75-5bb3-407e-b50f-74c5a61b259a, b5b0c3ff-c080-4454-a066-7b10b8ce6ecc, c1b2b935-d57c-45c0-9ef1-399c641e003b, d55aef78-17dc-4165-b926-48d78d303bac, eb73aef1-a189-4e72-bb16-3901ee0b2a03, ede2f789-1bfc-4001-a84f-9f6be4f20a6a, fe9b1a3a-b184-4bb1-b514-cb785822e634, fffee8b6-939b-4188-a59f-c99e5f363ed5]
102521 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet
102529 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet
102529 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet => [02f93ecb-226b-47cd-8c99-40ce61d003b7, 064460c4-dd4c-4f07-b096-c33afb1a8a94, 1915da38-3f79-4599-93a2-62926a16e243, 251653e2-5e72-4fee-8060-91fb8fd071cc, 28cf81f5-a30e-429d-a0fe-d2250a41f4ed, 3727f9a5-7500-4d20-98bf-dd1fa6fd33ee, 37526e44-424c-4abf-8255-9b565ae2a416, 4c05c7cb-67a6-402c-8590-61522413b350, 56170900-6412-4587-86a6-e00143f7e5df, 581a863b-a205-4922-ac38-7a278087e8a4, 618a3475-46b3-4a68-aa5b-ef53fbe4deab, 6acc5657-6c45-41bf-aaca-c6e85c07f66d, 6b4847b0-fa26-4dd6-9bf6-356c7be66a73, 7c62e33f-395f-47cf-b368-8d42b0169182, 89ad623f-e864-4f05-8ca8-08e003e6bea4, 8db81187-4722-4558-bf0f-76a9428dc2f7, 98390fe9-fadb-4d01-a653-90b2a70d9fad, 9b3792f1-e31f-4dfe-89f5-425f01950aa9, 9f2f6127-65b5-40bb-8c21-358cfdd9485e]
102554 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet
102560 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 157 row keys from /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet
102560 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet => [ad9e34d2-e5f9-4570-8789-5bc4cf714980, c84db2c1-5147-4f6d-8543-82285d3a0e1a, ce4b3619-4a1f-4b04-b580-20e11f52a9cc, d3417891-0188-4d1f-ba30-db2daa93edde, d37a5d70-950e-4b80-8525-64b79a054d0a, d383c92d-c7ff-42ec-80bc-c51117656958, d9067e8b-8899-4057-8598-7e2c0869f20d, d9f371fe-e2e9-4e53-916e-691a42dba80e, e09c2309-825b-4add-986b-2381bdea85ac, ed3f700f-9a65-49de-9392-429598ee6630]
102574 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195310.parquet
102612 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 168 row keys from /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195310.parquet
102612 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195310.parquet => [04391b18-982d-44df-aec4-9bd0118084f3, 0669e2d0-c87a-489a-8c8a-fc00a6fc4427, 0718d90e-b166-4da2-9fcc-a6483f44914b, 14414995-ca72-4905-b11f-4e83303963f5, 14c9681c-92a9-4ad4-8f99-b7e760469835, 1a054602-42f3-48ae-9e4e-1d3804016f69, 1e995a7b-2e99-4ef0-95a0-49ff2b0ff534, 27243f0d-c566-44a8-a173-78b2a1600282, 28898d9d-1307-4070-8b19-1a94b7273407, 2ec389e6-f9d8-469d-a064-44152ee07724, 2fe23727-557e-485c-a7e5-ad103b783022, 3efc15e4-6ec4-4277-ba54-94fbdae8b477, 3fa289fb-8da3-4674-872c-47a2131e14ee, 46232136-3edf-4ce8-8b2a-9f5f3f45a4a2, 4aab791b-efe5-403d-81f7-c4ef99cfb59a, 4f38dc16-6c48-4086-a069-7497a2b1e832, 53242c5c-c1da-4baa-b237-4fcbb9c0964f, 5740ddfe-fc24-412b-80c8-881fa119703c, 6960df46-8087-4265-b23a-d79c148cc26d, 743ea730-c8b1-4e54-a743-68a21459900c, 96372425-816d-4bcb-b551-f97a79f4a6e8, 989e43ad-061b-47cf-8961-43f60554b98a, 9addbeba-b5e0-4c29-81c3-6bacc74e4dd3, a10d0058-8cfb-4869-8174-03cd01d3b804, aefcc5a8-2d06-4802-b015-9da30e50fbb1, af829356-c3c6-4ffa-a96c-3303f1afeb96, b1667950-f6f7-498b-89f9-7fdc136485b3, b47ce91b-2e0a-47ed-861b-572eb78e5515, c71b55da-f347-48cd-ac87-0f31fbb01fe7, cdeadda3-18b0-410b-a97f-d070593d50ce, d4c6954b-11fe-43be-bd62-a579fa905d1a, d4df1523-f8a7-455b-9a4c-466fa39aef11, dbd6ab66-c2ba-42c5-a9b9-d4841bfe2f21, dbe143fb-39a1-408c-836d-4f626bd6a89d, dca4224b-f803-4290-9735-f7e96f027f76, f649f17b-0438-4880-8182-b0033c36d93e, f71a9e83-0972-4322-885d-eca975aaa113]
102693 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=95}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=37}}}
102746 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
102746 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=26445654-e577-41c9-982f-0a865ce28eb4}, 1=BucketInfo {bucketType=UPDATE, fileLoc=174b94ea-f487-4d1b-9686-f5ae82323e7d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=8e773186-6452-4a9d-bd8f-01603ae843b8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{26445654-e577-41c9-982f-0a865ce28eb4=0, 174b94ea-f487-4d1b-9686-f5ae82323e7d=1, 8e773186-6452-4a9d-bd8f-01603ae843b8=2}
102758 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195313
102758 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195313
103186 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
103186 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
103345 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
103345 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
103529 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
103557 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195313 as complete
103557 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195313
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195313.parquet; isDirectory=false; length=451073; replication=1; blocksize=33554432; modification_time=1584643994000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195310.parquet; isDirectory=false; length=450985; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2016/03/15/174b94ea-f487-4d1b-9686-f5ae82323e7d_1_20200319195307.parquet; isDirectory=false; length=450951; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195313.parquet; isDirectory=false; length=449505; replication=1; blocksize=33554432; modification_time=1584643994000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195310.parquet; isDirectory=false; length=449418; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/16/26445654-e577-41c9-982f-0a865ce28eb4_0_20200319195307.parquet; isDirectory=false; length=449401; replication=1; blocksize=33554432; modification_time=1584643988000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195313.parquet; isDirectory=false; length=450495; replication=1; blocksize=33554432; modification_time=1584643994000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195310.parquet; isDirectory=false; length=450397; replication=1; blocksize=33554432; modification_time=1584643991000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Data File - HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit2085488211029850149/2015/03/17/8e773186-6452-4a9d-bd8f-01603ae843b8_2_20200319195307.parquet; isDirectory=false; length=450379; replication=1; blocksize=33554432; modification_time=1584643989000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
103804 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195314
103999 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
103999 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
104137 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195314
104137 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195314
104330 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104339 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104339 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104339 [pool-592-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104398 [pool-592-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104412 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104413 [pool-593-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104442 [pool-593-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104455 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104455 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104455 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104456 [pool-594-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104482 [pool-594-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104505 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104506 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104506 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104506 [pool-595-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104536 [pool-595-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104560 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104560 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104560 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104561 [pool-596-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104588 [pool-596-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104601 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104602 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104602 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104602 [pool-597-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104640 [pool-597-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104654 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104655 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104655 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104655 [pool-598-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104682 [pool-598-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104695 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104696 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104696 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104696 [pool-599-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104739 [pool-599-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104772 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104772 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104772 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104772 [pool-600-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104850 [pool-600-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104876 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104876 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104876 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104877 [pool-601-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104915 [pool-601-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104928 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104928 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104928 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104929 [pool-602-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
104962 [pool-602-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
104979 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
104979 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
104979 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
104979 [pool-603-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105013 [pool-603-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105045 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105045 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105046 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105046 [pool-604-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105079 [pool-604-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105097 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105097 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105097 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105097 [pool-605-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105124 [pool-605-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105140 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105140 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105140 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105140 [pool-606-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105169 [pool-606-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105193 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105194 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105194 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105194 [pool-607-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105221 [pool-607-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105258 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105258 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105258 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105260 [pool-608-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105300 [pool-608-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105320 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105321 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105321 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105321 [pool-609-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105354 [pool-609-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105395 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105395 [pool-610-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105421 [pool-610-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105437 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105438 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105438 [pool-611-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105465 [pool-611-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105490 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105491 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105491 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105495 [pool-612-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105522 [pool-612-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105546 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105546 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105546 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105547 [pool-613-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105585 [pool-613-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105602 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105603 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105603 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105603 [pool-614-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105635 [pool-614-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105651 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105651 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105651 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105652 [pool-615-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105692 [pool-615-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105704 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105704 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105704 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105705 [pool-616-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105740 [pool-616-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105757 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105758 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105758 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105758 [pool-617-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105796 [pool-617-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105809 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105809 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105809 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105810 [pool-618-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105844 [pool-618-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105873 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105873 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105873 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105874 [pool-619-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105901 [pool-619-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105916 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105916 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105916 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105925 [pool-620-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
105957 [pool-620-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
105989 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
105990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
105990 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
105990 [pool-621-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106075 [pool-621-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106104 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106104 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106104 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106104 [pool-622-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106179 [pool-622-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106199 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106199 [pool-623-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106236 [pool-623-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106257 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106258 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106258 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106258 [pool-624-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106286 [pool-624-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106299 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106300 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106300 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106300 [pool-625-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106328 [pool-625-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106352 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106352 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106352 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106352 [pool-626-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106382 [pool-626-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106397 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106398 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106398 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106398 [pool-627-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106427 [pool-627-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106464 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106465 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106465 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106465 [pool-628-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106501 [pool-628-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106547 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106548 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106548 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106548 [pool-629-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106593 [pool-629-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106611 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106611 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106611 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106612 [pool-630-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106646 [pool-630-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106663 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106664 [pool-631-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106697 [pool-631-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106717 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106717 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106717 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106718 [pool-632-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106742 [pool-632-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
106756 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
106756 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
106756 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
106757 [pool-633-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
106987 [pool-633-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107003 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107004 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107004 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107004 [pool-634-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107031 [pool-634-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107044 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107045 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107045 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107045 [pool-635-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107080 [pool-635-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107093 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107094 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107094 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107094 [pool-636-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107122 [pool-636-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107141 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107142 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107142 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107142 [pool-637-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107168 [pool-637-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107203 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107204 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107204 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107204 [pool-638-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107231 [pool-638-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107262 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107263 [pool-639-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107289 [pool-639-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107302 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107303 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107303 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107303 [pool-640-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107336 [pool-640-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107350 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107351 [pool-641-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107379 [pool-641-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107401 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107402 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107402 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107402 [pool-642-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107436 [pool-642-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107458 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107458 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107458 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107459 [pool-643-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107490 [pool-643-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107503 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107504 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107504 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107504 [pool-644-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107530 [pool-644-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107584 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107584 [pool-645-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107672 [pool-645-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107685 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107685 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107685 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107686 [pool-646-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107811 [pool-646-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107848 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107848 [pool-647-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
107848 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107849 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107924 [pool-647-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
107948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
107948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
107948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
107949 [pool-648-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108042 [pool-648-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108064 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108064 [pool-649-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108114 [pool-649-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108145 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108145 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108145 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108146 [pool-650-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108201 [pool-650-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108226 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108227 [pool-651-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108255 [pool-651-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108269 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108269 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108269 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108269 [pool-652-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108302 [pool-652-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108316 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108316 [pool-653-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108348 [pool-653-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108369 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108370 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108370 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108370 [pool-654-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108406 [pool-654-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108443 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108443 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108443 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108444 [pool-655-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108471 [pool-655-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108485 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108486 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108486 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108486 [pool-656-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108515 [pool-656-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108529 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108530 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108530 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108530 [pool-657-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108580 [pool-657-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108610 [pool-658-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108690 [pool-658-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108708 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108709 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108709 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108709 [pool-659-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108753 [pool-659-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108788 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108789 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108789 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108789 [pool-660-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108817 [pool-660-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108837 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108837 [pool-661-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108868 [pool-661-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108883 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108883 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108883 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108884 [pool-662-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108916 [pool-662-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
108931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
108931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
108931 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
108931 [pool-663-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
108984 [pool-663-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
109018 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
109019 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
109019 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
109019 [pool-664-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
109062 [pool-664-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
109077 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
109077 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
109077 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
109077 [pool-665-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
109109 [pool-665-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
109124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
109124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
109124 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
109124 [pool-666-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
109158 [pool-666-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
109193 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
109193 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
109575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
109575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
109711 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
109722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195314 as complete
109722 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195314
112106 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 75
112106 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
112200 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/00326b51-d9f7-468c-93e6-99ecc7563f4e_74_20200319195314.parquet
112207 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/00326b51-d9f7-468c-93e6-99ecc7563f4e_74_20200319195314.parquet
112207 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/00326b51-d9f7-468c-93e6-99ecc7563f4e_74_20200319195314.parquet => [cfd899da-654c-4ce8-8013-20fb22edab01]
112217 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/0113af91-4e26-4a9f-86c1-c8705dab81f3_53_20200319195314.parquet
112222 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/0113af91-4e26-4a9f-86c1-c8705dab81f3_53_20200319195314.parquet
112222 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/0113af91-4e26-4a9f-86c1-c8705dab81f3_53_20200319195314.parquet => [f599d4d0-30cf-4a56-b01a-ea503f094314]
112231 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/02133c36-13c6-471e-b865-24fa97723894_14_20200319195314.parquet
112261 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/02133c36-13c6-471e-b865-24fa97723894_14_20200319195314.parquet
112261 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/02133c36-13c6-471e-b865-24fa97723894_14_20200319195314.parquet => [6bf4469c-0874-4315-9e3b-884fd727b64b]
112281 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/0f6ae072-2c1e-40b1-9d84-6ca1d71dd727_0_20200319195314.parquet
112308 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/0f6ae072-2c1e-40b1-9d84-6ca1d71dd727_0_20200319195314.parquet
112308 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/0f6ae072-2c1e-40b1-9d84-6ca1d71dd727_0_20200319195314.parquet => [09d0ce09-43c5-4bf7-a3e3-a0b05bfb3922]
112317 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/15a3f4bf-b449-4482-89fd-1171ac25ed84_56_20200319195314.parquet
112327 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/15a3f4bf-b449-4482-89fd-1171ac25ed84_56_20200319195314.parquet
112327 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/15a3f4bf-b449-4482-89fd-1171ac25ed84_56_20200319195314.parquet => [0c039ee4-295c-4a56-8acd-b2cea55d5a98]
112335 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/17604588-a48c-45b6-a076-d72f73f9085c_63_20200319195314.parquet
112351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/17604588-a48c-45b6-a076-d72f73f9085c_63_20200319195314.parquet
112351 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/17604588-a48c-45b6-a076-d72f73f9085c_63_20200319195314.parquet => [58b6407f-11db-4b26-8727-5ae9ef3f2d33]
112360 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/1cccace7-bea5-417d-8c89-63097c606041_30_20200319195314.parquet
112371 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/1cccace7-bea5-417d-8c89-63097c606041_30_20200319195314.parquet
112371 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/1cccace7-bea5-417d-8c89-63097c606041_30_20200319195314.parquet => [d784685e-4b5e-4566-9dae-294adab9c6be]
112386 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/1ec42e3c-1833-4f7f-a72c-4dda76bdf8b3_44_20200319195314.parquet
112395 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/1ec42e3c-1833-4f7f-a72c-4dda76bdf8b3_44_20200319195314.parquet
112395 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/1ec42e3c-1833-4f7f-a72c-4dda76bdf8b3_44_20200319195314.parquet => [8e8cbf21-1cf2-43de-878e-fa484f5bdc61]
112410 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/26b78b69-d4ae-40ae-9e1f-02c88cf90a40_6_20200319195314.parquet
112418 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/26b78b69-d4ae-40ae-9e1f-02c88cf90a40_6_20200319195314.parquet
112418 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/26b78b69-d4ae-40ae-9e1f-02c88cf90a40_6_20200319195314.parquet => [3005119e-9636-4b78-a716-b2a65e91879e]
112427 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/278e29b4-0701-4d87-8397-c43b54b14f1c_70_20200319195314.parquet
112434 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/278e29b4-0701-4d87-8397-c43b54b14f1c_70_20200319195314.parquet
112434 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/278e29b4-0701-4d87-8397-c43b54b14f1c_70_20200319195314.parquet => [b2bbebb8-aa1c-4bb7-b5ed-5ad9fa7b7c9c]
112445 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/286cf5dc-aa08-47bb-aea1-63584ef5f05f_26_20200319195314.parquet
112450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/286cf5dc-aa08-47bb-aea1-63584ef5f05f_26_20200319195314.parquet
112450 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/286cf5dc-aa08-47bb-aea1-63584ef5f05f_26_20200319195314.parquet => [c1c03c7f-f0c0-4c77-9de8-a44a146504df]
112459 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/32f87cbb-e95f-446e-a120-6d359ffc421b_16_20200319195314.parquet
112464 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/32f87cbb-e95f-446e-a120-6d359ffc421b_16_20200319195314.parquet
112464 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/32f87cbb-e95f-446e-a120-6d359ffc421b_16_20200319195314.parquet => [7fbd1519-143d-4a3e-806f-1925b3750e56]
112473 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/35bc88f9-96ef-43da-beff-4039841f9442_67_20200319195314.parquet
112482 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/35bc88f9-96ef-43da-beff-4039841f9442_67_20200319195314.parquet
112482 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/35bc88f9-96ef-43da-beff-4039841f9442_67_20200319195314.parquet => [97ca241b-aac4-40a2-8ecc-ae629f4d97cc]
112491 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/384ac636-2677-4d18-ad83-be06f172bf70_66_20200319195314.parquet
112496 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/384ac636-2677-4d18-ad83-be06f172bf70_66_20200319195314.parquet
112496 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/384ac636-2677-4d18-ad83-be06f172bf70_66_20200319195314.parquet => [96f8955d-e82c-4bf9-96bc-92cc1ed06055]
112522 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/3a73d32f-1fd7-4f20-8cb8-aa940734ead5_71_20200319195314.parquet
112690 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/3a73d32f-1fd7-4f20-8cb8-aa940734ead5_71_20200319195314.parquet
112690 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/3a73d32f-1fd7-4f20-8cb8-aa940734ead5_71_20200319195314.parquet => [b62a40d9-485e-4332-9e30-456ddc7c1a7c]
112702 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/43c05654-9280-4292-93e4-295165df226b_11_20200319195314.parquet
112707 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/43c05654-9280-4292-93e4-295165df226b_11_20200319195314.parquet
112707 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/43c05654-9280-4292-93e4-295165df226b_11_20200319195314.parquet => [4d8bc9e6-a308-486e-9d78-ae536a08537f]
112718 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/459153ac-8192-4866-a2ff-a1f4e7ca4dee_49_20200319195314.parquet
112725 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/459153ac-8192-4866-a2ff-a1f4e7ca4dee_49_20200319195314.parquet
112725 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/459153ac-8192-4866-a2ff-a1f4e7ca4dee_49_20200319195314.parquet => [a95f4f7a-3926-423c-8c73-8ef26c8fa6a8]
112734 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/4e37d6bd-c9c4-4c69-be9a-a4d0f268ac6f_17_20200319195314.parquet
112767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/4e37d6bd-c9c4-4c69-be9a-a4d0f268ac6f_17_20200319195314.parquet
112767 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/4e37d6bd-c9c4-4c69-be9a-a4d0f268ac6f_17_20200319195314.parquet => [82b00675-e5f9-47b8-9125-9fafc0dfe466]
112776 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/514e524c-e809-4f73-8f2c-480982495a09_58_20200319195314.parquet
112795 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/514e524c-e809-4f73-8f2c-480982495a09_58_20200319195314.parquet
112796 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/514e524c-e809-4f73-8f2c-480982495a09_58_20200319195314.parquet => [2f55e529-da4b-4146-b0e3-b1ac5eefebea]
112805 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/516dfc54-9bef-4a2f-8959-ee14b2b140f5_33_20200319195314.parquet
112823 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/516dfc54-9bef-4a2f-8959-ee14b2b140f5_33_20200319195314.parquet
112823 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/516dfc54-9bef-4a2f-8959-ee14b2b140f5_33_20200319195314.parquet => [e519156c-6bc3-4d62-966c-4f72f8faa559]
112832 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/5a19fcd0-0606-4a81-a79b-b86f14d76bbe_19_20200319195314.parquet
112848 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/5a19fcd0-0606-4a81-a79b-b86f14d76bbe_19_20200319195314.parquet
112848 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/5a19fcd0-0606-4a81-a79b-b86f14d76bbe_19_20200319195314.parquet => [97715a15-c09b-4fe8-bac5-90329b15c041]
112858 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/5a47f26e-9ba4-4746-a2af-f3df24e07ee6_12_20200319195314.parquet
112863 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/5a47f26e-9ba4-4746-a2af-f3df24e07ee6_12_20200319195314.parquet
112863 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/5a47f26e-9ba4-4746-a2af-f3df24e07ee6_12_20200319195314.parquet => [57b70e3f-d37d-4353-9d8b-305b8677079d]
112877 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/5ae4baa6-04d7-4c40-9dc2-61da5f856951_48_20200319195314.parquet
112882 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/5ae4baa6-04d7-4c40-9dc2-61da5f856951_48_20200319195314.parquet
112882 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/5ae4baa6-04d7-4c40-9dc2-61da5f856951_48_20200319195314.parquet => [9b8f0664-5074-4df4-b97e-4b37e30ee394]
112893 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/5ba9af8a-14d3-4f36-8e44-0fa38d91177f_39_20200319195314.parquet
112897 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/5ba9af8a-14d3-4f36-8e44-0fa38d91177f_39_20200319195314.parquet
112897 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/5ba9af8a-14d3-4f36-8e44-0fa38d91177f_39_20200319195314.parquet => [50c2eb8a-589c-405f-95cb-f348775deb59]
112906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/5eef6f3a-d15b-4136-9229-5a3f29df9d7e_15_20200319195314.parquet
112911 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/5eef6f3a-d15b-4136-9229-5a3f29df9d7e_15_20200319195314.parquet
112911 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/5eef6f3a-d15b-4136-9229-5a3f29df9d7e_15_20200319195314.parquet => [74d5a855-fe3d-41d1-8480-d7a3bf3c41c4]
112919 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/6751b99e-1edc-48f7-8d58-a90165e2637b_54_20200319195314.parquet
112935 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/6751b99e-1edc-48f7-8d58-a90165e2637b_54_20200319195314.parquet
112935 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/6751b99e-1edc-48f7-8d58-a90165e2637b_54_20200319195314.parquet => [02fa330a-d09b-4471-8162-80f116f06df2]
112944 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/6982700c-89c2-41ef-8956-be06a8bee247_38_20200319195314.parquet
112949 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/6982700c-89c2-41ef-8956-be06a8bee247_38_20200319195314.parquet
112949 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/6982700c-89c2-41ef-8956-be06a8bee247_38_20200319195314.parquet => [2af3ef6c-1874-475f-bee5-378804eaac38]
112958 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/6cbcd973-abdb-441c-b9ac-d0c60d0dd103_13_20200319195314.parquet
112965 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/6cbcd973-abdb-441c-b9ac-d0c60d0dd103_13_20200319195314.parquet
112965 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/6cbcd973-abdb-441c-b9ac-d0c60d0dd103_13_20200319195314.parquet => [63faf644-981a-457a-bbdb-e4e5fd393bb1]
112974 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/6dda64f9-633c-4a55-a066-6747daafe8d0_9_20200319195314.parquet
112978 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/6dda64f9-633c-4a55-a066-6747daafe8d0_9_20200319195314.parquet
112978 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/6dda64f9-633c-4a55-a066-6747daafe8d0_9_20200319195314.parquet => [452005d7-a926-43e5-b3cd-d47c70e41fe6]
112988 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/6eef51ee-27ad-4118-b081-862332374d3e_46_20200319195314.parquet
112999 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/6eef51ee-27ad-4118-b081-862332374d3e_46_20200319195314.parquet
112999 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/6eef51ee-27ad-4118-b081-862332374d3e_46_20200319195314.parquet => [9409e0ea-f802-4d7d-8b75-367b4ff69640]
113007 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/717c9e60-9ff6-4762-a17a-4935367923da_32_20200319195314.parquet
113017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/717c9e60-9ff6-4762-a17a-4935367923da_32_20200319195314.parquet
113017 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/717c9e60-9ff6-4762-a17a-4935367923da_32_20200319195314.parquet => [e4f302cf-c88b-46b6-8e91-1a59dac14c01]
113025 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7498c1b5-0cc4-4888-841b-c413b602a78a_4_20200319195314.parquet
113030 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7498c1b5-0cc4-4888-841b-c413b602a78a_4_20200319195314.parquet
113030 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7498c1b5-0cc4-4888-841b-c413b602a78a_4_20200319195314.parquet => [1a94e6e8-475b-4e34-8f53-1619f618a5a5]
113039 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/75ca060c-ae47-4243-adde-db1c421b979b_43_20200319195314.parquet
113043 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/75ca060c-ae47-4243-adde-db1c421b979b_43_20200319195314.parquet
113043 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/75ca060c-ae47-4243-adde-db1c421b979b_43_20200319195314.parquet => [8955f3c1-7b6a-498d-9326-f6f2d38ddfb0]
113052 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7701a676-a781-456d-8dd1-6276aef35ef8_2_20200319195314.parquet
113058 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7701a676-a781-456d-8dd1-6276aef35ef8_2_20200319195314.parquet
113058 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7701a676-a781-456d-8dd1-6276aef35ef8_2_20200319195314.parquet => [0ee6cd99-aa16-4cfe-8ac4-bc88cae330e4]
113079 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7af35ed2-66ef-4e2e-8b50-325eebe9148c_7_20200319195314.parquet
113087 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7af35ed2-66ef-4e2e-8b50-325eebe9148c_7_20200319195314.parquet
113087 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7af35ed2-66ef-4e2e-8b50-325eebe9148c_7_20200319195314.parquet => [3447b3a2-7982-4516-9da9-d46ecd08a061]
113096 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7c1d973d-5cb0-48a8-b784-4d9e92c52107_1_20200319195314.parquet
113100 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7c1d973d-5cb0-48a8-b784-4d9e92c52107_1_20200319195314.parquet
113100 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7c1d973d-5cb0-48a8-b784-4d9e92c52107_1_20200319195314.parquet => [0a4006d4-921c-40ee-8b0b-396dcc99a642]
113109 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/7e1b2cbd-ae46-46a2-b5cb-d4a420629a62_52_20200319195314.parquet
113113 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/7e1b2cbd-ae46-46a2-b5cb-d4a420629a62_52_20200319195314.parquet
113113 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/7e1b2cbd-ae46-46a2-b5cb-d4a420629a62_52_20200319195314.parquet => [e9e3d083-2a77-4eb6-a795-f481205a554f]
113128 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7eaaf714-673f-45c0-9616-40d470cabe9e_28_20200319195314.parquet
113167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7eaaf714-673f-45c0-9616-40d470cabe9e_28_20200319195314.parquet
113167 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7eaaf714-673f-45c0-9616-40d470cabe9e_28_20200319195314.parquet => [cee3bfa7-20d2-42c4-9cca-ae38d3700d46]
113175 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/7f01f8cb-9792-494a-8adf-2a546085cd46_60_20200319195314.parquet
113197 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/7f01f8cb-9792-494a-8adf-2a546085cd46_60_20200319195314.parquet
113197 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/7f01f8cb-9792-494a-8adf-2a546085cd46_60_20200319195314.parquet => [426ec2ca-3028-4689-af04-795667e894ca]
113205 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/7f4112b1-5223-4013-9da1-f59095fa082f_61_20200319195314.parquet
113225 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/7f4112b1-5223-4013-9da1-f59095fa082f_61_20200319195314.parquet
113225 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/7f4112b1-5223-4013-9da1-f59095fa082f_61_20200319195314.parquet => [496af42e-5bdb-4143-b95f-e7fd7ebcdfd9]
113234 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/7fe42994-79d2-4966-9236-fae6b1220eaf_42_20200319195314.parquet
113246 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/7fe42994-79d2-4966-9236-fae6b1220eaf_42_20200319195314.parquet
113246 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/7fe42994-79d2-4966-9236-fae6b1220eaf_42_20200319195314.parquet => [79afb8cd-b260-46b8-a64a-1984ca1d7945]
113254 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/7ffeebc8-7485-4b5e-bf3c-2265b72d9e27_21_20200319195314.parquet
113259 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/7ffeebc8-7485-4b5e-bf3c-2265b72d9e27_21_20200319195314.parquet
113259 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/7ffeebc8-7485-4b5e-bf3c-2265b72d9e27_21_20200319195314.parquet => [b300a3e0-e73a-4d7f-a41b-beb7342083ed]
113269 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/8114056c-5fb5-4c71-b7d3-df4c5f0b3e3b_59_20200319195314.parquet
113273 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/8114056c-5fb5-4c71-b7d3-df4c5f0b3e3b_59_20200319195314.parquet
113273 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/8114056c-5fb5-4c71-b7d3-df4c5f0b3e3b_59_20200319195314.parquet => [40ba067c-8229-4019-9a28-2e44c0add9d4]
113288 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/8352775a-68fe-4895-ac40-ce68ebb1ccbe_40_20200319195314.parquet
113308 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/8352775a-68fe-4895-ac40-ce68ebb1ccbe_40_20200319195314.parquet
113308 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/8352775a-68fe-4895-ac40-ce68ebb1ccbe_40_20200319195314.parquet => [5baf20f5-c5b1-4926-bf81-351f097f5113]
113317 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/83efe198-56cc-48a0-9f29-24a624cf2bf2_62_20200319195314.parquet
113327 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/83efe198-56cc-48a0-9f29-24a624cf2bf2_62_20200319195314.parquet
113327 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/83efe198-56cc-48a0-9f29-24a624cf2bf2_62_20200319195314.parquet => [545f9cd8-9d3e-42c8-83e7-a85629796358]
113337 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/8730f0ec-9b52-4f03-a8a6-7578c712f4b7_27_20200319195314.parquet
113342 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/8730f0ec-9b52-4f03-a8a6-7578c712f4b7_27_20200319195314.parquet
113342 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/8730f0ec-9b52-4f03-a8a6-7578c712f4b7_27_20200319195314.parquet => [cba91558-3524-4d36-b822-3586d0a2f25d]
113350 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/94b6c971-3153-4316-9c72-abfde561be50_37_20200319195314.parquet
113355 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/94b6c971-3153-4316-9c72-abfde561be50_37_20200319195314.parquet
113355 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/94b6c971-3153-4316-9c72-abfde561be50_37_20200319195314.parquet => [1c232881-0fc3-4482-bde5-f814f04ad30a]
113363 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/9882a7ca-2a57-443b-94d0-8ddf007676ce_22_20200319195314.parquet
113368 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/9882a7ca-2a57-443b-94d0-8ddf007676ce_22_20200319195314.parquet
113368 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/9882a7ca-2a57-443b-94d0-8ddf007676ce_22_20200319195314.parquet => [b5b96fde-7249-455e-a9c6-d30fe2c55480]
113377 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/9da7daf6-9e7a-463e-994b-8e61e2087d83_72_20200319195314.parquet
113381 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/9da7daf6-9e7a-463e-994b-8e61e2087d83_72_20200319195314.parquet
113381 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/9da7daf6-9e7a-463e-994b-8e61e2087d83_72_20200319195314.parquet => [cbb02632-c621-4fdf-80f1-1a1dc737dda0]
113390 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/9dd7419a-dd06-450f-aea4-83bbba37b152_47_20200319195314.parquet
113394 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/9dd7419a-dd06-450f-aea4-83bbba37b152_47_20200319195314.parquet
113394 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/9dd7419a-dd06-450f-aea4-83bbba37b152_47_20200319195314.parquet => [94d24c36-f02e-4fad-b23d-b672e06fee13]
113403 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/9f8fd2c8-f0c6-49f2-81e3-0d8f3ba8141a_68_20200319195314.parquet
113408 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/9f8fd2c8-f0c6-49f2-81e3-0d8f3ba8141a_68_20200319195314.parquet
113408 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/9f8fd2c8-f0c6-49f2-81e3-0d8f3ba8141a_68_20200319195314.parquet => [994ddf1d-984a-4f4f-827f-f5f1936405f6]
113420 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/9f9f20c4-8841-422e-8599-102c9fefdcfb_3_20200319195314.parquet
113425 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/9f9f20c4-8841-422e-8599-102c9fefdcfb_3_20200319195314.parquet
113425 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/9f9f20c4-8841-422e-8599-102c9fefdcfb_3_20200319195314.parquet => [1a493768-ca57-4e4f-8450-be7d6b88e113]
113440 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/a5df39cf-7cbc-4d38-b7a1-f37f4b400c91_8_20200319195314.parquet
113445 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/a5df39cf-7cbc-4d38-b7a1-f37f4b400c91_8_20200319195314.parquet
113445 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/a5df39cf-7cbc-4d38-b7a1-f37f4b400c91_8_20200319195314.parquet => [37fa762b-91ff-4e3e-84c4-b106a367d54d]
113455 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/ad523dab-6039-4361-b249-453a3afc5663_64_20200319195314.parquet
113460 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/ad523dab-6039-4361-b249-453a3afc5663_64_20200319195314.parquet
113460 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/ad523dab-6039-4361-b249-453a3afc5663_64_20200319195314.parquet => [5ade080b-84e4-4769-8f9c-2e093e0a2387]
113469 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/bb01f234-f579-4854-b5de-1ec288fd8452_65_20200319195314.parquet
113474 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/bb01f234-f579-4854-b5de-1ec288fd8452_65_20200319195314.parquet
113474 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/bb01f234-f579-4854-b5de-1ec288fd8452_65_20200319195314.parquet => [70d6b413-d2cf-4a3d-83b4-ea374360d37f]
113483 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/bc9ea9e5-b661-4d70-b4cf-12d7529fbf66_36_20200319195314.parquet
113488 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/bc9ea9e5-b661-4d70-b4cf-12d7529fbf66_36_20200319195314.parquet
113488 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/bc9ea9e5-b661-4d70-b4cf-12d7529fbf66_36_20200319195314.parquet => [fe80abda-830d-43ab-bcb5-765d19a070c9]
113497 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/bc9f5ef8-e1a0-44e5-8254-6faf5f969cb0_18_20200319195314.parquet
113501 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/bc9f5ef8-e1a0-44e5-8254-6faf5f969cb0_18_20200319195314.parquet
113502 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/bc9f5ef8-e1a0-44e5-8254-6faf5f969cb0_18_20200319195314.parquet => [8763a904-243f-4b75-a2c4-79ce42495d11]
113510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/bcf9e867-b288-4f2c-84d9-be4a6aff619c_20_20200319195314.parquet
113521 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/bcf9e867-b288-4f2c-84d9-be4a6aff619c_20_20200319195314.parquet
113521 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/bcf9e867-b288-4f2c-84d9-be4a6aff619c_20_20200319195314.parquet => [aabc3251-e890-4907-a953-d63b8200b744]
113532 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/bd0f7c86-43c2-43a8-8e03-c8de3961883e_55_20200319195314.parquet
113537 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/bd0f7c86-43c2-43a8-8e03-c8de3961883e_55_20200319195314.parquet
113537 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/bd0f7c86-43c2-43a8-8e03-c8de3961883e_55_20200319195314.parquet => [0723842c-fe3d-4130-acaa-a203ef4e6558]
113546 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/be7accbf-90f9-419e-b148-4f942a368c42_10_20200319195314.parquet
113563 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/be7accbf-90f9-419e-b148-4f942a368c42_10_20200319195314.parquet
113563 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/be7accbf-90f9-419e-b148-4f942a368c42_10_20200319195314.parquet => [4bddf0a2-1f20-4183-85e4-7ea53a5d4cfd]
113571 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/c1be6bd7-7b4e-4f97-b54c-23dc0196e49f_29_20200319195314.parquet
113576 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/c1be6bd7-7b4e-4f97-b54c-23dc0196e49f_29_20200319195314.parquet
113577 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/c1be6bd7-7b4e-4f97-b54c-23dc0196e49f_29_20200319195314.parquet => [d54201d2-87e6-43ec-b1f4-e94f25ae3c57]
113589 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/c44592bb-12f3-4706-b282-edb33a6b1ec6_34_20200319195314.parquet
113596 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/c44592bb-12f3-4706-b282-edb33a6b1ec6_34_20200319195314.parquet
113596 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/c44592bb-12f3-4706-b282-edb33a6b1ec6_34_20200319195314.parquet => [f7d7f448-3aa9-4193-9147-18419f0da30f]
113610 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/cbcb036a-3125-4db2-ae7d-bdb4385aa893_25_20200319195314.parquet
113616 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/cbcb036a-3125-4db2-ae7d-bdb4385aa893_25_20200319195314.parquet
113616 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/cbcb036a-3125-4db2-ae7d-bdb4385aa893_25_20200319195314.parquet => [c0386066-a89c-43ab-92ff-f9cae13b950d]
113629 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/d7967e07-1bf5-47aa-9067-375d393b4254_69_20200319195314.parquet
113642 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/d7967e07-1bf5-47aa-9067-375d393b4254_69_20200319195314.parquet
113642 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/d7967e07-1bf5-47aa-9067-375d393b4254_69_20200319195314.parquet => [b02ff128-500f-4bca-9680-67563c01fd3f]
113651 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/dea2a8b4-8fe5-461e-84e8-4e3d66070c87_45_20200319195314.parquet
113656 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/dea2a8b4-8fe5-461e-84e8-4e3d66070c87_45_20200319195314.parquet
113656 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/dea2a8b4-8fe5-461e-84e8-4e3d66070c87_45_20200319195314.parquet => [8fdf8631-7dc1-403e-b45c-11fa15f7e608]
113666 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/e01664f9-54b8-4ec7-8e3c-3afbeeda3a27_35_20200319195314.parquet
113673 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/e01664f9-54b8-4ec7-8e3c-3afbeeda3a27_35_20200319195314.parquet
113673 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/e01664f9-54b8-4ec7-8e3c-3afbeeda3a27_35_20200319195314.parquet => [f900b42d-e09b-4f92-a020-e66599cd9baa]
113682 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/e5fe3400-e4f2-4f37-8c2d-8ba1d4993998_31_20200319195314.parquet
113687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/e5fe3400-e4f2-4f37-8c2d-8ba1d4993998_31_20200319195314.parquet
113687 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/e5fe3400-e4f2-4f37-8c2d-8ba1d4993998_31_20200319195314.parquet => [df211f5c-6a94-47d6-a579-0471289d5503]
113849 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/e683afee-fcc0-45ad-92ab-774e445b20e2_23_20200319195314.parquet
113882 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/e683afee-fcc0-45ad-92ab-774e445b20e2_23_20200319195314.parquet
113882 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/e683afee-fcc0-45ad-92ab-774e445b20e2_23_20200319195314.parquet => [b7e24daa-2081-40ef-bdee-7302e9b6df3e]
113892 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/ed021372-e0a8-4bff-8f21-91ea61b3aab8_50_20200319195314.parquet
113906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/ed021372-e0a8-4bff-8f21-91ea61b3aab8_50_20200319195314.parquet
113906 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/ed021372-e0a8-4bff-8f21-91ea61b3aab8_50_20200319195314.parquet => [b005bd53-0693-4636-ac40-bbf20f063c02]
113916 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/f08faf68-5cdb-4be0-9de1-1a0950029194_5_20200319195314.parquet
113921 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/f08faf68-5cdb-4be0-9de1-1a0950029194_5_20200319195314.parquet
113921 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/f08faf68-5cdb-4be0-9de1-1a0950029194_5_20200319195314.parquet => [27d093da-1ef8-4af6-8ca4-772d9079c60d]
113931 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/f2b366c2-8026-4d45-82b3-403951888420_57_20200319195314.parquet
113947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/f2b366c2-8026-4d45-82b3-403951888420_57_20200319195314.parquet
113947 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/f2b366c2-8026-4d45-82b3-403951888420_57_20200319195314.parquet => [2121d8cd-9750-460e-a3cd-e9863466dddd]
113957 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/f2c0dfc6-0792-4e26-aa41-6b33ae5d8aca_51_20200319195314.parquet
113963 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/f2c0dfc6-0792-4e26-aa41-6b33ae5d8aca_51_20200319195314.parquet
113963 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/f2c0dfc6-0792-4e26-aa41-6b33ae5d8aca_51_20200319195314.parquet => [b28ea214-c247-4a79-a9bd-95332b5fb224]
113973 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2016/03/15/f335ad1a-55c7-4355-b32b-700016c96755_73_20200319195314.parquet
113979 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2016/03/15/f335ad1a-55c7-4355-b32b-700016c96755_73_20200319195314.parquet
113979 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2016/03/15/f335ad1a-55c7-4355-b32b-700016c96755_73_20200319195314.parquet => [ce3825d1-e762-47bf-94c9-fbc74a21d8bf]
113989 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/16/f9c52cf9-9476-4b03-9f38-bf558f740f43_24_20200319195314.parquet
113994 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/16/f9c52cf9-9476-4b03-9f38-bf558f740f43_24_20200319195314.parquet
113994 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/16/f9c52cf9-9476-4b03-9f38-bf558f740f43_24_20200319195314.parquet => [bd797082-e644-4e3d-8e1f-6e5c2779360a]
114004 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit3425375083504554774/2015/03/17/ff1dc552-6ac9-4876-8879-8b9e910d94df_41_20200319195314.parquet
114015 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit3425375083504554774/2015/03/17/ff1dc552-6ac9-4876-8879-8b9e910d94df_41_20200319195314.parquet
114015 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit3425375083504554774/2015/03/17/ff1dc552-6ac9-4876-8879-8b9e910d94df_41_20200319195314.parquet => [73df0798-e8cf-4aea-8316-4089a5eedddb]
114228 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
114301 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
114302 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
114587 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114588 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114588 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114588 [pool-832-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114618 [pool-832-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114638 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114639 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114639 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114648 [pool-833-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114686 [pool-833-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114713 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114713 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114713 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114714 [pool-834-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114748 [pool-834-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114761 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114762 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114762 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114762 [pool-835-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114807 [pool-835-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114834 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114835 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114835 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114835 [pool-836-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114919 [pool-836-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114932 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114933 [pool-837-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
114973 [pool-837-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
114986 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
114986 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
114986 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
114987 [pool-838-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115020 [pool-838-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115034 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115035 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115035 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115035 [pool-839-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115064 [pool-839-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115078 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115078 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115078 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115078 [pool-840-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115107 [pool-840-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115120 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115135 [pool-841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115136 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115136 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115162 [pool-841-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115174 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115175 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115175 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115175 [pool-842-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115202 [pool-842-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115240 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115241 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115241 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115241 [pool-843-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115275 [pool-843-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115289 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115289 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115289 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115290 [pool-844-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115326 [pool-844-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115342 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115342 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115342 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115342 [pool-845-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115369 [pool-845-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115390 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115391 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115391 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115391 [pool-846-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115417 [pool-846-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115435 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115435 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115435 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115436 [pool-847-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115463 [pool-847-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115476 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115477 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115477 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115477 [pool-848-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115511 [pool-848-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115523 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115524 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115524 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115524 [pool-849-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115550 [pool-849-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115563 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115564 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115564 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115564 [pool-850-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115601 [pool-850-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115620 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115621 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115621 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115622 [pool-851-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115657 [pool-851-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115672 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115672 [pool-852-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115698 [pool-852-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115720 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115721 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115721 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115721 [pool-853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115746 [pool-853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115760 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115761 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115761 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115761 [pool-854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115786 [pool-854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115799 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115799 [pool-855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115835 [pool-855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115855 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115856 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115856 [pool-856-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115883 [pool-856-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115896 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115897 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115897 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115897 [pool-857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115922 [pool-857-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115951 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115952 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115952 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115952 [pool-858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
115978 [pool-858-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
115991 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
115992 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
115992 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
115992 [pool-859-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116036 [pool-859-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116055 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116055 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116055 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116055 [pool-860-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116096 [pool-860-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116113 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116113 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116113 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116114 [pool-861-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116139 [pool-861-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116152 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116152 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116153 [pool-862-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116186 [pool-862-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116200 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116200 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116200 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116200 [pool-863-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116231 [pool-863-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116244 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116245 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116245 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116245 [pool-864-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116273 [pool-864-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116294 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116294 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116294 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116303 [pool-865-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116340 [pool-865-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116353 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116353 [pool-866-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116382 [pool-866-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116397 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116397 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116397 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116398 [pool-867-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116431 [pool-867-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116444 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116445 [pool-868-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116475 [pool-868-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116488 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116489 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116489 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116489 [pool-869-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116714 [pool-869-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116733 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116733 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116733 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116736 [pool-870-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116774 [pool-870-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116788 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116789 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116789 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116789 [pool-871-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116817 [pool-871-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116844 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116845 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116845 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116845 [pool-872-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116882 [pool-872-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116898 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116899 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116899 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116899 [pool-873-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116926 [pool-873-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116942 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116943 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116943 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116943 [pool-874-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
116976 [pool-874-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
116994 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
116994 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
116994 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
116994 [pool-875-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117021 [pool-875-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117034 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117034 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117034 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117034 [pool-876-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117067 [pool-876-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117081 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117081 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117081 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117082 [pool-877-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117108 [pool-877-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117124 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117125 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117125 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117125 [pool-878-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117151 [pool-878-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117178 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117178 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117178 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117178 [pool-879-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117205 [pool-879-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117220 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117220 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117220 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117220 [pool-880-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117246 [pool-880-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117260 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117261 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117261 [pool-881-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117294 [pool-881-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117308 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117309 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117309 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117309 [pool-882-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117340 [pool-882-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117354 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117355 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117355 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117374 [pool-883-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117410 [pool-883-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117423 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117424 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117424 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117424 [pool-884-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117466 [pool-884-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117479 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117480 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117480 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117480 [pool-885-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117507 [pool-885-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117529 [pool-886-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117556 [pool-886-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117572 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117572 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117572 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117573 [pool-887-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117599 [pool-887-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117621 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117621 [pool-888-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117649 [pool-888-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117663 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117664 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117664 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117665 [pool-889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117693 [pool-889-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117707 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117708 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117708 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117708 [pool-890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117743 [pool-890-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117756 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117756 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117756 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117757 [pool-891-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117802 [pool-891-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117831 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117832 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117832 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117832 [pool-892-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117875 [pool-892-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117906 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117907 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117907 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117907 [pool-893-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117935 [pool-893-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
117948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
117948 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
117948 [pool-894-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
117977 [pool-894-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
117999 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118000 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118000 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118000 [pool-895-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118033 [pool-895-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118047 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118047 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118047 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118049 [pool-896-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118077 [pool-896-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118089 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118090 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118090 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118090 [pool-897-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118126 [pool-897-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118138 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118138 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118138 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118139 [pool-898-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118167 [pool-898-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118180 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118180 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118180 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118180 [pool-899-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118213 [pool-899-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118225 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118226 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118226 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118226 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118258 [pool-900-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118282 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118282 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118282 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118283 [pool-901-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118308 [pool-901-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118329 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118330 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118330 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118330 [pool-902-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118355 [pool-902-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118369 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118369 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118369 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118370 [pool-903-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118397 [pool-903-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118414 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118414 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118414 [pool-904-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118448 [pool-904-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118461 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118462 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118462 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118462 [pool-905-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118488 [pool-905-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118501 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118501 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118501 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118502 [pool-906-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118534 [pool-906-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118549 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118549 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118549 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118550 [pool-907-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118575 [pool-907-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118596 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118596 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118597 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118597 [pool-908-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118624 [pool-908-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118658 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118659 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118659 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118659 [pool-909-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118688 [pool-909-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118707 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118708 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118708 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118708 [pool-910-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118734 [pool-910-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118747 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118747 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118747 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118747 [pool-911-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118781 [pool-911-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118795 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118796 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118796 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118796 [pool-912-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118824 [pool-912-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118837 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118837 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118837 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118838 [pool-913-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118866 [pool-913-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118886 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118886 [pool-914-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118913 [pool-914-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118929 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118930 [pool-915-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
118961 [pool-915-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
118977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
118977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
118977 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
118978 [pool-916-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119012 [pool-916-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119025 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119025 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119025 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119026 [pool-917-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119055 [pool-917-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119071 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119072 [pool-918-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119107 [pool-918-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119123 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119123 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119123 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119124 [pool-919-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119161 [pool-919-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119174 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119174 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119174 [pool-920-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119204 [pool-920-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119225 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119226 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119226 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119226 [pool-921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119252 [pool-921-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119266 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119266 [pool-922-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119292 [pool-922-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119305 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119306 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119306 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119308 [pool-923-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119341 [pool-923-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119356 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119356 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119356 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119357 [pool-924-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119384 [pool-924-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119401 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119401 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119401 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119401 [pool-925-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119452 [pool-925-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119473 [pool-926-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119524 [pool-926-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119540 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119541 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119541 [pool-927-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119590 [pool-927-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119609 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119610 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119610 [pool-928-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119636 [pool-928-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119649 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119649 [pool-929-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119677 [pool-929-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119691 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119691 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119692 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119692 [pool-930-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119725 [pool-930-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119742 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119742 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119742 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119742 [pool-931-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119770 [pool-931-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119782 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119783 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119783 [pool-932-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119811 [pool-932-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119831 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119832 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119832 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119832 [pool-933-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119858 [pool-933-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119872 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119872 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119873 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119873 [pool-934-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119898 [pool-934-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119912 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119912 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119912 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119912 [pool-935-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
119944 [pool-935-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
119959 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
119960 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
119960 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
119960 [pool-936-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120005 [pool-936-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120032 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120032 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120032 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120033 [pool-937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120064 [pool-937-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120078 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120079 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120079 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120079 [pool-938-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120108 [pool-938-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120121 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120122 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120122 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120122 [pool-939-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120150 [pool-939-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120173 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120174 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120174 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120174 [pool-940-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120200 [pool-940-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120213 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120213 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120213 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120214 [pool-941-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120239 [pool-941-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120282 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120283 [pool-942-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120365 [pool-942-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120392 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120393 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120393 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120393 [pool-943-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120469 [pool-943-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120504 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120504 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120504 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120504 [pool-944-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120592 [pool-944-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120631 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120634 [pool-945-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120718 [pool-945-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120734 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120735 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120735 [pool-946-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120792 [pool-946-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120814 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120814 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120814 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120814 [pool-947-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120851 [pool-947-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120863 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120864 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120864 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120865 [pool-948-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
120965 [pool-948-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
120986 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
120987 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
120987 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
120987 [pool-949-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121068 [pool-949-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121086 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121087 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121087 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121087 [pool-950-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121168 [pool-950-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121186 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121187 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121187 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121187 [pool-951-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121264 [pool-951-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121312 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121312 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121312 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121312 [pool-952-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121395 [pool-952-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121413 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121414 [pool-953-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121669 [pool-953-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121699 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121700 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121700 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121700 [pool-954-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121782 [pool-954-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121808 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121808 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121808 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121808 [pool-955-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121886 [pool-955-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121922 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121923 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121923 [pool-956-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
121981 [pool-956-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
121994 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
121994 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
121994 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
121995 [pool-957-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122021 [pool-957-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122037 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122037 [pool-958-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122091 [pool-958-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122106 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122106 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122106 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122107 [pool-959-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122136 [pool-959-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122152 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122153 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122153 [pool-960-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122249 [pool-960-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122269 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122270 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122270 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122270 [pool-961-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122345 [pool-961-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122372 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122373 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122373 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122373 [pool-962-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122453 [pool-962-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122486 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122486 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122486 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122486 [pool-963-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122558 [pool-963-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122584 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122585 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122585 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122585 [pool-964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122649 [pool-964-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122686 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122687 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122687 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122687 [pool-965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122754 [pool-965-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122775 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122775 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122775 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122776 [pool-966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122870 [pool-966-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122883 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122884 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122884 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122884 [pool-967-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
122966 [pool-967-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
122986 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
122987 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
122987 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
122987 [pool-968-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123118 [pool-968-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123130 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123131 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123131 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123131 [pool-969-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123194 [pool-969-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123227 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123228 [pool-970-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123294 [pool-970-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123320 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123321 [pool-971-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123394 [pool-971-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123419 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123420 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123420 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123420 [pool-972-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123502 [pool-972-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123528 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123528 [pool-973-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123597 [pool-973-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123632 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123633 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123633 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123633 [pool-974-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123723 [pool-974-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123741 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123741 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123741 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123742 [pool-975-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123809 [pool-975-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123840 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123841 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123841 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123841 [pool-976-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
123917 [pool-976-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
123939 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
123940 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
123940 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
123940 [pool-977-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124028 [pool-977-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124050 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124051 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124060 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124063 [pool-978-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124104 [pool-978-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124117 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124117 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124117 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124118 [pool-979-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124164 [pool-979-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124177 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124177 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124177 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124177 [pool-980-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124202 [pool-980-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124215 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124215 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124215 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124215 [pool-981-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124242 [pool-981-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124263 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124264 [pool-982-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124296 [pool-982-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124311 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124318 [pool-983-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124343 [pool-983-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124356 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124357 [pool-984-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124391 [pool-984-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124403 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124404 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124404 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124404 [pool-985-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124429 [pool-985-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124471 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124472 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124472 [pool-986-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124547 [pool-986-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124576 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124576 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124576 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124576 [pool-987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124654 [pool-987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124680 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124680 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124680 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124680 [pool-988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124759 [pool-988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124779 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124780 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124780 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124780 [pool-989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124862 [pool-989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124877 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124878 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124878 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124878 [pool-990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124948 [pool-990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
124964 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
124964 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
124964 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
124965 [pool-991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
124997 [pool-991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125010 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125011 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125011 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125011 [pool-992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125037 [pool-992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125054 [pool-993-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125083 [pool-993-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125103 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125103 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125103 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125112 [pool-994-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125137 [pool-994-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125155 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125156 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125156 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125156 [pool-995-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125182 [pool-995-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125214 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125244 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125244 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125244 [pool-996-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125311 [pool-996-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125324 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125325 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125325 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125325 [pool-997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125409 [pool-997-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125435 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125436 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125436 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125436 [pool-998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125524 [pool-998-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125537 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125538 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125538 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125538 [pool-999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125621 [pool-999-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125642 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125643 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125643 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125643 [pool-1000-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125670 [pool-1000-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125690 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125690 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125690 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125690 [pool-1001-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125716 [pool-1001-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125742 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125743 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125743 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125743 [pool-1002-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125768 [pool-1002-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125781 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125781 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125781 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125781 [pool-1003-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125813 [pool-1003-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125842 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125843 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125845 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125847 [pool-1004-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125875 [pool-1004-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125894 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125895 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125895 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125895 [pool-1005-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125928 [pool-1005-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125942 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125943 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125943 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125944 [pool-1006-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
125976 [pool-1006-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
125995 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
125995 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
125996 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
125999 [pool-1007-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126032 [pool-1007-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126052 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126053 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126053 [pool-1008-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126079 [pool-1008-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126092 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126093 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126093 [pool-1009-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126125 [pool-1009-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126138 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126139 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126139 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126139 [pool-1010-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126170 [pool-1010-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126183 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126184 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126184 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126184 [pool-1011-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126210 [pool-1011-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126223 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126224 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126224 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126224 [pool-1012-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126258 [pool-1012-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126274 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126274 [pool-1013-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126349 [pool-1013-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126382 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126382 [pool-1014-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126423 [pool-1014-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126443 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126443 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126443 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126443 [pool-1015-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126475 [pool-1015-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126488 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126489 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126489 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126489 [pool-1016-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126518 [pool-1016-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126545 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126545 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126545 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126545 [pool-1017-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126593 [pool-1017-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126610 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126611 [pool-1018-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126613 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126613 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126640 [pool-1018-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126662 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126663 [pool-1019-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126664 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126704 [pool-1019-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126718 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126718 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126718 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126719 [pool-1020-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126745 [pool-1020-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126760 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126760 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126760 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126760 [pool-1021-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126786 [pool-1021-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126815 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126816 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126816 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126816 [pool-1022-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126856 [pool-1022-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126876 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126877 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126877 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126877 [pool-1023-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126903 [pool-1023-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126925 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126925 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126925 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126926 [pool-1024-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126953 [pool-1024-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
126965 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
126966 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
126966 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
126966 [pool-1025-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
126992 [pool-1025-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127005 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127006 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127006 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127006 [pool-1026-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127042 [pool-1026-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127082 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127083 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127083 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127090 [pool-1027-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127141 [pool-1027-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127165 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127165 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127165 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127166 [pool-1028-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127199 [pool-1028-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127212 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127212 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127212 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127213 [pool-1029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127241 [pool-1029-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127254 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127254 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127254 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127254 [pool-1030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127285 [pool-1030-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127304 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
127305 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
127305 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
127305 [pool-1031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
127334 [pool-1031-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
127377 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
127377 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
128187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
128187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
128321 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
128328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
128328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
128328 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
131966 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 81
131966 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
132127 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/01bc087c-032d-467f-ae53-34172976e16d_101_001.parquet
132167 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/01bc087c-032d-467f-ae53-34172976e16d_101_001.parquet
132167 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/01bc087c-032d-467f-ae53-34172976e16d_101_001.parquet => [7ca171c1-b524-4af7-97b6-6c4d5db5f17e]
132177 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/03b571d1-9c3a-4ab2-bff9-8ac63a58c612_87_001.parquet
132195 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/03b571d1-9c3a-4ab2-bff9-8ac63a58c612_87_001.parquet
132195 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/03b571d1-9c3a-4ab2-bff9-8ac63a58c612_87_001.parquet => [4bd0011d-cadd-411e-9423-aaa12b07daa3]
132203 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/03f75e6b-4ee1-4ea4-ad9c-c57f20288cca_80_001.parquet
132211 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/03f75e6b-4ee1-4ea4-ad9c-c57f20288cca_80_001.parquet
132211 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/03f75e6b-4ee1-4ea4-ad9c-c57f20288cca_80_001.parquet => [3c977334-5040-4338-95bf-667843f1fde2]
132220 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/12489847-3c1f-4295-93ff-82083d2aed9a_141_001.parquet
132249 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/12489847-3c1f-4295-93ff-82083d2aed9a_141_001.parquet
132249 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/12489847-3c1f-4295-93ff-82083d2aed9a_141_001.parquet => [276a036e-fbcf-4ac7-93da-bee3efd8ffe5]
132258 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/1599d4b0-0514-4fe2-94f5-a150c991ed4f_49_001.parquet
132268 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/1599d4b0-0514-4fe2-94f5-a150c991ed4f_49_001.parquet
132268 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/1599d4b0-0514-4fe2-94f5-a150c991ed4f_49_001.parquet => [b3135038-14a2-4ae8-9c1b-1ec73edda945]
132285 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/16e4c4c1-f74e-4f3b-a2cb-315c983b477a_199_001.parquet
132295 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/16e4c4c1-f74e-4f3b-a2cb-315c983b477a_199_001.parquet
132295 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/16e4c4c1-f74e-4f3b-a2cb-315c983b477a_199_001.parquet => [fcfea7e5-35f4-416d-b02b-c231c8e574f0]
132304 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/1752f54c-ab6f-4c48-b21e-31ef3da7b1bd_0_001.parquet
132308 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/1752f54c-ab6f-4c48-b21e-31ef3da7b1bd_0_001.parquet
132308 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/1752f54c-ab6f-4c48-b21e-31ef3da7b1bd_0_001.parquet => [0229a8e1-95cd-42e6-b1d5-bc9f87c56b92]
132317 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/1e2aa031-2263-487a-b436-3139150ebda6_95_001.parquet
132321 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/1e2aa031-2263-487a-b436-3139150ebda6_95_001.parquet
132321 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/1e2aa031-2263-487a-b436-3139150ebda6_95_001.parquet => [6494aa78-1af1-4a12-be0f-167a9fbe1ea1]
132336 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/2288dee4-8fee-4788-a956-032ed38af310_165_001.parquet
132355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/2288dee4-8fee-4788-a956-032ed38af310_165_001.parquet
132355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/2288dee4-8fee-4788-a956-032ed38af310_165_001.parquet => [9d0a21b8-1812-496c-a6e5-00a07698409e]
132363 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/22d149e8-f9d5-438f-8714-e09035531933_140_001.parquet
132368 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/22d149e8-f9d5-438f-8714-e09035531933_140_001.parquet
132368 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/22d149e8-f9d5-438f-8714-e09035531933_140_001.parquet => [238d126c-ed76-49e5-ba21-e9671ac08f27]
132377 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/285da6d2-c5d5-4ebe-93e7-bef0a95c05be_84_001.parquet
132381 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/285da6d2-c5d5-4ebe-93e7-bef0a95c05be_84_001.parquet
132381 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/285da6d2-c5d5-4ebe-93e7-bef0a95c05be_84_001.parquet => [45d17104-184c-405e-a810-09f2a266333c]
132390 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/3059aa6e-7f56-4cae-a11b-69683c1fb571_104_001.parquet
132394 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/3059aa6e-7f56-4cae-a11b-69683c1fb571_104_001.parquet
132394 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/3059aa6e-7f56-4cae-a11b-69683c1fb571_104_001.parquet => [82b7827a-72cf-4f19-89bc-5f33d3e4435c]
132402 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/32a9bd42-9c15-4158-aebc-c7cecac563dd_70_001.parquet
132407 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/32a9bd42-9c15-4158-aebc-c7cecac563dd_70_001.parquet
132407 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/32a9bd42-9c15-4158-aebc-c7cecac563dd_70_001.parquet => [04e1f01e-cc15-4810-a6b6-0916805f742e]
132416 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/35737386-2ad8-490f-9a69-d54b43cd6230_133_001.parquet
132420 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/35737386-2ad8-490f-9a69-d54b43cd6230_133_001.parquet
132420 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/35737386-2ad8-490f-9a69-d54b43cd6230_133_001.parquet => [ed90835c-99ff-49bc-9105-4419fd164559]
132434 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/361963f0-04d4-4558-82ea-c1b85eb5459b_166_001.parquet
132438 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/361963f0-04d4-4558-82ea-c1b85eb5459b_166_001.parquet
132439 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/361963f0-04d4-4558-82ea-c1b85eb5459b_166_001.parquet => [a9db3483-9325-4f3d-a731-8a9bae0b61ca]
132450 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/38e07c9f-115f-4bbb-8124-7accb1932559_9_001.parquet
132462 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/38e07c9f-115f-4bbb-8124-7accb1932559_9_001.parquet
132462 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/38e07c9f-115f-4bbb-8124-7accb1932559_9_001.parquet => [22b07493-4505-4051-a1aa-b0b61a519612]
132470 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/39b66b5d-32d0-4f43-a08a-898ab8090ffc_168_001.parquet
132475 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/39b66b5d-32d0-4f43-a08a-898ab8090ffc_168_001.parquet
132475 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/39b66b5d-32d0-4f43-a08a-898ab8090ffc_168_001.parquet => [b32cecb6-35d8-4a9a-917d-749b0ca2ed45]
132484 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/3a049335-0131-4682-b92e-46ed044da083_111_001.parquet
132488 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/3a049335-0131-4682-b92e-46ed044da083_111_001.parquet
132488 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/3a049335-0131-4682-b92e-46ed044da083_111_001.parquet => [947b1005-967c-4e42-adb1-1af2965ecd7c]
132497 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/3b40f637-c826-406a-9c9b-2c62bc3d64bf_25_001.parquet
132501 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/3b40f637-c826-406a-9c9b-2c62bc3d64bf_25_001.parquet
132501 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/3b40f637-c826-406a-9c9b-2c62bc3d64bf_25_001.parquet => [50bb5019-49d3-4de0-9638-519e9d3c6281]
132509 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/3cccd204-6850-48e0-b70b-50d8c18bee03_105_001.parquet
132513 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/3cccd204-6850-48e0-b70b-50d8c18bee03_105_001.parquet
132513 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/3cccd204-6850-48e0-b70b-50d8c18bee03_105_001.parquet => [82e6aeb8-4c98-4bbd-a35d-8d8f4379415c]
132522 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/4476d726-0ac3-434e-9c73-bea3ea202d9a_83_001.parquet
132527 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/4476d726-0ac3-434e-9c73-bea3ea202d9a_83_001.parquet
132527 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/4476d726-0ac3-434e-9c73-bea3ea202d9a_83_001.parquet => [44381798-b0d7-4851-a14d-6b44654050cd]
132536 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/45fd8909-6833-4ab8-898b-d8f67268bf3d_163_001.parquet
132541 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/45fd8909-6833-4ab8-898b-d8f67268bf3d_163_001.parquet
132541 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/45fd8909-6833-4ab8-898b-d8f67268bf3d_163_001.parquet => [8acfa739-979c-41b1-a753-8dd144fc0ae0]
132549 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/49dd3216-4e79-4d8f-aa06-7daecc263284_19_001.parquet
132554 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/49dd3216-4e79-4d8f-aa06-7daecc263284_19_001.parquet
132554 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/49dd3216-4e79-4d8f-aa06-7daecc263284_19_001.parquet => [36f8cc5b-3c38-49c2-a026-0cc5f3853a2d]
132569 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/4aed95a4-d030-41fb-b5ac-9d5830ed82cd_71_001.parquet
132576 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/4aed95a4-d030-41fb-b5ac-9d5830ed82cd_71_001.parquet
132576 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/4aed95a4-d030-41fb-b5ac-9d5830ed82cd_71_001.parquet => [0a94724c-39e1-4463-9d7f-d506b9bfb1dd]
132585 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/4d743167-ca3c-4c50-9201-eb8498f45786_23_001.parquet
132589 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/4d743167-ca3c-4c50-9201-eb8498f45786_23_001.parquet
132589 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/4d743167-ca3c-4c50-9201-eb8498f45786_23_001.parquet => [4b3f2980-3270-4f38-a786-eba53ca34fc2]
132598 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/521c1d39-9142-469e-8c1a-90f0056060e9_15_001.parquet
132602 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/521c1d39-9142-469e-8c1a-90f0056060e9_15_001.parquet
132602 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/521c1d39-9142-469e-8c1a-90f0056060e9_15_001.parquet => [32bae200-4e4a-4230-bbc9-651f6dee4955]
132611 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/536a557b-9c4d-46ee-aee0-943d02bc05de_75_001.parquet
132617 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/536a557b-9c4d-46ee-aee0-943d02bc05de_75_001.parquet
132617 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/536a557b-9c4d-46ee-aee0-943d02bc05de_75_001.parquet => [0f3ef6e9-e6e5-4a92-a32f-3806ea560117]
132626 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/5a5f1a72-7118-4a30-a455-73496d7e3937_94_001.parquet
132632 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/5a5f1a72-7118-4a30-a455-73496d7e3937_94_001.parquet
132632 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/5a5f1a72-7118-4a30-a455-73496d7e3937_94_001.parquet => [62769e7c-e42f-4e64-8763-da70662bf017]
132641 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/5e36ac80-78bf-4042-af83-caf88bebdb51_155_001.parquet
132649 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/5e36ac80-78bf-4042-af83-caf88bebdb51_155_001.parquet
132649 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/5e36ac80-78bf-4042-af83-caf88bebdb51_155_001.parquet => [5609bcc5-0fb2-45cb-bb7b-11a4cc7a8ff4]
132666 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/5f6d32c8-a001-4595-a348-76d153fb50b9_192_001.parquet
132671 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/5f6d32c8-a001-4595-a348-76d153fb50b9_192_001.parquet
132671 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/5f6d32c8-a001-4595-a348-76d153fb50b9_192_001.parquet => [f0eedf3d-c744-4f5b-9c30-61ce0b34c039]
132679 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/6149e35a-605a-4678-a022-60517759e7f6_195_001.parquet
132693 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/6149e35a-605a-4678-a022-60517759e7f6_195_001.parquet
132693 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/6149e35a-605a-4678-a022-60517759e7f6_195_001.parquet => [f75bcb0e-25bd-421f-a3b1-6eb95493f400]
132704 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/6159f851-de13-4b20-8926-fa1851ea9388_181_001.parquet
132709 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/6159f851-de13-4b20-8926-fa1851ea9388_181_001.parquet
132709 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/6159f851-de13-4b20-8926-fa1851ea9388_181_001.parquet => [d7df700f-ec70-4124-8cd5-a508887bb968]
132717 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/61ad6d0a-8c35-428f-90c3-8365e4883953_135_001.parquet
132722 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/61ad6d0a-8c35-428f-90c3-8365e4883953_135_001.parquet
132722 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/61ad6d0a-8c35-428f-90c3-8365e4883953_135_001.parquet => [f3c9c8ce-7067-468b-bf73-c5bce453f538]
132730 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/670b706e-8976-4dc2-a387-fdf716f246b4_144_001.parquet
132739 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/670b706e-8976-4dc2-a387-fdf716f246b4_144_001.parquet
132739 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/670b706e-8976-4dc2-a387-fdf716f246b4_144_001.parquet => [32907ade-d4f1-4826-84c5-0e896aae8374]
132751 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/69026d34-1213-4b12-b2ba-0a73a9d909d1_8_001.parquet
132755 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/69026d34-1213-4b12-b2ba-0a73a9d909d1_8_001.parquet
132755 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/69026d34-1213-4b12-b2ba-0a73a9d909d1_8_001.parquet => [22a5fc03-47e6-48dc-8759-621200526411]
132764 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/6d506052-e24c-4dc6-a38f-fad9e8462f79_127_001.parquet
132768 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/6d506052-e24c-4dc6-a38f-fad9e8462f79_127_001.parquet
132768 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/6d506052-e24c-4dc6-a38f-fad9e8462f79_127_001.parquet => [dca27bdd-414b-4e9a-a268-07938d1b7577]
132776 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/7dbe4668-b1a5-449b-a04a-c78b6d5284b9_10_001.parquet
132798 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/7dbe4668-b1a5-449b-a04a-c78b6d5284b9_10_001.parquet
132798 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/7dbe4668-b1a5-449b-a04a-c78b6d5284b9_10_001.parquet => [23a33cea-e122-4281-a9b8-7696e21a0a85]
132807 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/7f1edca1-4f43-4467-b58c-dad648959800_171_001.parquet
132812 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/7f1edca1-4f43-4467-b58c-dad648959800_171_001.parquet
132812 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/7f1edca1-4f43-4467-b58c-dad648959800_171_001.parquet => [c645887d-5989-4628-98d2-7cb766aa3c77]
132974 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/800b6fac-404a-42e4-a5ff-b54b0f6355fe_93_001.parquet
132982 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/800b6fac-404a-42e4-a5ff-b54b0f6355fe_93_001.parquet
132982 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/800b6fac-404a-42e4-a5ff-b54b0f6355fe_93_001.parquet => [625110ab-d53e-4dd6-b347-d827ba2af870]
132996 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/8106bf46-148c-48d6-b451-c36e5bb816e0_46_001.parquet
133002 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/8106bf46-148c-48d6-b451-c36e5bb816e0_46_001.parquet
133002 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/8106bf46-148c-48d6-b451-c36e5bb816e0_46_001.parquet => [a2672691-5274-4630-8e77-d8df9c22f339]
133010 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/876011e3-0efe-4688-9c18-9e02967594dd_30_001.parquet
133030 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/876011e3-0efe-4688-9c18-9e02967594dd_30_001.parquet
133030 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/876011e3-0efe-4688-9c18-9e02967594dd_30_001.parquet => [6428a157-0e25-42ee-8f1f-0ca53342ed85]
133050 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/8dad587d-c2a9-41cb-bc30-9e61184b25d8_197_001.parquet
133080 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/8dad587d-c2a9-41cb-bc30-9e61184b25d8_197_001.parquet
133080 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/8dad587d-c2a9-41cb-bc30-9e61184b25d8_197_001.parquet => [f968d07d-d32a-44f6-a00c-e9a1caaa1467]
133089 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/8e929c4d-842f-44ac-b00a-08262751b510_64_001.parquet
133111 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/8e929c4d-842f-44ac-b00a-08262751b510_64_001.parquet
133111 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/8e929c4d-842f-44ac-b00a-08262751b510_64_001.parquet => [ee5c21ab-de76-4245-9776-7264a06608bd]
133119 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/918a558f-1611-40be-b818-544b043beec8_161_001.parquet
133132 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/918a558f-1611-40be-b818-544b043beec8_161_001.parquet
133132 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/918a558f-1611-40be-b818-544b043beec8_161_001.parquet => [875f7fe3-d03f-48f1-9005-16ea7b9393fa]
133140 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/93acbb10-5fd0-4590-9a49-0c1242448cf0_194_001.parquet
133165 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/93acbb10-5fd0-4590-9a49-0c1242448cf0_194_001.parquet
133165 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/93acbb10-5fd0-4590-9a49-0c1242448cf0_194_001.parquet => [f6f54480-05dc-4e93-a9c4-4eef5ba43110]
133175 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/9b6f67cf-7dc4-45aa-af4f-5ee099e9bb56_24_001.parquet
133180 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/9b6f67cf-7dc4-45aa-af4f-5ee099e9bb56_24_001.parquet
133180 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/9b6f67cf-7dc4-45aa-af4f-5ee099e9bb56_24_001.parquet => [4b517329-ab06-4374-9060-2e9ce45000a0]
133190 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/9d41d981-e719-4374-b1a8-54f71ddbfe80_162_001.parquet
133195 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/9d41d981-e719-4374-b1a8-54f71ddbfe80_162_001.parquet
133195 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/9d41d981-e719-4374-b1a8-54f71ddbfe80_162_001.parquet => [8877d278-ddc2-467e-aee6-f3556a3c3a35]
133203 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/a600aaa8-2d93-4986-9267-cf17a29fd9f5_188_001.parquet
133208 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/a600aaa8-2d93-4986-9267-cf17a29fd9f5_188_001.parquet
133208 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/a600aaa8-2d93-4986-9267-cf17a29fd9f5_188_001.parquet => [e3ab2d69-be5f-4ec7-8197-ae1faa045af7]
133217 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/ac9e37e8-957e-4e96-abcb-46edca78d161_73_001.parquet
133223 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/ac9e37e8-957e-4e96-abcb-46edca78d161_73_001.parquet
133223 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/ac9e37e8-957e-4e96-abcb-46edca78d161_73_001.parquet => [0cf43baf-6a52-42d7-9f07-43f81aeffb21]
133231 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/acf746c9-de0a-4583-8289-9a2d8c4b310b_134_001.parquet
133238 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/acf746c9-de0a-4583-8289-9a2d8c4b310b_134_001.parquet
133238 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/acf746c9-de0a-4583-8289-9a2d8c4b310b_134_001.parquet => [f1f8ef4e-db95-4eb3-a67f-7b628cae1e1b]
133246 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/b01c2c24-805e-4cc0-aba2-bd3188e79040_167_001.parquet
133251 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/b01c2c24-805e-4cc0-aba2-bd3188e79040_167_001.parquet
133251 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/b01c2c24-805e-4cc0-aba2-bd3188e79040_167_001.parquet => [b0caf57a-d1b7-4003-a4c3-9b555b4b3d47]
133260 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/b23bf3a6-bc03-4292-9b76-57c26a7881ba_121_001.parquet
133265 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/b23bf3a6-bc03-4292-9b76-57c26a7881ba_121_001.parquet
133265 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/b23bf3a6-bc03-4292-9b76-57c26a7881ba_121_001.parquet => [bee9e6fe-9c74-431d-af20-681fe626b86d]
133281 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/b31f4a6b-f44d-4034-99b5-9f17c5290c3f_41_001.parquet
133286 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/b31f4a6b-f44d-4034-99b5-9f17c5290c3f_41_001.parquet
133286 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/b31f4a6b-f44d-4034-99b5-9f17c5290c3f_41_001.parquet => [91f4407a-9393-4f24-a8bd-052ce4fc3dcb]
133295 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/b4747a92-5432-4123-967f-963da959ebef_130_001.parquet
133299 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/b4747a92-5432-4123-967f-963da959ebef_130_001.parquet
133299 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/b4747a92-5432-4123-967f-963da959ebef_130_001.parquet => [e84ea279-09c8-400e-8394-3c0b443c3ffc]
133310 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/b4cd93ed-96cf-45b7-9a34-29934fa8982e_32_001.parquet
133314 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/b4cd93ed-96cf-45b7-9a34-29934fa8982e_32_001.parquet
133314 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/b4cd93ed-96cf-45b7-9a34-29934fa8982e_32_001.parquet => [7061b974-a514-41fb-acfc-08c84fd476f2]
133323 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/b9e14bba-f16d-4f62-afac-05bd977ee425_56_001.parquet
133328 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/b9e14bba-f16d-4f62-afac-05bd977ee425_56_001.parquet
133328 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/b9e14bba-f16d-4f62-afac-05bd977ee425_56_001.parquet => [d68fe09d-9f5b-4397-8410-0b6ce3c75799]
133336 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/ba3cc780-de19-4779-b63e-971c525ef80f_110_001.parquet
133341 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/ba3cc780-de19-4779-b63e-971c525ef80f_110_001.parquet
133341 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/ba3cc780-de19-4779-b63e-971c525ef80f_110_001.parquet => [92ccaf47-be68-4187-8f37-9593f36636d0]
133350 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/bb1aa980-35a0-45cc-aa54-77f9d08e0651_159_001.parquet
133355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/bb1aa980-35a0-45cc-aa54-77f9d08e0651_159_001.parquet
133355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/bb1aa980-35a0-45cc-aa54-77f9d08e0651_159_001.parquet => [7f7e620b-e4c7-4910-95e2-d201686a1078]
133365 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/bdc23b21-437e-420c-b4d6-b98589dd0baf_108_001.parquet
133372 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/bdc23b21-437e-420c-b4d6-b98589dd0baf_108_001.parquet
133372 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/bdc23b21-437e-420c-b4d6-b98589dd0baf_108_001.parquet => [878d1d30-fd42-4d87-99f8-4d233817b315]
133380 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/c0dd0384-e2b3-403b-8d65-be08322504d3_67_001.parquet
133396 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/c0dd0384-e2b3-403b-8d65-be08322504d3_67_001.parquet
133396 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/c0dd0384-e2b3-403b-8d65-be08322504d3_67_001.parquet => [fd63467a-3fb6-454f-ab34-19170ed2aca0]
133406 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/c39df059-90e1-4a0f-a841-1ca3aeec5a20_113_001.parquet
133412 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/c39df059-90e1-4a0f-a841-1ca3aeec5a20_113_001.parquet
133412 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/c39df059-90e1-4a0f-a841-1ca3aeec5a20_113_001.parquet => [9befef90-7e6d-4ca6-a079-a80ebe7ef248]
133423 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/c5864bf7-8b94-4cb3-8f47-ae6329b9a44f_61_001.parquet
133428 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/c5864bf7-8b94-4cb3-8f47-ae6329b9a44f_61_001.parquet
133428 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/c5864bf7-8b94-4cb3-8f47-ae6329b9a44f_61_001.parquet => [de91fe99-0e5b-46bb-861a-c16657021563]
133436 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/c967f07d-42e0-4b68-9fc5-9927c70048a2_76_001.parquet
133440 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/c967f07d-42e0-4b68-9fc5-9927c70048a2_76_001.parquet
133440 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/c967f07d-42e0-4b68-9fc5-9927c70048a2_76_001.parquet => [11f36d5f-a96c-4cb8-bf28-2c42fb198434]
133449 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/cb0583d6-b4d0-4199-8635-8e50c40c3be1_11_001.parquet
133453 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/cb0583d6-b4d0-4199-8635-8e50c40c3be1_11_001.parquet
133453 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/cb0583d6-b4d0-4199-8635-8e50c40c3be1_11_001.parquet => [263fa2bf-788f-47fc-8fd9-49ad25f797ea]
133462 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/cebd173f-b616-4dd8-b583-5d1f68507409_44_001.parquet
133466 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/cebd173f-b616-4dd8-b583-5d1f68507409_44_001.parquet
133466 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/cebd173f-b616-4dd8-b583-5d1f68507409_44_001.parquet => [9f8fd817-f3dd-439f-9295-d5382a847157]
133476 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/d3fddd8b-0131-40fc-9a3c-c9c5d9f7993b_156_001.parquet
133480 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/d3fddd8b-0131-40fc-9a3c-c9c5d9f7993b_156_001.parquet
133480 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/d3fddd8b-0131-40fc-9a3c-c9c5d9f7993b_156_001.parquet => [56cd855d-91c8-4c5f-84b7-5f3dd8667e4d]
133489 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/dc4a4da2-cd8a-408d-bd63-0b77d91752fa_157_001.parquet
133494 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/dc4a4da2-cd8a-408d-bd63-0b77d91752fa_157_001.parquet
133494 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/dc4a4da2-cd8a-408d-bd63-0b77d91752fa_157_001.parquet => [6878ca51-1b87-435a-afec-d3fba2eb31e5]
133509 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/dd6fa9d3-c4c0-4430-8da0-db0e07e9be3d_26_001.parquet
133514 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/dd6fa9d3-c4c0-4430-8da0-db0e07e9be3d_26_001.parquet
133514 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/dd6fa9d3-c4c0-4430-8da0-db0e07e9be3d_26_001.parquet => [59a2d672-abd4-487e-ab74-a8249bf50891]
133522 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/de46f398-76a5-46e4-991a-76a7a0fb71f9_151_001.parquet
133526 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/de46f398-76a5-46e4-991a-76a7a0fb71f9_151_001.parquet
133526 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/de46f398-76a5-46e4-991a-76a7a0fb71f9_151_001.parquet => [4677de01-80d3-44ef-be0b-7db6fbf2b332]
133535 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/e70573e5-0979-4e75-93b7-5f0a4378936d_16_001.parquet
133540 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/e70573e5-0979-4e75-93b7-5f0a4378936d_16_001.parquet
133540 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/e70573e5-0979-4e75-93b7-5f0a4378936d_16_001.parquet => [331fa8de-932c-4bbf-83a0-7f2cfeec13d1]
133548 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/e7d3ff0f-5da5-43c1-968e-63a096de00dc_107_001.parquet
133553 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/e7d3ff0f-5da5-43c1-968e-63a096de00dc_107_001.parquet
133553 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/e7d3ff0f-5da5-43c1-968e-63a096de00dc_107_001.parquet => [85f4c4ca-8578-46db-8019-9c446294ddb6]
133562 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/e8276818-9813-4ffe-830b-55167efb023b_58_001.parquet
133566 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/e8276818-9813-4ffe-830b-55167efb023b_58_001.parquet
133566 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/e8276818-9813-4ffe-830b-55167efb023b_58_001.parquet => [d980e859-70a7-458c-88be-3be75582a2f1]
133575 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/e8deb184-c7d1-423e-9e6f-a0c17854e186_55_001.parquet
133580 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/e8deb184-c7d1-423e-9e6f-a0c17854e186_55_001.parquet
133580 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/e8deb184-c7d1-423e-9e6f-a0c17854e186_55_001.parquet => [cb87e0ef-1413-4627-9d5d-9db1f63de1f0]
133589 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/e9d2d123-1faa-4321-9314-3be8397ebd79_175_001.parquet
133614 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/e9d2d123-1faa-4321-9314-3be8397ebd79_175_001.parquet
133614 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/e9d2d123-1faa-4321-9314-3be8397ebd79_175_001.parquet => [ccf1d3ed-e23a-46ac-9caa-f76b867612de]
133622 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/ea29f756-0ab5-4112-b9a7-14ebe15ad2ae_38_001.parquet
133633 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/ea29f756-0ab5-4112-b9a7-14ebe15ad2ae_38_001.parquet
133633 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/ea29f756-0ab5-4112-b9a7-14ebe15ad2ae_38_001.parquet => [85c41d96-fbcb-44ef-9ef8-98f01fbe4a7a]
133641 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/ee15348f-ec0d-4a1e-9192-344217907aef_1_001.parquet
133646 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/ee15348f-ec0d-4a1e-9192-344217907aef_1_001.parquet
133646 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/ee15348f-ec0d-4a1e-9192-344217907aef_1_001.parquet => [0741bd9f-171e-4d27-927f-82cef96a0357]
133663 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/f82c2afd-ee49-4859-962d-8efe85d82303_2_001.parquet
133667 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/f82c2afd-ee49-4859-962d-8efe85d82303_2_001.parquet
133667 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/f82c2afd-ee49-4859-962d-8efe85d82303_2_001.parquet => [0bc08145-d1f6-434b-b913-db645de99ddc]
133677 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/f863cb45-94a1-4bcf-a58e-d50f07929272_149_001.parquet
133681 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/f863cb45-94a1-4bcf-a58e-d50f07929272_149_001.parquet
133681 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/f863cb45-94a1-4bcf-a58e-d50f07929272_149_001.parquet => [41925107-75a1-4fe4-9636-90e8741af30a]
133690 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/16/f8b9a037-08d0-4710-a85d-ce17e706396c_42_001.parquet
133694 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/16/f8b9a037-08d0-4710-a85d-ce17e706396c_42_001.parquet
133694 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/16/f8b9a037-08d0-4710-a85d-ce17e706396c_42_001.parquet => [984fe837-566b-4a5b-a6a3-ace46b2daffb]
133711 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2015/03/17/f8ce1bc8-128f-4325-acb0-56edb530d2d2_97_001.parquet
133723 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2015/03/17/f8ce1bc8-128f-4325-acb0-56edb530d2d2_97_001.parquet
133723 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2015/03/17/f8ce1bc8-128f-4325-acb0-56edb530d2d2_97_001.parquet => [6ca9744c-1fa5-45e8-93fb-ea174fca63c4]
133732 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit6818238807349966743/2016/03/15/fb64f25e-69f3-4e09-b8d6-918ebb452e6f_182_001.parquet
133736 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit6818238807349966743/2016/03/15/fb64f25e-69f3-4e09-b8d6-918ebb452e6f_182_001.parquet
133736 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit6818238807349966743/2016/03/15/fb64f25e-69f3-4e09-b8d6-918ebb452e6f_182_001.parquet => [d89222fe-5114-40e4-91e2-663c5dbe8625]
133816 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=81}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
133843 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 435020
133853 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :81, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=876011e3-0efe-4688-9c18-9e02967594dd}, 1=BucketInfo {bucketType=UPDATE, fileLoc=7f1edca1-4f43-4467-b58c-dad648959800}, 2=BucketInfo {bucketType=UPDATE, fileLoc=b4cd93ed-96cf-45b7-9a34-29934fa8982e}, 3=BucketInfo {bucketType=UPDATE, fileLoc=03f75e6b-4ee1-4ea4-ad9c-c57f20288cca}, 4=BucketInfo {bucketType=UPDATE, fileLoc=f8b9a037-08d0-4710-a85d-ce17e706396c}, 5=BucketInfo {bucketType=UPDATE, fileLoc=c967f07d-42e0-4b68-9fc5-9927c70048a2}, 6=BucketInfo {bucketType=UPDATE, fileLoc=b9e14bba-f16d-4f62-afac-05bd977ee425}, 7=BucketInfo {bucketType=UPDATE, fileLoc=ba3cc780-de19-4779-b63e-971c525ef80f}, 8=BucketInfo {bucketType=UPDATE, fileLoc=bdc23b21-437e-420c-b4d6-b98589dd0baf}, 9=BucketInfo {bucketType=UPDATE, fileLoc=16e4c4c1-f74e-4f3b-a2cb-315c983b477a}, 10=BucketInfo {bucketType=UPDATE, fileLoc=dd6fa9d3-c4c0-4430-8da0-db0e07e9be3d}, 11=BucketInfo {bucketType=UPDATE, fileLoc=acf746c9-de0a-4583-8289-9a2d8c4b310b}, 12=BucketInfo {bucketType=UPDATE, fileLoc=9b6f67cf-7dc4-45aa-af4f-5ee099e9bb56}, 13=BucketInfo {bucketType=UPDATE, fileLoc=8dad587d-c2a9-41cb-bc30-9e61184b25d8}, 14=BucketInfo {bucketType=UPDATE, fileLoc=3a049335-0131-4682-b92e-46ed044da083}, 15=BucketInfo {bucketType=UPDATE, fileLoc=3b40f637-c826-406a-9c9b-2c62bc3d64bf}, 16=BucketInfo {bucketType=UPDATE, fileLoc=b01c2c24-805e-4cc0-aba2-bd3188e79040}, 17=BucketInfo {bucketType=UPDATE, fileLoc=4aed95a4-d030-41fb-b5ac-9d5830ed82cd}, 18=BucketInfo {bucketType=UPDATE, fileLoc=bb1aa980-35a0-45cc-aa54-77f9d08e0651}, 19=BucketInfo {bucketType=UPDATE, fileLoc=5f6d32c8-a001-4595-a348-76d153fb50b9}, 20=BucketInfo {bucketType=UPDATE, fileLoc=f8ce1bc8-128f-4325-acb0-56edb530d2d2}, 21=BucketInfo {bucketType=UPDATE, fileLoc=285da6d2-c5d5-4ebe-93e7-bef0a95c05be}, 22=BucketInfo {bucketType=UPDATE, fileLoc=670b706e-8976-4dc2-a387-fdf716f246b4}, 23=BucketInfo {bucketType=UPDATE, fileLoc=4476d726-0ac3-434e-9c73-bea3ea202d9a}, 24=BucketInfo {bucketType=UPDATE, fileLoc=35737386-2ad8-490f-9a69-d54b43cd6230}, 25=BucketInfo {bucketType=UPDATE, fileLoc=dc4a4da2-cd8a-408d-bd63-0b77d91752fa}, 26=BucketInfo {bucketType=UPDATE, fileLoc=1e2aa031-2263-487a-b436-3139150ebda6}, 27=BucketInfo {bucketType=UPDATE, fileLoc=800b6fac-404a-42e4-a5ff-b54b0f6355fe}, 28=BucketInfo {bucketType=UPDATE, fileLoc=b23bf3a6-bc03-4292-9b76-57c26a7881ba}, 29=BucketInfo {bucketType=UPDATE, fileLoc=22d149e8-f9d5-438f-8714-e09035531933}, 30=BucketInfo {bucketType=UPDATE, fileLoc=b31f4a6b-f44d-4034-99b5-9f17c5290c3f}, 31=BucketInfo {bucketType=UPDATE, fileLoc=1752f54c-ab6f-4c48-b21e-31ef3da7b1bd}, 32=BucketInfo {bucketType=UPDATE, fileLoc=5e36ac80-78bf-4042-af83-caf88bebdb51}, 33=BucketInfo {bucketType=UPDATE, fileLoc=a600aaa8-2d93-4986-9267-cf17a29fd9f5}, 34=BucketInfo {bucketType=UPDATE, fileLoc=c0dd0384-e2b3-403b-8d65-be08322504d3}, 35=BucketInfo {bucketType=UPDATE, fileLoc=d3fddd8b-0131-40fc-9a3c-c9c5d9f7993b}, 36=BucketInfo {bucketType=UPDATE, fileLoc=3cccd204-6850-48e0-b70b-50d8c18bee03}, 37=BucketInfo {bucketType=UPDATE, fileLoc=9d41d981-e719-4374-b1a8-54f71ddbfe80}, 38=BucketInfo {bucketType=UPDATE, fileLoc=536a557b-9c4d-46ee-aee0-943d02bc05de}, 39=BucketInfo {bucketType=UPDATE, fileLoc=7dbe4668-b1a5-449b-a04a-c78b6d5284b9}, 40=BucketInfo {bucketType=UPDATE, fileLoc=f863cb45-94a1-4bcf-a58e-d50f07929272}, 41=BucketInfo {bucketType=UPDATE, fileLoc=c39df059-90e1-4a0f-a841-1ca3aeec5a20}, 42=BucketInfo {bucketType=UPDATE, fileLoc=de46f398-76a5-46e4-991a-76a7a0fb71f9}, 43=BucketInfo {bucketType=UPDATE, fileLoc=fb64f25e-69f3-4e09-b8d6-918ebb452e6f}, 44=BucketInfo {bucketType=UPDATE, fileLoc=e8276818-9813-4ffe-830b-55167efb023b}, 45=BucketInfo {bucketType=UPDATE, fileLoc=e8deb184-c7d1-423e-9e6f-a0c17854e186}, 46=BucketInfo {bucketType=UPDATE, fileLoc=1599d4b0-0514-4fe2-94f5-a150c991ed4f}, 47=BucketInfo {bucketType=UPDATE, fileLoc=6149e35a-605a-4678-a022-60517759e7f6}, 48=BucketInfo {bucketType=UPDATE, fileLoc=ea29f756-0ab5-4112-b9a7-14ebe15ad2ae}, 49=BucketInfo {bucketType=UPDATE, fileLoc=ee15348f-ec0d-4a1e-9192-344217907aef}, 50=BucketInfo {bucketType=UPDATE, fileLoc=01bc087c-032d-467f-ae53-34172976e16d}, 51=BucketInfo {bucketType=UPDATE, fileLoc=cb0583d6-b4d0-4199-8635-8e50c40c3be1}, 52=BucketInfo {bucketType=UPDATE, fileLoc=ac9e37e8-957e-4e96-abcb-46edca78d161}, 53=BucketInfo {bucketType=UPDATE, fileLoc=4d743167-ca3c-4c50-9201-eb8498f45786}, 54=BucketInfo {bucketType=UPDATE, fileLoc=49dd3216-4e79-4d8f-aa06-7daecc263284}, 55=BucketInfo {bucketType=UPDATE, fileLoc=521c1d39-9142-469e-8c1a-90f0056060e9}, 56=BucketInfo {bucketType=UPDATE, fileLoc=3059aa6e-7f56-4cae-a11b-69683c1fb571}, 57=BucketInfo {bucketType=UPDATE, fileLoc=2288dee4-8fee-4788-a956-032ed38af310}, 58=BucketInfo {bucketType=UPDATE, fileLoc=45fd8909-6833-4ab8-898b-d8f67268bf3d}, 59=BucketInfo {bucketType=UPDATE, fileLoc=e7d3ff0f-5da5-43c1-968e-63a096de00dc}, 60=BucketInfo {bucketType=UPDATE, fileLoc=93acbb10-5fd0-4590-9a49-0c1242448cf0}, 61=BucketInfo {bucketType=UPDATE, fileLoc=cebd173f-b616-4dd8-b583-5d1f68507409}, 62=BucketInfo {bucketType=UPDATE, fileLoc=38e07c9f-115f-4bbb-8124-7accb1932559}, 63=BucketInfo {bucketType=UPDATE, fileLoc=32a9bd42-9c15-4158-aebc-c7cecac563dd}, 64=BucketInfo {bucketType=UPDATE, fileLoc=6d506052-e24c-4dc6-a38f-fad9e8462f79}, 65=BucketInfo {bucketType=UPDATE, fileLoc=03b571d1-9c3a-4ab2-bff9-8ac63a58c612}, 66=BucketInfo {bucketType=UPDATE, fileLoc=8106bf46-148c-48d6-b451-c36e5bb816e0}, 67=BucketInfo {bucketType=UPDATE, fileLoc=69026d34-1213-4b12-b2ba-0a73a9d909d1}, 68=BucketInfo {bucketType=UPDATE, fileLoc=e70573e5-0979-4e75-93b7-5f0a4378936d}, 69=BucketInfo {bucketType=UPDATE, fileLoc=c5864bf7-8b94-4cb3-8f47-ae6329b9a44f}, 70=BucketInfo {bucketType=UPDATE, fileLoc=39b66b5d-32d0-4f43-a08a-898ab8090ffc}, 71=BucketInfo {bucketType=UPDATE, fileLoc=6159f851-de13-4b20-8926-fa1851ea9388}, 72=BucketInfo {bucketType=UPDATE, fileLoc=12489847-3c1f-4295-93ff-82083d2aed9a}, 73=BucketInfo {bucketType=UPDATE, fileLoc=61ad6d0a-8c35-428f-90c3-8365e4883953}, 74=BucketInfo {bucketType=UPDATE, fileLoc=5a5f1a72-7118-4a30-a455-73496d7e3937}, 75=BucketInfo {bucketType=UPDATE, fileLoc=361963f0-04d4-4558-82ea-c1b85eb5459b}, 76=BucketInfo {bucketType=UPDATE, fileLoc=8e929c4d-842f-44ac-b00a-08262751b510}, 77=BucketInfo {bucketType=UPDATE, fileLoc=e9d2d123-1faa-4321-9314-3be8397ebd79}, 78=BucketInfo {bucketType=UPDATE, fileLoc=f82c2afd-ee49-4859-962d-8efe85d82303}, 79=BucketInfo {bucketType=UPDATE, fileLoc=b4747a92-5432-4123-967f-963da959ebef}, 80=BucketInfo {bucketType=UPDATE, fileLoc=918a558f-1611-40be-b818-544b043beec8}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{876011e3-0efe-4688-9c18-9e02967594dd=0, 7f1edca1-4f43-4467-b58c-dad648959800=1, b4cd93ed-96cf-45b7-9a34-29934fa8982e=2, 03f75e6b-4ee1-4ea4-ad9c-c57f20288cca=3, f8b9a037-08d0-4710-a85d-ce17e706396c=4, c967f07d-42e0-4b68-9fc5-9927c70048a2=5, b9e14bba-f16d-4f62-afac-05bd977ee425=6, ba3cc780-de19-4779-b63e-971c525ef80f=7, bdc23b21-437e-420c-b4d6-b98589dd0baf=8, 16e4c4c1-f74e-4f3b-a2cb-315c983b477a=9, dd6fa9d3-c4c0-4430-8da0-db0e07e9be3d=10, acf746c9-de0a-4583-8289-9a2d8c4b310b=11, 9b6f67cf-7dc4-45aa-af4f-5ee099e9bb56=12, 8dad587d-c2a9-41cb-bc30-9e61184b25d8=13, 3a049335-0131-4682-b92e-46ed044da083=14, 3b40f637-c826-406a-9c9b-2c62bc3d64bf=15, b01c2c24-805e-4cc0-aba2-bd3188e79040=16, 4aed95a4-d030-41fb-b5ac-9d5830ed82cd=17, bb1aa980-35a0-45cc-aa54-77f9d08e0651=18, 5f6d32c8-a001-4595-a348-76d153fb50b9=19, f8ce1bc8-128f-4325-acb0-56edb530d2d2=20, 285da6d2-c5d5-4ebe-93e7-bef0a95c05be=21, 670b706e-8976-4dc2-a387-fdf716f246b4=22, 4476d726-0ac3-434e-9c73-bea3ea202d9a=23, 35737386-2ad8-490f-9a69-d54b43cd6230=24, dc4a4da2-cd8a-408d-bd63-0b77d91752fa=25, 1e2aa031-2263-487a-b436-3139150ebda6=26, 800b6fac-404a-42e4-a5ff-b54b0f6355fe=27, b23bf3a6-bc03-4292-9b76-57c26a7881ba=28, 22d149e8-f9d5-438f-8714-e09035531933=29, b31f4a6b-f44d-4034-99b5-9f17c5290c3f=30, 1752f54c-ab6f-4c48-b21e-31ef3da7b1bd=31, 5e36ac80-78bf-4042-af83-caf88bebdb51=32, a600aaa8-2d93-4986-9267-cf17a29fd9f5=33, c0dd0384-e2b3-403b-8d65-be08322504d3=34, d3fddd8b-0131-40fc-9a3c-c9c5d9f7993b=35, 3cccd204-6850-48e0-b70b-50d8c18bee03=36, 9d41d981-e719-4374-b1a8-54f71ddbfe80=37, 536a557b-9c4d-46ee-aee0-943d02bc05de=38, 7dbe4668-b1a5-449b-a04a-c78b6d5284b9=39, f863cb45-94a1-4bcf-a58e-d50f07929272=40, c39df059-90e1-4a0f-a841-1ca3aeec5a20=41, de46f398-76a5-46e4-991a-76a7a0fb71f9=42, fb64f25e-69f3-4e09-b8d6-918ebb452e6f=43, e8276818-9813-4ffe-830b-55167efb023b=44, e8deb184-c7d1-423e-9e6f-a0c17854e186=45, 1599d4b0-0514-4fe2-94f5-a150c991ed4f=46, 6149e35a-605a-4678-a022-60517759e7f6=47, ea29f756-0ab5-4112-b9a7-14ebe15ad2ae=48, ee15348f-ec0d-4a1e-9192-344217907aef=49, 01bc087c-032d-467f-ae53-34172976e16d=50, cb0583d6-b4d0-4199-8635-8e50c40c3be1=51, ac9e37e8-957e-4e96-abcb-46edca78d161=52, 4d743167-ca3c-4c50-9201-eb8498f45786=53, 49dd3216-4e79-4d8f-aa06-7daecc263284=54, 521c1d39-9142-469e-8c1a-90f0056060e9=55, 3059aa6e-7f56-4cae-a11b-69683c1fb571=56, 2288dee4-8fee-4788-a956-032ed38af310=57, 45fd8909-6833-4ab8-898b-d8f67268bf3d=58, e7d3ff0f-5da5-43c1-968e-63a096de00dc=59, 93acbb10-5fd0-4590-9a49-0c1242448cf0=60, cebd173f-b616-4dd8-b583-5d1f68507409=61, 38e07c9f-115f-4bbb-8124-7accb1932559=62, 32a9bd42-9c15-4158-aebc-c7cecac563dd=63, 6d506052-e24c-4dc6-a38f-fad9e8462f79=64, 03b571d1-9c3a-4ab2-bff9-8ac63a58c612=65, 8106bf46-148c-48d6-b451-c36e5bb816e0=66, 69026d34-1213-4b12-b2ba-0a73a9d909d1=67, e70573e5-0979-4e75-93b7-5f0a4378936d=68, c5864bf7-8b94-4cb3-8f47-ae6329b9a44f=69, 39b66b5d-32d0-4f43-a08a-898ab8090ffc=70, 6159f851-de13-4b20-8926-fa1851ea9388=71, 12489847-3c1f-4295-93ff-82083d2aed9a=72, 61ad6d0a-8c35-428f-90c3-8365e4883953=73, 5a5f1a72-7118-4a30-a455-73496d7e3937=74, 361963f0-04d4-4558-82ea-c1b85eb5459b=75, 8e929c4d-842f-44ac-b00a-08262751b510=76, e9d2d123-1faa-4321-9314-3be8397ebd79=77, f82c2afd-ee49-4859-962d-8efe85d82303=78, b4747a92-5432-4123-967f-963da959ebef=79, 918a558f-1611-40be-b818-544b043beec8=80}
133865 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 002
133883 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
140141 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
140141 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
141387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
141387 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
141525 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
141534 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
141534 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
141643 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
141958 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
141958 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
142283 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=65, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=60, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=75, numUpdates=0}}}
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 65, totalInsertBuckets => 1, recordsPerBucket => 500000
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 60, totalInsertBuckets => 1, recordsPerBucket => 500000
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 75, totalInsertBuckets => 1, recordsPerBucket => 500000
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
142348 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
142357 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
142358 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
142432 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
142453 [pool-1371-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
142480 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
142480 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
142522 [pool-1371-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
142534 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
142562 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
142569 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
142568 [pool-1372-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
142615 [pool-1372-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
142628 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
142657 [pool-1373-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
142662 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
142667 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
142725 [pool-1373-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
142757 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
142757 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
142827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
142827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
142939 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
142947 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
142947 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
143010 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
143361 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
143361 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
143558 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_1_001.parquet
143564 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_1_001.parquet
143564 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_1_001.parquet => [00f02836-3dc7-4d71-b98e-351b93b59879, 0451e8f9-9372-4aec-a358-8ce9ae747ada, 05a65fd8-214f-4840-8d2d-0aacf3b1ab91, 0cc65acb-e36b-4182-916d-0ac73bf9360f, 10e4d975-bcf1-4e6f-aec9-731d2ee69909, 15299105-c88d-4854-89d4-e672d1586ce4, 1c4d9a4f-a1c0-4bef-b5bb-f80686e5e341, 1de4af1f-a050-4b4f-9163-2a9a47c89eae, 1ded34de-65be-42f7-9379-30fe752dee45, 1e1bc8cb-29d3-4e0c-96df-0ebebe1fc042, 1e4ed7f8-949b-453b-868e-3225887296bb, 1e5ed6ce-5801-4b62-9fde-1558e5a5fe73, 2102cf7a-b49e-4006-a32c-6909dc4709df, 211e0346-c77e-44c8-a941-ce18bbc28695, 2216f5ae-b0aa-4ac5-bd5f-1205f3ff302b, 25c07457-53b1-44b5-a55c-40fdef73f059, 2817226c-5b9e-49ca-8f81-9f277e46e25e, 307e3cd0-e61c-4854-a0a8-aff290ecf8f9, 343cc1c9-07bd-47e1-8678-02edbb447536, 3ba2668c-e526-4665-8764-9b1b7b4bdbf4, 3bab7fd5-042d-4b37-baf2-9eb17a1102fd, 481b177f-498b-4ee5-9eb4-d58b2ed21ed2, 4c4ff3d8-65ab-41c7-acf4-078746c3b863, 4cc9eddd-bc5f-4b49-b229-63352106fe49, 51139b62-7f44-4961-a3a3-0c2db033ff94, 55fd07f8-ab2b-449e-b036-3ccba41f4df3, 5abe43e1-8a1e-497f-86fe-a4b345ec1a29, 5b57dd54-b64b-4ed4-bf5e-199fab7214d5, 5e9a2b89-d5e7-427e-ad85-1b62cbff7d55, 60ec2ea4-b752-4fd5-8c54-fbafe620c1b8, 635fc87c-8e11-4a05-a592-3976f9ab58c8, 6eaa21df-97be-4053-84e1-ac53fa966ded, 73eca9de-c817-44ce-b0e6-1dcaeee2ff86, 772a0458-1eb5-45a7-9bab-c6eec319e7e1, 7e4414a7-aa24-4be5-966b-d1ca3505f078, 8257252b-d2f8-4371-bbd7-07991f060089, 86b34e68-5d68-47d9-a51f-c6f86b56b606, 8f107cf6-8d9e-44c2-82d9-f6581516292d, 8f60bcf3-bb72-418f-9732-742e2d929708, 91643813-45fc-4d3a-b5f8-a7b228e51827, 96ba2f91-0d47-4bca-b6a0-704f2103a758, 99c37260-b5f1-4a01-8fd3-4139f92fc700, a1f03c68-d8ae-44cd-8e28-a87de37a7d4b, a3236aba-c547-48bd-8ad6-fe6edb53aa9b, a5bbb418-e409-406b-bb89-6349b4796ab5, ad23f84d-344f-4bdf-adf4-4898aca3a376, ae81b315-f165-4c5b-a97a-ff3954122ea4, b4870fd4-6973-4d4a-bfdb-576054822bc1, b49d0edf-b18e-45c1-9ce1-1be17cc4a21e, b6121340-8e09-4b5b-9b65-d6612c0a15a4, bfdd2ab7-0be5-4a6e-9f29-5edfa2c2428f, cc517d1c-91e6-4d59-975f-4dccd4d2609f, cdd0f434-4307-4eef-ad95-3388160f6b86, d350d657-2553-46b9-8ac8-648c69282500, db350c28-b43c-4782-a6d3-12ea5983bade, f06abb19-26b6-425d-8b61-6d985b21f8bd, f333c2d5-2175-4750-b5d3-ffad2c9571ae, f68c59e1-d753-4f7a-888a-67b23ea3150f, fc2cb7d9-1f76-412d-865d-1ca4e9260a11, fd78e21b-c55b-4a3c-937b-2f8f153cbccc]
143575 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 51 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet
143579 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet
143579 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 51 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet => [0314b458-dcb3-4db7-b7aa-aaef6f652dc3, 0a81e566-b002-469b-ade2-6fee8393bf9a, 1738514c-0e4f-444f-8549-72f903ad91e8, 230d99a0-adc1-4450-828d-272863ab52f2, 24f71b65-c92c-4a96-8392-1b55d8296c28, 26aaef4a-1522-4106-bf92-639aa6e6955a, 282cf8e0-0527-4ae4-b02c-54645efa9c8d, 2e9e2d53-1bdf-4b0e-bb53-f0836dc75718, 2fb2f682-99e3-4787-a2b4-03af8eb2370d, 30e6202e-045a-4ea4-a7b7-82e553bce1e8, 3246e2c7-ab7f-43c0-b14e-33f077f82499, 32920b8b-9263-49f5-8210-0f904140833d, 3a46e68d-acec-45e0-99b4-beb4eb38cbe4, 4325caef-7d59-47da-8fc3-f74d0bd003ff, 43ccdc3c-e205-4f15-821c-4035278baf43, 55ee4132-e188-40e3-be5d-13f3d764abef, 5716ae83-93d2-4170-aad5-e523829b4132, 594de09d-4490-4390-a50f-b4583789817a, 62ef3de3-fd77-4837-9799-1e6fdb31345b, 67c88b04-1f68-4941-b508-6679219f4835, 68fb8cc7-cf94-435d-adaa-41c927df3512, 690f5c07-b934-4867-bf58-b7517d332dea, 6e062c55-269a-40dc-b65b-3ff236cc2304, 7353467d-7ef2-4837-92ea-e344a00d3ca0, 793325a4-ff2e-4bf2-8326-ca3a5434c7f8, 7d9c900c-51a4-4dd2-8224-1e33d0474674, 8093d052-15a4-45fa-85ef-ce5b7a168515, 83521641-671d-4039-968f-6e0ac8b7df0f, 8373e2df-303b-4025-b596-85ce7910c6c1, 845cc75a-4819-43b2-8c2a-b47360c6cbf3, 883ca5b7-fffc-4645-afb8-93a993dbcbf1, 8a68c17b-0109-42a0-aff1-de75efcf2bf4, 8c968a54-9e66-4d2e-bebd-bebce0015df9, 9ee4337d-e690-4507-b11e-4b82b68d1135, 9f2937f5-ecae-4667-9d66-28e66eecafac, 9fc45588-dd8a-402b-8292-9ddaa61e165d, a4ac9a41-1c9f-47d6-97b2-bbac65a0be8c, a5908620-9a0b-4a51-9cd1-38a52d54b509, a609a744-f1c8-46ec-b7b1-ab25643720f4, abd58ce9-c682-465b-bf22-cdb04280a9e8, ac4735a7-b65a-41a2-bd75-be4bd399fe28, ac5048dc-88c9-4450-a438-a4541cd86463, aea64ce3-ab9a-4448-b00b-0dfa90867615, b296c98f-b69f-4a2b-b5d6-45d4896b01d3, b2e2b955-3b5e-49c2-9221-759e5672a983, b3434e47-8a1a-48ae-901b-a579b895e2cb, b3ad24b8-45af-4448-be7c-5fcf3cf69568, b52a5c25-8245-40c0-816d-615c47b144ba, b9222003-49c5-45e7-a1d7-7e00538fead6, ba16265c-1e97-4a67-8cd0-1b7ae364ad82, bb44e8b6-993b-4431-a83f-2587c09aba43]
143608 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 14 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet
143612 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet
143612 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 14 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_0_001.parquet => [c271e0cd-4752-4c04-86b1-4ae741ceaf96, c531ef10-36a2-4074-823c-2c84a67709e6, d4cfa584-8149-4daf-b9de-f99b24e27154, d6c0cc0f-499d-4e34-9066-63103f3d7850, d969f5de-bc2e-4247-a27f-07b50684b53c, dce837f4-ff91-4f60-915c-3e69333f98cc, e2daad36-e16a-4cd3-b00b-c0ad8592a8d2, e625894c-e044-434a-b1d0-c09e0dae5a3d, e673b7c3-43ca-4280-b9e0-4e31d244e8a4, e9fe1ec6-507e-4132-9444-d0ade536ea5c, eccdf89d-8a6e-49c9-a0b4-107f5905ab7e, edeffbe4-ad65-4fa8-a0c0-4476f3a629db, f371f2fb-c749-4290-83b6-7fb81d50c4ae, fbdf8fbc-87ae-4879-b02a-4b25bf00080d]
143623 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_2_001.parquet
143640 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_2_001.parquet
143640 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_2_001.parquet => [0536b3d3-3798-40b1-8940-e6b7b108d1b0, 0714a299-b228-41df-bc21-a2005ac89131, 088f3286-fcd6-436b-aec6-93c1c3913bba, 0bc4ae80-8dc4-4123-8f25-fb03e9d4387a, 14e61d06-ce44-4f1d-a5b2-346fe9325032, 1906adc3-faa5-44de-969b-ad3f47c585b6, 194c7822-65b0-45a7-ae79-8716814199a9, 19eb3ae8-efe4-4ff2-a93a-5dbe268297a6, 1b84f87b-1c3b-4806-bb13-23ab96691520, 1c912821-bd18-4600-aa2d-1e5af171b65d, 1eab7288-2423-42df-b9b6-1f9926388124, 20afb486-ab7b-45b4-b06c-12b31cff6c94, 225605da-66f6-4d2d-a74a-f62758cb5e66, 23515a4a-2e10-4958-903c-c87e7f774d31, 26a8f4ec-070f-40e1-9c9a-1bb94e62f82c, 2782cf34-edf5-4d2b-8bd1-f4a936704efb, 2eb66d7c-dcd6-4788-8aee-373e91398171, 2ec3265b-63c0-4fb3-a0eb-e8c59f293234, 337838f0-f916-4ac8-a893-5e71a268a559, 33f0b0b0-d47e-4d96-a0e7-9c530a8c2894, 3524b0af-761e-48d3-b58e-8f6c741f09f9, 36a63c9c-390b-4b90-838c-769ebcdbc95d, 384c7aa4-7069-4819-9bc0-a6ea7da0501d, 385e6ed4-1c10-4813-b05f-fc3a73abdbd5, 46ce0fcd-bb83-4d3c-9950-a4afe487f452, 49099c13-b04f-43ea-8a1c-65fab85f2f2c, 4a0e10c9-54e4-4d2f-a5eb-cd9e5179714a, 4d5b3466-ca5d-4f1f-b269-3d96475199bb, 501f66a2-db03-49d4-ad8b-83a72fc7c0b6, 51da7925-51ba-47e4-b6eb-1d0f029fe0f8, 5f16ee62-ff48-4daa-b7bb-1b694b82ed23, 6335f440-8641-4cce-8715-bb3092f2d678, 665e4580-5f0e-4d46-970f-f9d2c212196f, 6d7e809a-b7f6-4cfa-87ce-7cb9e8d768f9, 79bb5c20-2895-4964-9ed8-3ff50be3fb91, 7d888d72-6198-480a-be0a-42a54b7e88c3, 7fb98097-2d7b-4227-a558-4ff9548c3d7c, 81ad4961-1683-4d70-ab7c-0304e4acce8a, 8616abb8-047d-4687-ac5c-77a5498fb341, 877cd12a-5690-46b6-a14c-6598eec6b7f4, 89bde847-79aa-419c-9bde-f79646618251, 8b68ab9e-1e76-4349-b876-5e9c9170cd9b, 8f0517ba-4463-464b-b131-b9fd23653de8, 903a3d3f-74e2-424f-a3c6-0dcfe26f8880, 90d4fc9a-5bf6-465a-bb4c-48d6f0f14b31, 9356cec0-bf5e-48fd-8c3e-83cd4bf4df4f, 9d0997d4-c985-42e4-aa4c-04446d50d580, 9d38833a-5890-4425-bcf8-f86035ad3f8c, a34b7ae2-535c-492c-b873-beefbc8d36d3, b9be42e5-7bfd-4a54-b1a9-687a85b26147, bc2dd965-51cf-4507-9463-8ccfe5286acd, c213309f-6483-4ad3-94f9-d01f78afbc9b, c4041cd6-aa9d-47c1-9a98-c3e50a079070, c46d3f84-5c7b-44af-9f35-820a9a98a344, c51b50b8-dd86-4947-a39b-3b79133fd5e7, c8351783-c9d4-4792-ba0c-776c8fe1c248, cebae470-ddf9-4c08-a3cf-af53cb5f1468, dbcc940a-9658-45e0-b53b-142e7d34638a, dcb1184e-3e3e-4f2e-8c4d-85005598331a, dd79e220-a483-4657-ad17-5381a70921ba, dd87947a-5edb-4090-9238-6f94e164db03, e17ecc09-f24a-4b91-b493-8bc88d37d5b1, e4a2c6ac-099c-4c68-a5cb-936c1a2d45ad, e5261983-a27b-4c70-b116-f0161307791d, e6d4d9c0-f8ce-4dc9-a96d-473700d7dcfe, e9b5a410-7d4c-45c0-83f6-ae210b770eb1, ea352175-0762-4cfc-855b-77d73ba3e698, eba57c7f-2f67-47a9-bace-07c8cd1e116e, ecd21f36-718a-40d1-b571-8443025855ac, f0268552-f9f0-409f-84b5-9e449162d80b, f1906f37-b54d-4d55-b225-0b8f58244a7e, f3007575-c713-468a-8ddf-cbd030353397, f5a934d6-5157-4b10-8020-621aac292d37, f5e4f332-9ed0-4eac-8c38-bd3a76de2f55, fe6e4c0a-3e26-4df9-9b8a-467bb8ac5d3e]
143719 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=60}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=75}}}
143742 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
143742 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b8d56dcf-2d4a-482f-b885-0dcd4d13b95c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=08b301c8-a376-46eb-9e58-e082a903c64d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede=0, b8d56dcf-2d4a-482f-b885-0dcd4d13b95c=1, 08b301c8-a376-46eb-9e58-e082a903c64d=2}
143754 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
143754 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
144513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
144513 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
144656 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
144656 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
144761 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
144791 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
144791 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
144870 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepointing latest commit 002
145096 [Executor task launch worker-2] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2016/03/15
145101 [Executor task launch worker-2] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/16
145102 [Executor task launch worker-2] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/17
145115 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 created
145116 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
145363 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
145363 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
145546 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_002.parquet
145551 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_002.parquet
145551 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_002.parquet => [00f02836-3dc7-4d71-b98e-351b93b59879, 0451e8f9-9372-4aec-a358-8ce9ae747ada, 05a65fd8-214f-4840-8d2d-0aacf3b1ab91, 0cc65acb-e36b-4182-916d-0ac73bf9360f, 10e4d975-bcf1-4e6f-aec9-731d2ee69909, 15299105-c88d-4854-89d4-e672d1586ce4, 1c4d9a4f-a1c0-4bef-b5bb-f80686e5e341, 1de4af1f-a050-4b4f-9163-2a9a47c89eae, 1ded34de-65be-42f7-9379-30fe752dee45, 1e1bc8cb-29d3-4e0c-96df-0ebebe1fc042, 1e4ed7f8-949b-453b-868e-3225887296bb, 1e5ed6ce-5801-4b62-9fde-1558e5a5fe73, 2102cf7a-b49e-4006-a32c-6909dc4709df, 211e0346-c77e-44c8-a941-ce18bbc28695, 2216f5ae-b0aa-4ac5-bd5f-1205f3ff302b, 25c07457-53b1-44b5-a55c-40fdef73f059, 2817226c-5b9e-49ca-8f81-9f277e46e25e, 307e3cd0-e61c-4854-a0a8-aff290ecf8f9, 343cc1c9-07bd-47e1-8678-02edbb447536, 3ba2668c-e526-4665-8764-9b1b7b4bdbf4, 3bab7fd5-042d-4b37-baf2-9eb17a1102fd, 481b177f-498b-4ee5-9eb4-d58b2ed21ed2, 4c4ff3d8-65ab-41c7-acf4-078746c3b863, 4cc9eddd-bc5f-4b49-b229-63352106fe49, 51139b62-7f44-4961-a3a3-0c2db033ff94, 55fd07f8-ab2b-449e-b036-3ccba41f4df3, 5abe43e1-8a1e-497f-86fe-a4b345ec1a29, 5b57dd54-b64b-4ed4-bf5e-199fab7214d5, 5e9a2b89-d5e7-427e-ad85-1b62cbff7d55, 60ec2ea4-b752-4fd5-8c54-fbafe620c1b8, 635fc87c-8e11-4a05-a592-3976f9ab58c8, 6eaa21df-97be-4053-84e1-ac53fa966ded, 73eca9de-c817-44ce-b0e6-1dcaeee2ff86, 772a0458-1eb5-45a7-9bab-c6eec319e7e1, 7e4414a7-aa24-4be5-966b-d1ca3505f078, 8257252b-d2f8-4371-bbd7-07991f060089, 86b34e68-5d68-47d9-a51f-c6f86b56b606, 8f107cf6-8d9e-44c2-82d9-f6581516292d, 8f60bcf3-bb72-418f-9732-742e2d929708, 91643813-45fc-4d3a-b5f8-a7b228e51827, 96ba2f91-0d47-4bca-b6a0-704f2103a758, 99c37260-b5f1-4a01-8fd3-4139f92fc700, a1f03c68-d8ae-44cd-8e28-a87de37a7d4b, a3236aba-c547-48bd-8ad6-fe6edb53aa9b, a5bbb418-e409-406b-bb89-6349b4796ab5, ad23f84d-344f-4bdf-adf4-4898aca3a376, ae81b315-f165-4c5b-a97a-ff3954122ea4, b4870fd4-6973-4d4a-bfdb-576054822bc1, b49d0edf-b18e-45c1-9ce1-1be17cc4a21e, b6121340-8e09-4b5b-9b65-d6612c0a15a4, bfdd2ab7-0be5-4a6e-9f29-5edfa2c2428f, cc517d1c-91e6-4d59-975f-4dccd4d2609f, cdd0f434-4307-4eef-ad95-3388160f6b86, d350d657-2553-46b9-8ac8-648c69282500, db350c28-b43c-4782-a6d3-12ea5983bade, f06abb19-26b6-425d-8b61-6d985b21f8bd, f333c2d5-2175-4750-b5d3-ffad2c9571ae, f68c59e1-d753-4f7a-888a-67b23ea3150f, fc2cb7d9-1f76-412d-865d-1ca4e9260a11, fd78e21b-c55b-4a3c-937b-2f8f153cbccc]
145580 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 45 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet
145592 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet
145592 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 45 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet => [0314b458-dcb3-4db7-b7aa-aaef6f652dc3, 0a81e566-b002-469b-ade2-6fee8393bf9a, 1738514c-0e4f-444f-8549-72f903ad91e8, 230d99a0-adc1-4450-828d-272863ab52f2, 24f71b65-c92c-4a96-8392-1b55d8296c28, 26aaef4a-1522-4106-bf92-639aa6e6955a, 282cf8e0-0527-4ae4-b02c-54645efa9c8d, 2e9e2d53-1bdf-4b0e-bb53-f0836dc75718, 2fb2f682-99e3-4787-a2b4-03af8eb2370d, 30e6202e-045a-4ea4-a7b7-82e553bce1e8, 3246e2c7-ab7f-43c0-b14e-33f077f82499, 32920b8b-9263-49f5-8210-0f904140833d, 3a46e68d-acec-45e0-99b4-beb4eb38cbe4, 4325caef-7d59-47da-8fc3-f74d0bd003ff, 43ccdc3c-e205-4f15-821c-4035278baf43, 55ee4132-e188-40e3-be5d-13f3d764abef, 5716ae83-93d2-4170-aad5-e523829b4132, 594de09d-4490-4390-a50f-b4583789817a, 62ef3de3-fd77-4837-9799-1e6fdb31345b, 67c88b04-1f68-4941-b508-6679219f4835, 68fb8cc7-cf94-435d-adaa-41c927df3512, 690f5c07-b934-4867-bf58-b7517d332dea, 6e062c55-269a-40dc-b65b-3ff236cc2304, 7353467d-7ef2-4837-92ea-e344a00d3ca0, 793325a4-ff2e-4bf2-8326-ca3a5434c7f8, 7d9c900c-51a4-4dd2-8224-1e33d0474674, 8093d052-15a4-45fa-85ef-ce5b7a168515, 83521641-671d-4039-968f-6e0ac8b7df0f, 8373e2df-303b-4025-b596-85ce7910c6c1, 845cc75a-4819-43b2-8c2a-b47360c6cbf3, 883ca5b7-fffc-4645-afb8-93a993dbcbf1, 8a68c17b-0109-42a0-aff1-de75efcf2bf4, 8c968a54-9e66-4d2e-bebd-bebce0015df9, 9ee4337d-e690-4507-b11e-4b82b68d1135, 9f2937f5-ecae-4667-9d66-28e66eecafac, 9fc45588-dd8a-402b-8292-9ddaa61e165d, a4ac9a41-1c9f-47d6-97b2-bbac65a0be8c, a5908620-9a0b-4a51-9cd1-38a52d54b509, a609a744-f1c8-46ec-b7b1-ab25643720f4, abd58ce9-c682-465b-bf22-cdb04280a9e8, ac4735a7-b65a-41a2-bd75-be4bd399fe28, ac5048dc-88c9-4450-a438-a4541cd86463, aea64ce3-ab9a-4448-b00b-0dfa90867615, b296c98f-b69f-4a2b-b5d6-45d4896b01d3, b2e2b955-3b5e-49c2-9221-759e5672a983]
145622 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet
145626 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet
145626 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_002.parquet => [b3434e47-8a1a-48ae-901b-a579b895e2cb, b3ad24b8-45af-4448-be7c-5fcf3cf69568, b52a5c25-8245-40c0-816d-615c47b144ba, b9222003-49c5-45e7-a1d7-7e00538fead6, ba16265c-1e97-4a67-8cd0-1b7ae364ad82, bb44e8b6-993b-4431-a83f-2587c09aba43, c271e0cd-4752-4c04-86b1-4ae741ceaf96, c531ef10-36a2-4074-823c-2c84a67709e6, d4cfa584-8149-4daf-b9de-f99b24e27154, d6c0cc0f-499d-4e34-9066-63103f3d7850, d969f5de-bc2e-4247-a27f-07b50684b53c, dce837f4-ff91-4f60-915c-3e69333f98cc, e2daad36-e16a-4cd3-b00b-c0ad8592a8d2, e625894c-e044-434a-b1d0-c09e0dae5a3d, e673b7c3-43ca-4280-b9e0-4e31d244e8a4, e9fe1ec6-507e-4132-9444-d0ade536ea5c, eccdf89d-8a6e-49c9-a0b4-107f5905ab7e, edeffbe4-ad65-4fa8-a0c0-4476f3a629db, f371f2fb-c749-4290-83b6-7fb81d50c4ae, fbdf8fbc-87ae-4879-b02a-4b25bf00080d]
145637 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_002.parquet
145641 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_002.parquet
145641 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_002.parquet => [0536b3d3-3798-40b1-8940-e6b7b108d1b0, 0714a299-b228-41df-bc21-a2005ac89131, 088f3286-fcd6-436b-aec6-93c1c3913bba, 0bc4ae80-8dc4-4123-8f25-fb03e9d4387a, 14e61d06-ce44-4f1d-a5b2-346fe9325032, 1906adc3-faa5-44de-969b-ad3f47c585b6, 194c7822-65b0-45a7-ae79-8716814199a9, 19eb3ae8-efe4-4ff2-a93a-5dbe268297a6, 1b84f87b-1c3b-4806-bb13-23ab96691520, 1c912821-bd18-4600-aa2d-1e5af171b65d, 1eab7288-2423-42df-b9b6-1f9926388124, 20afb486-ab7b-45b4-b06c-12b31cff6c94, 225605da-66f6-4d2d-a74a-f62758cb5e66, 23515a4a-2e10-4958-903c-c87e7f774d31, 26a8f4ec-070f-40e1-9c9a-1bb94e62f82c, 2782cf34-edf5-4d2b-8bd1-f4a936704efb, 2eb66d7c-dcd6-4788-8aee-373e91398171, 2ec3265b-63c0-4fb3-a0eb-e8c59f293234, 337838f0-f916-4ac8-a893-5e71a268a559, 33f0b0b0-d47e-4d96-a0e7-9c530a8c2894, 3524b0af-761e-48d3-b58e-8f6c741f09f9, 36a63c9c-390b-4b90-838c-769ebcdbc95d, 384c7aa4-7069-4819-9bc0-a6ea7da0501d, 385e6ed4-1c10-4813-b05f-fc3a73abdbd5, 46ce0fcd-bb83-4d3c-9950-a4afe487f452, 49099c13-b04f-43ea-8a1c-65fab85f2f2c, 4a0e10c9-54e4-4d2f-a5eb-cd9e5179714a, 4d5b3466-ca5d-4f1f-b269-3d96475199bb, 501f66a2-db03-49d4-ad8b-83a72fc7c0b6, 51da7925-51ba-47e4-b6eb-1d0f029fe0f8, 5f16ee62-ff48-4daa-b7bb-1b694b82ed23, 6335f440-8641-4cce-8715-bb3092f2d678, 665e4580-5f0e-4d46-970f-f9d2c212196f, 6d7e809a-b7f6-4cfa-87ce-7cb9e8d768f9, 79bb5c20-2895-4964-9ed8-3ff50be3fb91, 7d888d72-6198-480a-be0a-42a54b7e88c3, 7fb98097-2d7b-4227-a558-4ff9548c3d7c, 81ad4961-1683-4d70-ab7c-0304e4acce8a, 8616abb8-047d-4687-ac5c-77a5498fb341, 877cd12a-5690-46b6-a14c-6598eec6b7f4, 89bde847-79aa-419c-9bde-f79646618251, 8b68ab9e-1e76-4349-b876-5e9c9170cd9b, 8f0517ba-4463-464b-b131-b9fd23653de8, 903a3d3f-74e2-424f-a3c6-0dcfe26f8880, 90d4fc9a-5bf6-465a-bb4c-48d6f0f14b31, 9356cec0-bf5e-48fd-8c3e-83cd4bf4df4f, 9d0997d4-c985-42e4-aa4c-04446d50d580, 9d38833a-5890-4425-bcf8-f86035ad3f8c, a34b7ae2-535c-492c-b873-beefbc8d36d3, b9be42e5-7bfd-4a54-b1a9-687a85b26147, bc2dd965-51cf-4507-9463-8ccfe5286acd, c213309f-6483-4ad3-94f9-d01f78afbc9b, c4041cd6-aa9d-47c1-9a98-c3e50a079070, c46d3f84-5c7b-44af-9f35-820a9a98a344, c51b50b8-dd86-4947-a39b-3b79133fd5e7, c8351783-c9d4-4792-ba0c-776c8fe1c248, cebae470-ddf9-4c08-a3cf-af53cb5f1468, dbcc940a-9658-45e0-b53b-142e7d34638a, dcb1184e-3e3e-4f2e-8c4d-85005598331a, dd79e220-a483-4657-ad17-5381a70921ba, dd87947a-5edb-4090-9238-6f94e164db03, e17ecc09-f24a-4b91-b493-8bc88d37d5b1, e4a2c6ac-099c-4c68-a5cb-936c1a2d45ad, e5261983-a27b-4c70-b116-f0161307791d, e6d4d9c0-f8ce-4dc9-a96d-473700d7dcfe, e9b5a410-7d4c-45c0-83f6-ae210b770eb1, ea352175-0762-4cfc-855b-77d73ba3e698, eba57c7f-2f67-47a9-bace-07c8cd1e116e, ecd21f36-718a-40d1-b571-8443025855ac, f0268552-f9f0-409f-84b5-9e449162d80b, f1906f37-b54d-4d55-b225-0b8f58244a7e, f3007575-c713-468a-8ddf-cbd030353397, f5a934d6-5157-4b10-8020-621aac292d37, f5e4f332-9ed0-4eac-8c38-bd3a76de2f55, fe6e4c0a-3e26-4df9-9b8a-467bb8ac5d3e]
145723 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=60}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=75}}}
145738 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
145738 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b8d56dcf-2d4a-482f-b885-0dcd4d13b95c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=08b301c8-a376-46eb-9e58-e082a903c64d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede=0, b8d56dcf-2d4a-482f-b885-0dcd4d13b95c=1, 08b301c8-a376-46eb-9e58-e082a903c64d=2}
145747 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
145747 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
146391 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
146391 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
146493 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
146493 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
146657 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
146725 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
146725 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
146769 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
147120 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
147120 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
147291 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet
147321 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet
147321 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet => [00f02836-3dc7-4d71-b98e-351b93b59879, 0451e8f9-9372-4aec-a358-8ce9ae747ada, 05a65fd8-214f-4840-8d2d-0aacf3b1ab91, 0cc65acb-e36b-4182-916d-0ac73bf9360f, 10e4d975-bcf1-4e6f-aec9-731d2ee69909, 15299105-c88d-4854-89d4-e672d1586ce4, 1c4d9a4f-a1c0-4bef-b5bb-f80686e5e341, 1de4af1f-a050-4b4f-9163-2a9a47c89eae, 1ded34de-65be-42f7-9379-30fe752dee45, 1e1bc8cb-29d3-4e0c-96df-0ebebe1fc042, 1e4ed7f8-949b-453b-868e-3225887296bb, 1e5ed6ce-5801-4b62-9fde-1558e5a5fe73, 2102cf7a-b49e-4006-a32c-6909dc4709df, 211e0346-c77e-44c8-a941-ce18bbc28695, 2216f5ae-b0aa-4ac5-bd5f-1205f3ff302b, 25c07457-53b1-44b5-a55c-40fdef73f059, 2817226c-5b9e-49ca-8f81-9f277e46e25e, 307e3cd0-e61c-4854-a0a8-aff290ecf8f9, 343cc1c9-07bd-47e1-8678-02edbb447536, 3ba2668c-e526-4665-8764-9b1b7b4bdbf4, 3bab7fd5-042d-4b37-baf2-9eb17a1102fd, 481b177f-498b-4ee5-9eb4-d58b2ed21ed2, 4c4ff3d8-65ab-41c7-acf4-078746c3b863, 4cc9eddd-bc5f-4b49-b229-63352106fe49, 51139b62-7f44-4961-a3a3-0c2db033ff94, 55fd07f8-ab2b-449e-b036-3ccba41f4df3, 5abe43e1-8a1e-497f-86fe-a4b345ec1a29, 5b57dd54-b64b-4ed4-bf5e-199fab7214d5, 5e9a2b89-d5e7-427e-ad85-1b62cbff7d55, 60ec2ea4-b752-4fd5-8c54-fbafe620c1b8, 635fc87c-8e11-4a05-a592-3976f9ab58c8, 6eaa21df-97be-4053-84e1-ac53fa966ded, 73eca9de-c817-44ce-b0e6-1dcaeee2ff86, 772a0458-1eb5-45a7-9bab-c6eec319e7e1, 7e4414a7-aa24-4be5-966b-d1ca3505f078, 8257252b-d2f8-4371-bbd7-07991f060089, 86b34e68-5d68-47d9-a51f-c6f86b56b606, 8f107cf6-8d9e-44c2-82d9-f6581516292d, 8f60bcf3-bb72-418f-9732-742e2d929708, 91643813-45fc-4d3a-b5f8-a7b228e51827, 96ba2f91-0d47-4bca-b6a0-704f2103a758, 99c37260-b5f1-4a01-8fd3-4139f92fc700, a1f03c68-d8ae-44cd-8e28-a87de37a7d4b, a3236aba-c547-48bd-8ad6-fe6edb53aa9b, a5bbb418-e409-406b-bb89-6349b4796ab5, ad23f84d-344f-4bdf-adf4-4898aca3a376, ae81b315-f165-4c5b-a97a-ff3954122ea4, b4870fd4-6973-4d4a-bfdb-576054822bc1, b49d0edf-b18e-45c1-9ce1-1be17cc4a21e, b6121340-8e09-4b5b-9b65-d6612c0a15a4, bfdd2ab7-0be5-4a6e-9f29-5edfa2c2428f, cc517d1c-91e6-4d59-975f-4dccd4d2609f, cdd0f434-4307-4eef-ad95-3388160f6b86, d350d657-2553-46b9-8ac8-648c69282500, db350c28-b43c-4782-a6d3-12ea5983bade, f06abb19-26b6-425d-8b61-6d985b21f8bd, f333c2d5-2175-4750-b5d3-ffad2c9571ae, f68c59e1-d753-4f7a-888a-67b23ea3150f, fc2cb7d9-1f76-412d-865d-1ca4e9260a11, fd78e21b-c55b-4a3c-937b-2f8f153cbccc]
147332 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 43 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
147350 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
147350 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 43 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet => [0314b458-dcb3-4db7-b7aa-aaef6f652dc3, 0a81e566-b002-469b-ade2-6fee8393bf9a, 1738514c-0e4f-444f-8549-72f903ad91e8, 230d99a0-adc1-4450-828d-272863ab52f2, 24f71b65-c92c-4a96-8392-1b55d8296c28, 26aaef4a-1522-4106-bf92-639aa6e6955a, 282cf8e0-0527-4ae4-b02c-54645efa9c8d, 2e9e2d53-1bdf-4b0e-bb53-f0836dc75718, 2fb2f682-99e3-4787-a2b4-03af8eb2370d, 30e6202e-045a-4ea4-a7b7-82e553bce1e8, 3246e2c7-ab7f-43c0-b14e-33f077f82499, 32920b8b-9263-49f5-8210-0f904140833d, 3a46e68d-acec-45e0-99b4-beb4eb38cbe4, 4325caef-7d59-47da-8fc3-f74d0bd003ff, 43ccdc3c-e205-4f15-821c-4035278baf43, 55ee4132-e188-40e3-be5d-13f3d764abef, 5716ae83-93d2-4170-aad5-e523829b4132, 594de09d-4490-4390-a50f-b4583789817a, 62ef3de3-fd77-4837-9799-1e6fdb31345b, 67c88b04-1f68-4941-b508-6679219f4835, 68fb8cc7-cf94-435d-adaa-41c927df3512, 690f5c07-b934-4867-bf58-b7517d332dea, 6e062c55-269a-40dc-b65b-3ff236cc2304, 7353467d-7ef2-4837-92ea-e344a00d3ca0, 793325a4-ff2e-4bf2-8326-ca3a5434c7f8, 7d9c900c-51a4-4dd2-8224-1e33d0474674, 8093d052-15a4-45fa-85ef-ce5b7a168515, 83521641-671d-4039-968f-6e0ac8b7df0f, 8373e2df-303b-4025-b596-85ce7910c6c1, 845cc75a-4819-43b2-8c2a-b47360c6cbf3, 883ca5b7-fffc-4645-afb8-93a993dbcbf1, 8a68c17b-0109-42a0-aff1-de75efcf2bf4, 8c968a54-9e66-4d2e-bebd-bebce0015df9, 9ee4337d-e690-4507-b11e-4b82b68d1135, 9f2937f5-ecae-4667-9d66-28e66eecafac, 9fc45588-dd8a-402b-8292-9ddaa61e165d, a4ac9a41-1c9f-47d6-97b2-bbac65a0be8c, a5908620-9a0b-4a51-9cd1-38a52d54b509, a609a744-f1c8-46ec-b7b1-ab25643720f4, abd58ce9-c682-465b-bf22-cdb04280a9e8, ac4735a7-b65a-41a2-bd75-be4bd399fe28, ac5048dc-88c9-4450-a438-a4541cd86463, aea64ce3-ab9a-4448-b00b-0dfa90867615]
147385 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
147418 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
147418 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet => [b296c98f-b69f-4a2b-b5d6-45d4896b01d3, b2e2b955-3b5e-49c2-9221-759e5672a983, b3434e47-8a1a-48ae-901b-a579b895e2cb, b3ad24b8-45af-4448-be7c-5fcf3cf69568, b52a5c25-8245-40c0-816d-615c47b144ba, b9222003-49c5-45e7-a1d7-7e00538fead6, ba16265c-1e97-4a67-8cd0-1b7ae364ad82, bb44e8b6-993b-4431-a83f-2587c09aba43, c271e0cd-4752-4c04-86b1-4ae741ceaf96, c531ef10-36a2-4074-823c-2c84a67709e6, d4cfa584-8149-4daf-b9de-f99b24e27154, d6c0cc0f-499d-4e34-9066-63103f3d7850, d969f5de-bc2e-4247-a27f-07b50684b53c, dce837f4-ff91-4f60-915c-3e69333f98cc, e2daad36-e16a-4cd3-b00b-c0ad8592a8d2, e625894c-e044-434a-b1d0-c09e0dae5a3d, e673b7c3-43ca-4280-b9e0-4e31d244e8a4, e9fe1ec6-507e-4132-9444-d0ade536ea5c, eccdf89d-8a6e-49c9-a0b4-107f5905ab7e, edeffbe4-ad65-4fa8-a0c0-4476f3a629db, f371f2fb-c749-4290-83b6-7fb81d50c4ae, fbdf8fbc-87ae-4879-b02a-4b25bf00080d]
147429 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet
147446 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet
147446 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet => [0536b3d3-3798-40b1-8940-e6b7b108d1b0, 0714a299-b228-41df-bc21-a2005ac89131, 088f3286-fcd6-436b-aec6-93c1c3913bba, 0bc4ae80-8dc4-4123-8f25-fb03e9d4387a, 14e61d06-ce44-4f1d-a5b2-346fe9325032, 1906adc3-faa5-44de-969b-ad3f47c585b6, 194c7822-65b0-45a7-ae79-8716814199a9, 19eb3ae8-efe4-4ff2-a93a-5dbe268297a6, 1b84f87b-1c3b-4806-bb13-23ab96691520, 1c912821-bd18-4600-aa2d-1e5af171b65d, 1eab7288-2423-42df-b9b6-1f9926388124, 20afb486-ab7b-45b4-b06c-12b31cff6c94, 225605da-66f6-4d2d-a74a-f62758cb5e66, 23515a4a-2e10-4958-903c-c87e7f774d31, 26a8f4ec-070f-40e1-9c9a-1bb94e62f82c, 2782cf34-edf5-4d2b-8bd1-f4a936704efb, 2eb66d7c-dcd6-4788-8aee-373e91398171, 2ec3265b-63c0-4fb3-a0eb-e8c59f293234, 337838f0-f916-4ac8-a893-5e71a268a559, 33f0b0b0-d47e-4d96-a0e7-9c530a8c2894, 3524b0af-761e-48d3-b58e-8f6c741f09f9, 36a63c9c-390b-4b90-838c-769ebcdbc95d, 384c7aa4-7069-4819-9bc0-a6ea7da0501d, 385e6ed4-1c10-4813-b05f-fc3a73abdbd5, 46ce0fcd-bb83-4d3c-9950-a4afe487f452, 49099c13-b04f-43ea-8a1c-65fab85f2f2c, 4a0e10c9-54e4-4d2f-a5eb-cd9e5179714a, 4d5b3466-ca5d-4f1f-b269-3d96475199bb, 501f66a2-db03-49d4-ad8b-83a72fc7c0b6, 51da7925-51ba-47e4-b6eb-1d0f029fe0f8, 5f16ee62-ff48-4daa-b7bb-1b694b82ed23, 6335f440-8641-4cce-8715-bb3092f2d678, 665e4580-5f0e-4d46-970f-f9d2c212196f, 6d7e809a-b7f6-4cfa-87ce-7cb9e8d768f9, 79bb5c20-2895-4964-9ed8-3ff50be3fb91, 7d888d72-6198-480a-be0a-42a54b7e88c3, 7fb98097-2d7b-4227-a558-4ff9548c3d7c, 81ad4961-1683-4d70-ab7c-0304e4acce8a, 8616abb8-047d-4687-ac5c-77a5498fb341, 877cd12a-5690-46b6-a14c-6598eec6b7f4, 89bde847-79aa-419c-9bde-f79646618251, 8b68ab9e-1e76-4349-b876-5e9c9170cd9b, 8f0517ba-4463-464b-b131-b9fd23653de8, 903a3d3f-74e2-424f-a3c6-0dcfe26f8880, 90d4fc9a-5bf6-465a-bb4c-48d6f0f14b31, 9356cec0-bf5e-48fd-8c3e-83cd4bf4df4f, 9d0997d4-c985-42e4-aa4c-04446d50d580, 9d38833a-5890-4425-bcf8-f86035ad3f8c, a34b7ae2-535c-492c-b873-beefbc8d36d3, b9be42e5-7bfd-4a54-b1a9-687a85b26147, bc2dd965-51cf-4507-9463-8ccfe5286acd, c213309f-6483-4ad3-94f9-d01f78afbc9b, c4041cd6-aa9d-47c1-9a98-c3e50a079070, c46d3f84-5c7b-44af-9f35-820a9a98a344, c51b50b8-dd86-4947-a39b-3b79133fd5e7, c8351783-c9d4-4792-ba0c-776c8fe1c248, cebae470-ddf9-4c08-a3cf-af53cb5f1468, dbcc940a-9658-45e0-b53b-142e7d34638a, dcb1184e-3e3e-4f2e-8c4d-85005598331a, dd79e220-a483-4657-ad17-5381a70921ba, dd87947a-5edb-4090-9238-6f94e164db03, e17ecc09-f24a-4b91-b493-8bc88d37d5b1, e4a2c6ac-099c-4c68-a5cb-936c1a2d45ad, e5261983-a27b-4c70-b116-f0161307791d, e6d4d9c0-f8ce-4dc9-a96d-473700d7dcfe, e9b5a410-7d4c-45c0-83f6-ae210b770eb1, ea352175-0762-4cfc-855b-77d73ba3e698, eba57c7f-2f67-47a9-bace-07c8cd1e116e, ecd21f36-718a-40d1-b571-8443025855ac, f0268552-f9f0-409f-84b5-9e449162d80b, f1906f37-b54d-4d55-b225-0b8f58244a7e, f3007575-c713-468a-8ddf-cbd030353397, f5a934d6-5157-4b10-8020-621aac292d37, f5e4f332-9ed0-4eac-8c38-bd3a76de2f55, fe6e4c0a-3e26-4df9-9b8a-467bb8ac5d3e]
147574 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=60}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=75}}}
147642 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
147642 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b8d56dcf-2d4a-482f-b885-0dcd4d13b95c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=08b301c8-a376-46eb-9e58-e082a903c64d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede=0, b8d56dcf-2d4a-482f-b885-0dcd4d13b95c=1, 08b301c8-a376-46eb-9e58-e082a903c64d=2}
147653 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
147653 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
148220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
148220 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
148338 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
148338 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
148454 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
148470 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
148470 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
148647 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 deleted
148648 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [004]
148649 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [004]
148808 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
148810 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_004.parquet	true
148810 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
148811 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_004.parquet	true
148811 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
148811 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_004.parquet	true
148814 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [004]
148814 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [004]
148822 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [004] rollback is complete
148822 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
148822 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
149073 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
149073 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
149258 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet
149269 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet
149269 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit2596486880955499956/2015/03/16/08b301c8-a376-46eb-9e58-e082a903c64d_2_003.parquet => [00f02836-3dc7-4d71-b98e-351b93b59879, 0451e8f9-9372-4aec-a358-8ce9ae747ada, 05a65fd8-214f-4840-8d2d-0aacf3b1ab91, 0cc65acb-e36b-4182-916d-0ac73bf9360f, 10e4d975-bcf1-4e6f-aec9-731d2ee69909, 15299105-c88d-4854-89d4-e672d1586ce4, 1c4d9a4f-a1c0-4bef-b5bb-f80686e5e341, 1de4af1f-a050-4b4f-9163-2a9a47c89eae, 1ded34de-65be-42f7-9379-30fe752dee45, 1e1bc8cb-29d3-4e0c-96df-0ebebe1fc042, 1e4ed7f8-949b-453b-868e-3225887296bb, 1e5ed6ce-5801-4b62-9fde-1558e5a5fe73, 2102cf7a-b49e-4006-a32c-6909dc4709df, 211e0346-c77e-44c8-a941-ce18bbc28695, 2216f5ae-b0aa-4ac5-bd5f-1205f3ff302b, 25c07457-53b1-44b5-a55c-40fdef73f059, 2817226c-5b9e-49ca-8f81-9f277e46e25e, 307e3cd0-e61c-4854-a0a8-aff290ecf8f9, 343cc1c9-07bd-47e1-8678-02edbb447536, 3ba2668c-e526-4665-8764-9b1b7b4bdbf4, 3bab7fd5-042d-4b37-baf2-9eb17a1102fd, 481b177f-498b-4ee5-9eb4-d58b2ed21ed2, 4c4ff3d8-65ab-41c7-acf4-078746c3b863, 4cc9eddd-bc5f-4b49-b229-63352106fe49, 51139b62-7f44-4961-a3a3-0c2db033ff94, 55fd07f8-ab2b-449e-b036-3ccba41f4df3, 5abe43e1-8a1e-497f-86fe-a4b345ec1a29, 5b57dd54-b64b-4ed4-bf5e-199fab7214d5, 5e9a2b89-d5e7-427e-ad85-1b62cbff7d55, 60ec2ea4-b752-4fd5-8c54-fbafe620c1b8, 635fc87c-8e11-4a05-a592-3976f9ab58c8, 6eaa21df-97be-4053-84e1-ac53fa966ded, 73eca9de-c817-44ce-b0e6-1dcaeee2ff86, 772a0458-1eb5-45a7-9bab-c6eec319e7e1, 7e4414a7-aa24-4be5-966b-d1ca3505f078, 8257252b-d2f8-4371-bbd7-07991f060089, 86b34e68-5d68-47d9-a51f-c6f86b56b606, 8f107cf6-8d9e-44c2-82d9-f6581516292d, 8f60bcf3-bb72-418f-9732-742e2d929708, 91643813-45fc-4d3a-b5f8-a7b228e51827, 96ba2f91-0d47-4bca-b6a0-704f2103a758, 99c37260-b5f1-4a01-8fd3-4139f92fc700, a1f03c68-d8ae-44cd-8e28-a87de37a7d4b, a3236aba-c547-48bd-8ad6-fe6edb53aa9b, a5bbb418-e409-406b-bb89-6349b4796ab5, ad23f84d-344f-4bdf-adf4-4898aca3a376, ae81b315-f165-4c5b-a97a-ff3954122ea4, b4870fd4-6973-4d4a-bfdb-576054822bc1, b49d0edf-b18e-45c1-9ce1-1be17cc4a21e, b6121340-8e09-4b5b-9b65-d6612c0a15a4, bfdd2ab7-0be5-4a6e-9f29-5edfa2c2428f, cc517d1c-91e6-4d59-975f-4dccd4d2609f, cdd0f434-4307-4eef-ad95-3388160f6b86, d350d657-2553-46b9-8ac8-648c69282500, db350c28-b43c-4782-a6d3-12ea5983bade, f06abb19-26b6-425d-8b61-6d985b21f8bd, f333c2d5-2175-4750-b5d3-ffad2c9571ae, f68c59e1-d753-4f7a-888a-67b23ea3150f, fc2cb7d9-1f76-412d-865d-1ca4e9260a11, fd78e21b-c55b-4a3c-937b-2f8f153cbccc]
149284 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 36 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
149288 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
149288 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 36 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet => [0314b458-dcb3-4db7-b7aa-aaef6f652dc3, 0a81e566-b002-469b-ade2-6fee8393bf9a, 1738514c-0e4f-444f-8549-72f903ad91e8, 230d99a0-adc1-4450-828d-272863ab52f2, 24f71b65-c92c-4a96-8392-1b55d8296c28, 26aaef4a-1522-4106-bf92-639aa6e6955a, 282cf8e0-0527-4ae4-b02c-54645efa9c8d, 2e9e2d53-1bdf-4b0e-bb53-f0836dc75718, 2fb2f682-99e3-4787-a2b4-03af8eb2370d, 30e6202e-045a-4ea4-a7b7-82e553bce1e8, 3246e2c7-ab7f-43c0-b14e-33f077f82499, 32920b8b-9263-49f5-8210-0f904140833d, 3a46e68d-acec-45e0-99b4-beb4eb38cbe4, 4325caef-7d59-47da-8fc3-f74d0bd003ff, 43ccdc3c-e205-4f15-821c-4035278baf43, 55ee4132-e188-40e3-be5d-13f3d764abef, 5716ae83-93d2-4170-aad5-e523829b4132, 594de09d-4490-4390-a50f-b4583789817a, 62ef3de3-fd77-4837-9799-1e6fdb31345b, 67c88b04-1f68-4941-b508-6679219f4835, 68fb8cc7-cf94-435d-adaa-41c927df3512, 690f5c07-b934-4867-bf58-b7517d332dea, 6e062c55-269a-40dc-b65b-3ff236cc2304, 7353467d-7ef2-4837-92ea-e344a00d3ca0, 793325a4-ff2e-4bf2-8326-ca3a5434c7f8, 7d9c900c-51a4-4dd2-8224-1e33d0474674, 8093d052-15a4-45fa-85ef-ce5b7a168515, 83521641-671d-4039-968f-6e0ac8b7df0f, 8373e2df-303b-4025-b596-85ce7910c6c1, 845cc75a-4819-43b2-8c2a-b47360c6cbf3, 883ca5b7-fffc-4645-afb8-93a993dbcbf1, 8a68c17b-0109-42a0-aff1-de75efcf2bf4, 8c968a54-9e66-4d2e-bebd-bebce0015df9, 9ee4337d-e690-4507-b11e-4b82b68d1135, 9f2937f5-ecae-4667-9d66-28e66eecafac, 9fc45588-dd8a-402b-8292-9ddaa61e165d]
149315 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
149320 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 65 row keys from /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet
149321 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit2596486880955499956/2016/03/15/b8d56dcf-2d4a-482f-b885-0dcd4d13b95c_1_003.parquet => [a4ac9a41-1c9f-47d6-97b2-bbac65a0be8c, a5908620-9a0b-4a51-9cd1-38a52d54b509, a609a744-f1c8-46ec-b7b1-ab25643720f4, abd58ce9-c682-465b-bf22-cdb04280a9e8, ac4735a7-b65a-41a2-bd75-be4bd399fe28, ac5048dc-88c9-4450-a438-a4541cd86463, aea64ce3-ab9a-4448-b00b-0dfa90867615, b296c98f-b69f-4a2b-b5d6-45d4896b01d3, b2e2b955-3b5e-49c2-9221-759e5672a983, b3434e47-8a1a-48ae-901b-a579b895e2cb, b3ad24b8-45af-4448-be7c-5fcf3cf69568, b52a5c25-8245-40c0-816d-615c47b144ba, b9222003-49c5-45e7-a1d7-7e00538fead6, ba16265c-1e97-4a67-8cd0-1b7ae364ad82, bb44e8b6-993b-4431-a83f-2587c09aba43, c271e0cd-4752-4c04-86b1-4ae741ceaf96, c531ef10-36a2-4074-823c-2c84a67709e6, d4cfa584-8149-4daf-b9de-f99b24e27154, d6c0cc0f-499d-4e34-9066-63103f3d7850, d969f5de-bc2e-4247-a27f-07b50684b53c, dce837f4-ff91-4f60-915c-3e69333f98cc, e2daad36-e16a-4cd3-b00b-c0ad8592a8d2, e625894c-e044-434a-b1d0-c09e0dae5a3d, e673b7c3-43ca-4280-b9e0-4e31d244e8a4, e9fe1ec6-507e-4132-9444-d0ade536ea5c, eccdf89d-8a6e-49c9-a0b4-107f5905ab7e, edeffbe4-ad65-4fa8-a0c0-4476f3a629db, f371f2fb-c749-4290-83b6-7fb81d50c4ae, fbdf8fbc-87ae-4879-b02a-4b25bf00080d]
149331 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet
149337 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet
149337 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit2596486880955499956/2015/03/17/bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede_0_003.parquet => [0536b3d3-3798-40b1-8940-e6b7b108d1b0, 0714a299-b228-41df-bc21-a2005ac89131, 088f3286-fcd6-436b-aec6-93c1c3913bba, 0bc4ae80-8dc4-4123-8f25-fb03e9d4387a, 14e61d06-ce44-4f1d-a5b2-346fe9325032, 1906adc3-faa5-44de-969b-ad3f47c585b6, 194c7822-65b0-45a7-ae79-8716814199a9, 19eb3ae8-efe4-4ff2-a93a-5dbe268297a6, 1b84f87b-1c3b-4806-bb13-23ab96691520, 1c912821-bd18-4600-aa2d-1e5af171b65d, 1eab7288-2423-42df-b9b6-1f9926388124, 20afb486-ab7b-45b4-b06c-12b31cff6c94, 225605da-66f6-4d2d-a74a-f62758cb5e66, 23515a4a-2e10-4958-903c-c87e7f774d31, 26a8f4ec-070f-40e1-9c9a-1bb94e62f82c, 2782cf34-edf5-4d2b-8bd1-f4a936704efb, 2eb66d7c-dcd6-4788-8aee-373e91398171, 2ec3265b-63c0-4fb3-a0eb-e8c59f293234, 337838f0-f916-4ac8-a893-5e71a268a559, 33f0b0b0-d47e-4d96-a0e7-9c530a8c2894, 3524b0af-761e-48d3-b58e-8f6c741f09f9, 36a63c9c-390b-4b90-838c-769ebcdbc95d, 384c7aa4-7069-4819-9bc0-a6ea7da0501d, 385e6ed4-1c10-4813-b05f-fc3a73abdbd5, 46ce0fcd-bb83-4d3c-9950-a4afe487f452, 49099c13-b04f-43ea-8a1c-65fab85f2f2c, 4a0e10c9-54e4-4d2f-a5eb-cd9e5179714a, 4d5b3466-ca5d-4f1f-b269-3d96475199bb, 501f66a2-db03-49d4-ad8b-83a72fc7c0b6, 51da7925-51ba-47e4-b6eb-1d0f029fe0f8, 5f16ee62-ff48-4daa-b7bb-1b694b82ed23, 6335f440-8641-4cce-8715-bb3092f2d678, 665e4580-5f0e-4d46-970f-f9d2c212196f, 6d7e809a-b7f6-4cfa-87ce-7cb9e8d768f9, 79bb5c20-2895-4964-9ed8-3ff50be3fb91, 7d888d72-6198-480a-be0a-42a54b7e88c3, 7fb98097-2d7b-4227-a558-4ff9548c3d7c, 81ad4961-1683-4d70-ab7c-0304e4acce8a, 8616abb8-047d-4687-ac5c-77a5498fb341, 877cd12a-5690-46b6-a14c-6598eec6b7f4, 89bde847-79aa-419c-9bde-f79646618251, 8b68ab9e-1e76-4349-b876-5e9c9170cd9b, 8f0517ba-4463-464b-b131-b9fd23653de8, 903a3d3f-74e2-424f-a3c6-0dcfe26f8880, 90d4fc9a-5bf6-465a-bb4c-48d6f0f14b31, 9356cec0-bf5e-48fd-8c3e-83cd4bf4df4f, 9d0997d4-c985-42e4-aa4c-04446d50d580, 9d38833a-5890-4425-bcf8-f86035ad3f8c, a34b7ae2-535c-492c-b873-beefbc8d36d3, b9be42e5-7bfd-4a54-b1a9-687a85b26147, bc2dd965-51cf-4507-9463-8ccfe5286acd, c213309f-6483-4ad3-94f9-d01f78afbc9b, c4041cd6-aa9d-47c1-9a98-c3e50a079070, c46d3f84-5c7b-44af-9f35-820a9a98a344, c51b50b8-dd86-4947-a39b-3b79133fd5e7, c8351783-c9d4-4792-ba0c-776c8fe1c248, cebae470-ddf9-4c08-a3cf-af53cb5f1468, dbcc940a-9658-45e0-b53b-142e7d34638a, dcb1184e-3e3e-4f2e-8c4d-85005598331a, dd79e220-a483-4657-ad17-5381a70921ba, dd87947a-5edb-4090-9238-6f94e164db03, e17ecc09-f24a-4b91-b493-8bc88d37d5b1, e4a2c6ac-099c-4c68-a5cb-936c1a2d45ad, e5261983-a27b-4c70-b116-f0161307791d, e6d4d9c0-f8ce-4dc9-a96d-473700d7dcfe, e9b5a410-7d4c-45c0-83f6-ae210b770eb1, ea352175-0762-4cfc-855b-77d73ba3e698, eba57c7f-2f67-47a9-bace-07c8cd1e116e, ecd21f36-718a-40d1-b571-8443025855ac, f0268552-f9f0-409f-84b5-9e449162d80b, f1906f37-b54d-4d55-b225-0b8f58244a7e, f3007575-c713-468a-8ddf-cbd030353397, f5a934d6-5157-4b10-8020-621aac292d37, f5e4f332-9ed0-4eac-8c38-bd3a76de2f55, fe6e4c0a-3e26-4df9-9b8a-467bb8ac5d3e]
149467 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=65}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=60}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=75}}}
149489 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
149489 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede}, 1=BucketInfo {bucketType=UPDATE, fileLoc=b8d56dcf-2d4a-482f-b885-0dcd4d13b95c}, 2=BucketInfo {bucketType=UPDATE, fileLoc=08b301c8-a376-46eb-9e58-e082a903c64d}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{bcf6bf3f-b5c0-4e4f-87ab-d98c65ce3ede=0, b8d56dcf-2d4a-482f-b885-0dcd4d13b95c=1, 08b301c8-a376-46eb-9e58-e082a903c64d=2}
149519 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
149519 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
150008 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
150008 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
150133 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
150133 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
150262 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
150272 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
150272 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
150555 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01], with policy KEEP_LATEST_FILE_VERSIONS
150555 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
150738 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
150820 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=72, numUpdates=0}}}
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
150827 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
150836 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
150836 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
150912 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
150912 [pool-1460-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
150936 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
150937 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
150970 [pool-1460-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
150983 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151009 [pool-1461-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151018 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151020 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151058 [pool-1461-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151072 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
151087 [pool-1462-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
151092 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
151092 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
151130 [pool-1462-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
151217 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
151217 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
151306 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
151306 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
151405 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
151413 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
151413 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
152088 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
152088 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
152224 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 64 for /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet
152245 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet
152245 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 64 results, for file /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet => [0075cc91-2b82-4f62-acc2-f2d14862b159, 04057cbf-fe8c-4642-bfe1-e738c4675c40, 04074155-5540-44fb-a49c-f880b7921722, 07d725e7-3c2a-4768-915e-eb90a98ab103, 09b3551a-2714-4910-af17-810cb9fbdeef, 17cb04a8-283c-4732-8756-3b1fd4958fea, 18dafcc2-a443-412b-9fda-a844ac285320, 1a06bb42-6c45-4991-adc9-8f85ecf4da02, 1f9e989f-14c1-420d-bd67-9167030d51e3, 33513a19-bc95-47d2-8249-b5b731758e8f, 346f44dc-19d2-4e65-b636-0ca1f3a45bb1, 3c90a45d-f44f-4a55-8b8a-50dea872ea4f, 3fbd0545-e0ad-445f-9aac-e0145621fdd9, 4497e243-cbdf-495a-a868-054b74a45a19, 44c256f5-88df-44d2-9d20-8782a351ad11, 46df920e-f65c-4f45-8c01-1fbd7e4df165, 4d0f8785-9948-43af-84fa-23d4a3a50372, 528f6e81-0853-4758-81c0-d89975f96e89, 54192570-0eb3-4101-8ac8-67a59eb62006, 558a2bc0-0bae-4d9c-8977-dc867fea63b1, 5707a184-34f1-4b56-b754-dd812aca5bb7, 5a0a3a64-88fe-4578-8167-66f3ec15a006, 6033993b-72e7-4c62-a3cb-5efecd613ed2, 65f1fc97-7f35-4c7a-a299-c07dfcb09503, 684ce940-5db4-4480-9adb-df1ceb13dac7, 6894fa6c-3dcb-4a98-beca-ac4535032284, 73d6898c-f576-4765-a612-582ba61c755e, 74fea0e8-8532-46f1-8f6e-c1e2730ef3f3, 76be3d03-a8fd-482e-aad9-ff13188dad3b, 780eb3c4-645a-4b3d-b206-a94f34310274, 790d7f82-3b32-45d3-917b-d40300fe335b, 8496cb9d-1a04-4b1f-a740-241733554e0b, 859ee738-9199-42a5-a3a1-54c287d56d8d, 8e8f8f42-cf92-4dad-8c0a-f11b924a418d, 8fd95ed2-9b2f-4558-b194-9afd6ec1cfe5, 970341ce-3b2d-418e-b6cf-5104f198323d, 9f1a655d-047f-4ba5-a5ec-d4382eaffdb4, a2db7da3-506d-4791-99c5-5d7e10ba8459, ac8c1915-29b9-433d-a536-3d20da3d3ab8, ac9ae99d-e2bf-4ac1-9cec-ba87b41077ab, afaf8d7b-7965-4398-8394-468fdfece0e0, b0148e40-1f02-4177-829c-4f9948204223, b2553e55-9c9b-4429-a141-72bda1a56c52, bbb266e6-7b56-4a2a-a6cb-9fa83c5a9802, be971638-155e-4e4c-9099-c773b29f4f52, c234f22e-5410-4447-9367-a6e1d1c64659, c93936ea-9523-4333-81c9-05475c11ff3a, ccb134ef-03dc-4ce9-ad73-7c16e0581a4c, ce77a17f-bd5d-413e-ad0f-61a27c7aba98, dbd5b1bc-a8a1-4766-b3a6-4275d02abe2a, dc101105-c955-4df2-aa67-e66196b6c56c, e2527ed2-b0eb-4438-bc81-1510345a2bcc, e4804bb2-d400-4364-8178-cefcdb2e060f, e59fc912-603a-43c7-9166-6576c5d1877e, ea758895-39d9-47ff-98c5-c2d11bb71ab7, eaef6e85-5882-43ae-8382-cc7e94b011fc, eb1b60d0-d448-4449-bb38-6d5b81516aa7, eef726e2-dd11-49a4-83c2-aa23ee712fa0, ef3a2d3c-a843-48b3-897b-2af3bee69388, ef3b219d-5c38-400f-8437-c3db9794eaf3, ef95573e-6b96-45ed-a950-2ef9dfbc62d1, fa163665-1d62-4a5d-b704-f0bdfd85cac1, fd829b90-3ab4-446c-b484-4cdaafec90cb, fe41505d-3b1a-4337-9d16-60b333e49b57]
152255 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 72 for /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152266 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152266 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 72 results, for file /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet => [0256470e-0138-4935-aefe-ede8c6e4c693, 060a824e-8c48-4b50-b20a-43700165cc8a, 0cce2ecd-47c4-4a75-9bfe-256a56ecb5f6, 14590cb7-ecfb-415d-8b51-d1ec74c854c7, 1b0718fb-3b1e-4c9b-8090-7b4c1cbb499b, 2163738d-5bb4-4392-a777-e67d1d138755, 2818002e-af7f-4fcb-8f62-0317d58ba0dd, 29a2ece0-769f-43fc-b302-1d44e06d96ff, 29b9da12-94d8-460a-81d7-8ca32e3a5806, 2bbdb4a4-1ed4-43ef-8518-e64e1cba78e5, 2bc82214-154e-449f-a4fa-ca32ae927d13, 2ef784bb-c628-4284-b6dc-fa4bcf6bc2c8, 3515a406-184a-45fe-b256-5fef3cc8ae22, 36571697-41c9-4338-8bc4-3407a1313906, 388d48e9-4e72-480c-8845-c0e82a9f797d, 3a700242-a181-452e-ae3a-1926ae86d9f0, 3ea2518b-fbc3-4922-aabb-d47b6f9cb1ae, 40ed96da-899a-4005-b0ba-234288710179, 40fee225-dcf9-46de-8d39-77880eaca867, 479465d1-0d66-44b7-afef-d4299d34a528, 49c3a4b0-9e4a-4041-9bca-fffb751b353a, 4aacde69-7c14-4c26-84e7-2b360e9a581d, 4b0448ed-1155-454e-9c54-0c12fb513800, 4c3cdb29-05f1-4f90-90de-537f248e75d8, 518a0ed4-e605-4011-a47f-f9c180005376, 51c6fef9-8234-475a-be1d-7f33f3ccf068, 52ff0d7e-3a3a-4c10-9ec7-2e1d68a2e64e, 577333ed-a947-473d-b115-4fee0e55e699, 5ebe10b2-e654-47a0-9fd4-a686fb50747b, 60fa9e58-42ff-48b3-aa3d-4244b8c352ec, 6a9803ce-014c-4014-b8b1-7c2cdf8c6987, 6b2a87fd-3b16-4094-9327-3f11f256c2f8, 6bee2694-b51b-48e0-8c8c-d7c882c66d7a, 73dbd95c-4452-41ed-bc31-7c2c2073fb82, 76fb92e7-dfdd-4d2c-b9dd-bc3680dd2221, 7d4dab14-fd53-4c55-ba74-b2a5fd785ccb, 82e5f137-6cee-404c-9570-dab0525363fc, 835e8e1c-985d-468e-9f5e-c1347dec1506, 8777a073-5013-475b-9c0f-276b50a788c4, 893e06de-d92d-45da-8147-2334351e5a95, 8a9cac7f-4ea5-4605-8fd4-77486cc3b00f, 8b2349e7-5bff-4650-9813-2111ea3bf799, 8bd7ba81-fc46-4e9c-88ba-b35ffd7fa3d2, 914c7f59-8e0b-4c33-8d1d-a66e12d58219, 9bb8f2ad-698b-4af3-9694-e247557cda9f, 9ee986ff-3af9-49f6-8eaf-30ed2fa8a441, a0de1cd4-e337-43a5-a4ff-1106b8464cb9, a1fce8fd-ac36-42d9-a5b7-db0a0a5ba86f, ad8cacce-c9f3-49b8-92a8-0f6fd2b0de25, baf1b9c3-0813-4977-a63d-001e6ae349d7, bd5583ad-ac2e-4685-9bb2-d7f9e1d42fab, bf821694-a98a-479e-936e-2f6a0940b50d, c031b370-bf67-47c2-96ef-7ace5e68d936, c194ef77-7006-4019-985e-a2293b42e139, c66910f6-57ae-4644-92da-02af3ddb471c, ca7c4426-284d-409b-8cb9-00d797cf131e, cb4c173f-3b7a-47da-b858-6d498d8642a6, cc07d7d3-5b53-45a1-b70b-e5eb184427a2, d00ef545-aa68-4d70-81f4-d2964bb77203, d1fceb03-8e3d-4405-89b1-e621263a8cdd, d98cda4e-2797-4832-8bc3-b00a977465e6, dc0150d7-b92d-43ba-a0ae-b41c6d7c17ed, e1a5c506-b1bc-4b0b-8c79-6f28dc599486, e38f7049-7e26-4392-a9f6-59c2f6852713, e4a5606d-2f72-40a3-9515-d5c1f6349d6e, e6bef379-4f73-4475-9970-5c8deae60099, e7067fb7-7e4e-46ec-9664-f642d43c4a45, e89ca9ff-9dae-41e0-9205-46c0ed215089, ed1e4d9f-7edb-46aa-bab9-638aff39e7c9, eea8fdb8-036d-4302-b7a1-f561e7a45356, fac2ab0b-7c2e-4a57-975e-df9dc118b8c9, fda46283-e247-4a18-be0a-193a7cf39529]
152279 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 64 for /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet
152310 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet
152310 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 64 results, for file /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet => [06c6bbf6-e272-4cca-b0de-6804bd2c6c93, 0b078063-b6ae-4a96-8e86-5b62210f31fd, 0be59011-ef68-4c85-9372-b5b3db6a7e4c, 1351f4e5-d977-4e39-a4fb-9b7ab34325e6, 14e0bde8-9ad0-4212-938f-7c9d1faa0c72, 17fc0ebc-5faa-4da2-b1ba-0f8123e7ea06, 1c273ae7-6373-46fd-a98d-30e1949553fe, 1c6c580d-f0ca-4efb-93ce-4dc701135fa1, 1e226ea2-1b2c-43e1-bddb-4d9205440653, 245952d9-5751-4d64-92e8-625280dec28b, 2daf21eb-145e-491f-ab3a-ec104677c1b4, 32d79e63-3ae6-4e0f-834e-145771319ea3, 33fafdef-bae7-45f0-9f4f-103400d0f936, 34c50de8-c33e-445d-ba0f-d780cdb79083, 353b3e91-54ac-458c-904b-a682fbf202d3, 353fa3b9-2810-4d4d-8d1b-888040befb4b, 36fd8277-1e6e-43f5-9ab5-12aab40e78a1, 37b21817-3d7c-4682-84bb-75315e5a0277, 3f23fc67-df4f-41a6-bd4a-7bfdffd32148, 404d7d42-6d67-4139-ab78-ae9994ac5ece, 41cb14a5-4c27-41c0-9c84-7ed92486e7a6, 46785b4d-91d6-429a-ba6f-538dbfe21684, 52ee97eb-598b-41c1-9a79-5cd53cc63cda, 53284399-dac1-4c56-8ef4-8a6d7b912eef, 57f9736d-90e2-43ed-8806-8d80822b0269, 5b990450-642c-4928-9d9c-adf245afcdd1, 5b9a9f2a-36d3-4e4e-a29b-ed9d94524bcc, 64cd6b94-5803-4f23-9413-21078f4fead0, 67e29ae0-578e-410e-9893-323b8a145fde, 7a917e34-f582-4bbb-b1ca-b1efa0108010, 7e70c5f2-9c8f-461c-95e5-9ebca9d191a8, 812839a9-eb97-4b39-a709-3e64ebe265f7, 830862ce-b0ab-4fa0-b6e2-c16e0f8c9636, 891f92a5-cb92-4f7f-9be0-dbb1f107e6f9, 8be9ee3d-7d13-4336-99b9-3d587e8f1a84, 8bec5765-980a-4503-9deb-4f5c5c4d109b, 94a04f95-911f-4fe4-86de-751cb8ee5b1f, 9b9367ef-f89b-46cf-9709-3bd17af0ab65, 9ead1f47-73b3-457a-9091-e5abde7e1499, a846ed2b-1f89-4f7d-a471-2c2617b96be8, aba3e5db-081e-4a56-8a72-3acba5d1fdad, aed573b9-619f-4e6e-a794-1ae6bec9c4d3, af44a1be-c364-4bab-bbe9-e91ab1a683c4, b0f7bd4c-08e5-4ef1-8fb3-8ea88c579011, b74b43d3-f5a4-4932-b2ba-8010b1207513, ba52237b-15a0-402e-aa6c-5a7728b60fe5, bc65f3b5-5a81-43ec-8761-071439bb3605, be0084b9-27ca-4cda-b284-a02c48bdaa4f, c2e0f68e-9586-4ac0-b803-b73d627fb501, c3ec0b81-9589-4fda-b8f2-38c799c697e6, c5ba3fb7-e7ba-4de8-8515-e076f15066b1, d0caa8fd-28d1-4fd9-bc8b-6869f4b19db2, d1ed9524-0f19-468a-9500-88fc93b060bc, d26bc852-82fe-45fe-82d2-7c56949b0c14, dcb4759b-e9e1-4ec5-b8c9-dfb14c483e01, de8c8800-c843-4299-84cd-e2531d4d6fae, e00891df-f4cd-4a3d-8d3b-6edd86c7bfe7, e6079fc7-7da6-4fa7-8738-f8bb242f0ebf, eca93e9f-e66b-42e5-b00d-631228dd7512, f64530d4-7f62-4099-b471-6e46be05cc02, f980d3ba-a42e-46ed-a43c-3c475c2d4067, faee354a-deb0-4d34-9908-bcd210723156, fbbbcd7e-d408-4a95-9730-2c5bd0a4a80a, fc7c2990-1102-4485-9b4f-f9b09fcdb06b]
152357 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
152645 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 78
152645 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
152864 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 27 for /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet
152882 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet
152882 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 27 results, for file /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_0_001.parquet => [0075cc91-2b82-4f62-acc2-f2d14862b159, 18dafcc2-a443-412b-9fda-a844ac285320, 33513a19-bc95-47d2-8249-b5b731758e8f, 3fbd0545-e0ad-445f-9aac-e0145621fdd9, 4497e243-cbdf-495a-a868-054b74a45a19, 46df920e-f65c-4f45-8c01-1fbd7e4df165, 54192570-0eb3-4101-8ac8-67a59eb62006, 5707a184-34f1-4b56-b754-dd812aca5bb7, 73d6898c-f576-4765-a612-582ba61c755e, 74fea0e8-8532-46f1-8f6e-c1e2730ef3f3, 780eb3c4-645a-4b3d-b206-a94f34310274, 790d7f82-3b32-45d3-917b-d40300fe335b, 859ee738-9199-42a5-a3a1-54c287d56d8d, 970341ce-3b2d-418e-b6cf-5104f198323d, 9f1a655d-047f-4ba5-a5ec-d4382eaffdb4, a2db7da3-506d-4791-99c5-5d7e10ba8459, afaf8d7b-7965-4398-8394-468fdfece0e0, c234f22e-5410-4447-9367-a6e1d1c64659, c93936ea-9523-4333-81c9-05475c11ff3a, dc101105-c955-4df2-aa67-e66196b6c56c, e2527ed2-b0eb-4438-bc81-1510345a2bcc, eaef6e85-5882-43ae-8382-cc7e94b011fc, eef726e2-dd11-49a4-83c2-aa23ee712fa0, ef3b219d-5c38-400f-8437-c3db9794eaf3, fa163665-1d62-4a5d-b704-f0bdfd85cac1, fd829b90-3ab4-446c-b484-4cdaafec90cb, fe41505d-3b1a-4337-9d16-60b333e49b57]
152893 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152910 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152910 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet => [14590cb7-ecfb-415d-8b51-d1ec74c854c7, 1b0718fb-3b1e-4c9b-8090-7b4c1cbb499b, 2818002e-af7f-4fcb-8f62-0317d58ba0dd, 29a2ece0-769f-43fc-b302-1d44e06d96ff, 2bbdb4a4-1ed4-43ef-8518-e64e1cba78e5, 2bc82214-154e-449f-a4fa-ca32ae927d13, 2ef784bb-c628-4284-b6dc-fa4bcf6bc2c8, 388d48e9-4e72-480c-8845-c0e82a9f797d, 49c3a4b0-9e4a-4041-9bca-fffb751b353a, 518a0ed4-e605-4011-a47f-f9c180005376, 51c6fef9-8234-475a-be1d-7f33f3ccf068, 52ff0d7e-3a3a-4c10-9ec7-2e1d68a2e64e]
152928 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 21 for /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152966 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet
152966 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 21 results, for file /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_2_001.parquet => [577333ed-a947-473d-b115-4fee0e55e699, 6a9803ce-014c-4014-b8b1-7c2cdf8c6987, 76fb92e7-dfdd-4d2c-b9dd-bc3680dd2221, 835e8e1c-985d-468e-9f5e-c1347dec1506, 8777a073-5013-475b-9c0f-276b50a788c4, 8b2349e7-5bff-4650-9813-2111ea3bf799, 914c7f59-8e0b-4c33-8d1d-a66e12d58219, 9bb8f2ad-698b-4af3-9694-e247557cda9f, a0de1cd4-e337-43a5-a4ff-1106b8464cb9, c194ef77-7006-4019-985e-a2293b42e139, c66910f6-57ae-4644-92da-02af3ddb471c, cb4c173f-3b7a-47da-b858-6d498d8642a6, d00ef545-aa68-4d70-81f4-d2964bb77203, dc0150d7-b92d-43ba-a0ae-b41c6d7c17ed, e38f7049-7e26-4392-a9f6-59c2f6852713, e4a5606d-2f72-40a3-9515-d5c1f6349d6e, e7067fb7-7e4e-46ec-9664-f642d43c4a45, ed1e4d9f-7edb-46aa-bab9-638aff39e7c9, eea8fdb8-036d-4302-b7a1-f561e7a45356, fac2ab0b-7c2e-4a57-975e-df9dc118b8c9, fda46283-e247-4a18-be0a-193a7cf39529]
152977 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet
152994 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet
152994 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_1_001.parquet => [1351f4e5-d977-4e39-a4fb-9b7ab34325e6, 14e0bde8-9ad0-4212-938f-7c9d1faa0c72, 245952d9-5751-4d64-92e8-625280dec28b, 2daf21eb-145e-491f-ab3a-ec104677c1b4, 32d79e63-3ae6-4e0f-834e-145771319ea3, 353fa3b9-2810-4d4d-8d1b-888040befb4b, 3f23fc67-df4f-41a6-bd4a-7bfdffd32148, 41cb14a5-4c27-41c0-9c84-7ed92486e7a6, 53284399-dac1-4c56-8ef4-8a6d7b912eef, 7a917e34-f582-4bbb-b1ca-b1efa0108010, 830862ce-b0ab-4fa0-b6e2-c16e0f8c9636, 94a04f95-911f-4fe4-86de-751cb8ee5b1f, 9ead1f47-73b3-457a-9091-e5abde7e1499, a846ed2b-1f89-4f7d-a471-2c2617b96be8, aba3e5db-081e-4a56-8a72-3acba5d1fdad, d1ed9524-0f19-468a-9500-88fc93b060bc, de8c8800-c843-4299-84cd-e2531d4d6fae, fbbbcd7e-d408-4a95-9730-2c5bd0a4a80a]
153047 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=78}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=27}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=18}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=33}}}
153112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
153112 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=1d542d94-1a30-45b0-8053-5ae1014fa220}, 1=BucketInfo {bucketType=UPDATE, fileLoc=1959a931-c8fb-43c5-be3c-249ebd5a717d}, 2=BucketInfo {bucketType=UPDATE, fileLoc=3e548a3b-b1c0-4518-a906-fa7a17310fbc}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{1d542d94-1a30-45b0-8053-5ae1014fa220=0, 1959a931-c8fb-43c5-be3c-249ebd5a717d=1, 3e548a3b-b1c0-4518-a906-fa7a17310fbc=2}
153123 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
153123 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
153465 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
153465 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
153554 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
153554 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
153683 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
153693 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
153693 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
153965 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 78
153965 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
154061 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 27 for /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_1_004.parquet
154108 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_1_004.parquet
154108 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 27 results, for file /tmp/junit7152736167890972792/2016/03/15/1959a931-c8fb-43c5-be3c-249ebd5a717d_1_004.parquet => [0075cc91-2b82-4f62-acc2-f2d14862b159, 18dafcc2-a443-412b-9fda-a844ac285320, 33513a19-bc95-47d2-8249-b5b731758e8f, 3fbd0545-e0ad-445f-9aac-e0145621fdd9, 4497e243-cbdf-495a-a868-054b74a45a19, 46df920e-f65c-4f45-8c01-1fbd7e4df165, 54192570-0eb3-4101-8ac8-67a59eb62006, 5707a184-34f1-4b56-b754-dd812aca5bb7, 73d6898c-f576-4765-a612-582ba61c755e, 74fea0e8-8532-46f1-8f6e-c1e2730ef3f3, 780eb3c4-645a-4b3d-b206-a94f34310274, 790d7f82-3b32-45d3-917b-d40300fe335b, 859ee738-9199-42a5-a3a1-54c287d56d8d, 970341ce-3b2d-418e-b6cf-5104f198323d, 9f1a655d-047f-4ba5-a5ec-d4382eaffdb4, a2db7da3-506d-4791-99c5-5d7e10ba8459, afaf8d7b-7965-4398-8394-468fdfece0e0, c234f22e-5410-4447-9367-a6e1d1c64659, c93936ea-9523-4333-81c9-05475c11ff3a, dc101105-c955-4df2-aa67-e66196b6c56c, e2527ed2-b0eb-4438-bc81-1510345a2bcc, eaef6e85-5882-43ae-8382-cc7e94b011fc, eef726e2-dd11-49a4-83c2-aa23ee712fa0, ef3b219d-5c38-400f-8437-c3db9794eaf3, fa163665-1d62-4a5d-b704-f0bdfd85cac1, fd829b90-3ab4-446c-b484-4cdaafec90cb, fe41505d-3b1a-4337-9d16-60b333e49b57]
154119 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 33 for /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_0_004.parquet
154123 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 72 row keys from /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_0_004.parquet
154123 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 33 results, for file /tmp/junit7152736167890972792/2015/03/17/1d542d94-1a30-45b0-8053-5ae1014fa220_0_004.parquet => [14590cb7-ecfb-415d-8b51-d1ec74c854c7, 1b0718fb-3b1e-4c9b-8090-7b4c1cbb499b, 2818002e-af7f-4fcb-8f62-0317d58ba0dd, 29a2ece0-769f-43fc-b302-1d44e06d96ff, 2bbdb4a4-1ed4-43ef-8518-e64e1cba78e5, 2bc82214-154e-449f-a4fa-ca32ae927d13, 2ef784bb-c628-4284-b6dc-fa4bcf6bc2c8, 388d48e9-4e72-480c-8845-c0e82a9f797d, 49c3a4b0-9e4a-4041-9bca-fffb751b353a, 518a0ed4-e605-4011-a47f-f9c180005376, 51c6fef9-8234-475a-be1d-7f33f3ccf068, 52ff0d7e-3a3a-4c10-9ec7-2e1d68a2e64e, 577333ed-a947-473d-b115-4fee0e55e699, 6a9803ce-014c-4014-b8b1-7c2cdf8c6987, 76fb92e7-dfdd-4d2c-b9dd-bc3680dd2221, 835e8e1c-985d-468e-9f5e-c1347dec1506, 8777a073-5013-475b-9c0f-276b50a788c4, 8b2349e7-5bff-4650-9813-2111ea3bf799, 914c7f59-8e0b-4c33-8d1d-a66e12d58219, 9bb8f2ad-698b-4af3-9694-e247557cda9f, a0de1cd4-e337-43a5-a4ff-1106b8464cb9, c194ef77-7006-4019-985e-a2293b42e139, c66910f6-57ae-4644-92da-02af3ddb471c, cb4c173f-3b7a-47da-b858-6d498d8642a6, d00ef545-aa68-4d70-81f4-d2964bb77203, dc0150d7-b92d-43ba-a0ae-b41c6d7c17ed, e38f7049-7e26-4392-a9f6-59c2f6852713, e4a5606d-2f72-40a3-9515-d5c1f6349d6e, e7067fb7-7e4e-46ec-9664-f642d43c4a45, ed1e4d9f-7edb-46aa-bab9-638aff39e7c9, eea8fdb8-036d-4302-b7a1-f561e7a45356, fac2ab0b-7c2e-4a57-975e-df9dc118b8c9, fda46283-e247-4a18-be0a-193a7cf39529]
154136 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_2_004.parquet
154140 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 64 row keys from /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_2_004.parquet
154140 [Executor task launch worker-3] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit7152736167890972792/2015/03/16/3e548a3b-b1c0-4518-a906-fa7a17310fbc_2_004.parquet => [1351f4e5-d977-4e39-a4fb-9b7ab34325e6, 14e0bde8-9ad0-4212-938f-7c9d1faa0c72, 245952d9-5751-4d64-92e8-625280dec28b, 2daf21eb-145e-491f-ab3a-ec104677c1b4, 32d79e63-3ae6-4e0f-834e-145771319ea3, 353fa3b9-2810-4d4d-8d1b-888040befb4b, 3f23fc67-df4f-41a6-bd4a-7bfdffd32148, 41cb14a5-4c27-41c0-9c84-7ed92486e7a6, 53284399-dac1-4c56-8ef4-8a6d7b912eef, 7a917e34-f582-4bbb-b1ca-b1efa0108010, 830862ce-b0ab-4fa0-b6e2-c16e0f8c9636, 94a04f95-911f-4fe4-86de-751cb8ee5b1f, 9ead1f47-73b3-457a-9091-e5abde7e1499, a846ed2b-1f89-4f7d-a471-2c2617b96be8, aba3e5db-081e-4a56-8a72-3acba5d1fdad, d1ed9524-0f19-468a-9500-88fc93b060bc, de8c8800-c843-4299-84cd-e2531d4d6fae, fbbbcd7e-d408-4a95-9730-2c5bd0a4a80a]
155627 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
155628 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
155731 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
155731 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
155845 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
155846 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
155959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
155959 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
156150 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
156150 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
156224 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
156455 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
156455 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
156707 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/09/26=WorkloadStat {numInserts=100, numUpdates=0}}}
156750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
156750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => []
156750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 100, totalInsertBuckets => 1, recordsPerBucket => 100
156750 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
156751 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
156760 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
156760 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
156860 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
156880 [pool-1540-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
156913 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
156913 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
156967 [pool-1540-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
156990 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
156990 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
157026 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
157026 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
157101 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
157109 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
157109 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
157169 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
157556 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 140
157556 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
157706 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 52 for /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet
157711 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet
157711 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 52 results, for file /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet => [01586dce-abe2-4313-b399-dbcf58e452bf, 032c8ea6-7adc-4a7c-b547-ef4da0fb5c43, 0353dbf4-a99d-48d7-a453-07334b631fc5, 045ef704-2606-435c-9b8d-aba4fbef2b2e, 0c1703a4-9d0c-44e9-9cc0-6c351173a888, 0de9e60e-60e6-4330-bb4b-a028cbdd7a51, 0e93f797-68e2-4e24-806d-bbc9fce5a7c8, 0eb02a5b-31e7-4365-95a4-998778dcc857, 0f41bfa0-3766-4140-b4bd-1140670c0f58, 1129bb16-c6e6-4061-a89a-b513c079e4ff, 131ef61f-d718-4340-9927-cbee36a71c46, 1588430f-081b-4138-a0a6-feb36146888d, 172ba8a7-eb6c-469e-b912-65ef2dbfdef5, 1755c396-3f52-4e6f-8e34-193b96ca06ad, 190eba2b-4968-452a-a010-fbb7eb312d10, 197dc902-dd8a-4179-8e36-3a9cf8874b77, 1dbb8d4b-77b4-43c7-84d7-e9f549580045, 27f49c90-242e-4bf3-828d-2cc1598cbfde, 2866314c-9f42-4a45-a369-ef09c90abc1b, 2a3a6f27-d863-4439-83a1-ffd9d7f6ba52, 2f308c3e-2edd-4fa6-907e-4b0cf90fb35c, 30979661-4089-48d9-8a39-828322d617d0, 31319e6b-8fdb-4e5b-ad25-978e5176028e, 36923913-18bf-44ee-b6aa-0e26cd67a745, 389a6553-75cd-4935-989c-6666cf0e233e, 38b4b6a3-7c1e-4b6f-9c00-f9c28da042a0, 3e3b218f-84b6-469f-a6f9-4eb2cc2bafdb, 3f64d6d2-f9cd-4bca-bc39-f719e0f5389e, 411d01f4-1d5c-47a7-b7ca-e1d8255eab73, 41ee4bbb-72b5-435d-a192-f100f82ebcd2, 49f1db05-4bef-4645-9d61-85b7146e3f66, 4ca73ae1-2143-492a-b2c3-e1f72b6521f4, 4da00b62-0ba9-4a99-ae5b-c67e370f9245, 536e8944-63e2-4d69-823e-174ef6d0eb09, 542c0562-63fa-4d4d-ab0f-0349200eba0f, 55d90b42-c22e-4003-b0f3-9c97a7dbca32, 5630f6c3-0e1d-4d2f-b9a9-44e38fc7f9ec, 56659688-f4d6-481c-988c-17c25d8bfe77, 56fc56f9-ee5b-4e62-89a2-5f2e63c95766, 5c8ece54-fddf-4973-af5e-7aaf86c8eb3e, 5d90b009-53eb-4cde-80ec-d49ff28dcda8, 61813f54-e61f-4ec4-8c95-a1ef9bbdfbaf, 621a541b-7d3a-4c64-9996-072f6ad47ec2, 64bab94a-e84c-47ec-8efc-32f3eec34cfe, 663b725f-afef-43db-9ef0-c3ea46b7d085, 6a8aa2d4-ea79-4f18-86bd-20f25afc87cb, 7687f874-ce56-48d6-b94b-2b3feba06f62, 7700ef6c-e4f9-4995-a458-99934a6fc6bd, 78082248-3ea5-485f-9b17-31a4ceecb844, 7a98b5e1-35ca-4b39-99d9-1c38b2325dc6, 7b57a89a-0f25-409f-9ef8-ec5fe94a6239, 7f24105e-5e50-48d1-8aca-9550ae0e4e9b]
157750 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 48 for /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet
157756 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 100 row keys from /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet
157756 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 48 results, for file /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_001.parquet => [865b6126-c4c1-4fbf-940f-a62882c8543f, 86a319f8-3b36-457a-af2f-28e692916894, 873ca849-b41a-4fb8-bc9d-940206293745, 87b3b385-6ba9-4d03-8198-f98ad0d8a5ad, 8ae05411-3584-4dd1-b35b-8524f826df07, 990bf04e-bb82-4a35-a286-89e9b8397d97, 9c689fb3-0871-4847-a18a-11bf17c1fdc9, 9fd0bd08-9746-452a-9479-319b390c509c, a03e4ace-74e3-4ee5-a374-dbece6fa6c06, a1d2e884-7020-4054-a618-59652020f4b2, a77c5aa7-e70a-4289-ae76-918b933eaf17, a7d87bcc-db7e-4b3d-96df-a9d12fe84849, a8bba90f-35e6-40d9-9283-b037a153317c, abf7287e-23f1-4bb8-bf31-983b3980eb92, ac5eaf0b-7dfd-4f26-a1ed-dc234ca5314b, b325a96a-1d45-4e7b-a98c-7b008c1cc446, b55934f2-eb11-4d26-8d81-be5b9f4bc6cb, b7f40408-ad91-4aaa-8823-428d706e001d, b99b6969-8e08-4b79-b80a-005cc50b6482, bdcef0fe-adf5-4a40-8a17-f101e82c716d, bde3aec2-3243-440b-ae6a-ede3b22db008, bea12d9b-af84-4729-982c-bfdd81b8794f, c26a5a5f-9e76-4d18-b199-ed07c5dd9cb1, c67ecc26-1b30-46b6-b74a-78c8a8905e8a, c7c10a3c-9705-4a5c-854f-52cbcad56cd7, c91d6f0d-1a4f-49f3-a313-8d9161d38f69, d28e78e5-aa14-436c-9884-de7a27bed91d, d29fcc07-3142-4dc5-9601-36a951297602, d3f47d9a-624b-4d11-8318-4443c2092a57, d78a52e7-6120-4f53-9827-fec7972cd160, d93b0d95-2ae5-41a5-8fa3-4a9a5a9a1f38, dafc503f-631f-4fc8-ab7d-5c3525d67fb3, dbf5d92c-7670-4b69-98c2-ac7cb89ffca1, dc5b55ff-7c9e-4847-8752-8aa517244d18, dc9865b0-c5fd-4fbc-b6d1-465979f25ded, dd2a1f03-688e-439f-9ba9-e3744ea5541c, e3a6a782-d981-42a2-b3c6-dfb680c63679, e7f7d143-73d6-46e5-9b86-e1ea75f756ff, e9195699-1fcf-45a1-899f-6c1889a2d13f, ea16f7e8-fa0f-4d57-984c-18f8d64143d7, eadcc8c5-931c-42c5-87c4-a8d66c17d035, f00b4311-4e92-4278-bf4d-4da5427cd491, f79290b5-8238-40a4-a448-616edcbe5a0b, f7d026fe-2c13-4d84-b98d-e123444cb773, fc004964-7195-49a2-9bca-d7f5052c6fb3, fc9f573c-e3d3-4524-8236-065a4692557b, fd64c631-69e5-4a7d-86f0-aba4fc3a8d6d, fffb77ee-6c90-45e9-9855-26d1a9a73d7b]
157867 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=40, numUpdates=100}, partitionStat={2016/09/26=WorkloadStat {numInserts=40, numUpdates=100}}}
157904 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 4438
157904 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=001, fileId=80154378-eece-4bf3-865d-74c8b90adc01}, sizeBytes=443784}]
157905 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 40 inserts to existing update bucket 0
157905 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=1.0}]
157905 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :1, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=80154378-eece-4bf3-865d-74c8b90adc01}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{80154378-eece-4bf3-865d-74c8b90adc01=0}
157923 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
157923 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
158179 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
158179 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
158232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
158232 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
158329 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
158348 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
158348 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
158419 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
158698 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 237
158698 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
158901 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet
158907 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 140 row keys from /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet
158907 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet => [10ed0850-da0f-4654-a906-275ceddcd879, 1ae4edbe-f694-4994-ae38-7248ceb628e8, 21eb84a1-5b33-4bd6-ad48-b6c3cd7a842a, 221b2503-9ed8-472f-aca3-4ad8704588be, 2621b451-953d-49dd-b6b3-a5d96254a541, 2cc7eb1f-306b-4f9c-8cbb-609cbac66f8b, 3536c7fe-b5bf-44ec-8b32-36d830d3167e, 3a8167f2-280a-48a6-9339-6402ae49777c, 3f422ed8-935d-4778-8bd2-66c999057397, 4334fb90-dda1-4a79-9c8d-f5821f5c1703, 438e4ec1-4bcc-4376-b558-2bbd7ac957e3, 47895ee3-4a49-477f-9c5a-2cf1eecc63fb, 5987e431-8fdd-4e74-bda4-58d51d81c943, 5c5da7e8-c2c6-4a9c-85a7-d10ef05df856, 61f866fd-ffb8-4bf6-b8e5-00a2016d5a98, 668d15d3-6ffb-406a-bd39-c32b6da290bd, 7c7c0f15-8eec-4583-807b-404f8d01e4fe, 82968f95-6c43-4163-8f7a-eef54a784fb1]
158934 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet
158938 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 140 row keys from /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet
158938 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit4787700891371699969/2016/09/26/80154378-eece-4bf3-865d-74c8b90adc01_0_002.parquet => [86e13996-fef5-489d-8f08-3b23b2907784, 96334344-098b-4c8a-9219-485c7fd8e416, 9c3d32dd-5839-4c24-bf5a-21d98e8f7de9, a44f0c1a-d5ef-4ebf-9eff-71479ede9ac8, ad0f8343-fd26-4124-9538-173d6c88227b, be944e77-b4a1-49ab-82a4-9f95dde3bd04, c0b1dc81-34d0-4831-9d11-bf6f86662b12, c86f0f81-7ee8-4116-88e6-d572496489a1, cdb8afbe-7f36-4f52-a3e3-b3691dfaa2e2, cddc9cab-c6fb-4a5b-9792-6925eaa0d687, d0173988-89f0-4949-8aec-0d0311d6109c, d3e55678-e300-44e3-a369-c5de423aeb94, d4015ed1-a1e6-457e-abf6-e6bc4cf13908, d4d048da-4e5b-43c7-bc7b-a546ba68ec24, e7d24a86-49bc-48ec-8c14-e4fe22388019, e85eae3b-2d46-483d-894e-47824e938f7c, e9a0c082-bc07-40d9-be67-df3baa8223eb, ee79e0ec-a113-439b-8ef3-20c40e9edd9d, f1b458de-9b66-46a3-8c1a-aaba0d13b47b, f3a39a00-454a-4533-8e50-2acbc2ca15aa, f7576b12-45d6-4ca4-bb38-9c066c7ad90b, fd461787-3adc-4bc0-878c-7a0f15a90d08]
159025 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=40}, partitionStat={2016/09/26=WorkloadStat {numInserts=200, numUpdates=40}}}
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 3195
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/09/26 Small Files => [SmallFile {location=HoodieRecordLocation {commitTime=002, fileId=80154378-eece-4bf3-865d-74c8b90adc01}, sizeBytes=447162}]
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Assigning 180 inserts to existing update bucket 0
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 20, totalInsertBuckets => 1, recordsPerBucket => 100
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/09/26 => [WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]
159049 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :2, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=80154378-eece-4bf3-865d-74c8b90adc01}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/09/26=[WorkloadStat {bucketNumber=0, weight=0.9}, WorkloadStat {bucketNumber=1, weight=0.1}]}, 
UpdateLocations mapped to buckets =>{80154378-eece-4bf3-865d-74c8b90adc01=0}
159058 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
159058 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
159445 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
159459 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
159459 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
159459 [pool-1559-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
159485 [pool-1559-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
159501 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
159501 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
159575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/09/26], with policy KEEP_LATEST_COMMITS
159575 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 1
159663 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
159686 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
159686 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
159980 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
160116 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=54, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=70, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=76, numUpdates=0}}}
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 54, totalInsertBuckets => 1, recordsPerBucket => 500000
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 70, totalInsertBuckets => 1, recordsPerBucket => 500000
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 76, totalInsertBuckets => 1, recordsPerBucket => 500000
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
160131 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
160143 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
160143 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
160214 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
160226 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
160226 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
160226 [pool-1579-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
160279 [pool-1579-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
160312 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
160325 [pool-1580-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
160339 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
160344 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
160450 [pool-1580-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
160480 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
160505 [pool-1581-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
160512 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
160512 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
160630 [pool-1581-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
161050 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2016-03-15_4985f45d-8297-40e9-a73f-ade603165b02_0_001_3_3.parquet to /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
161532 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2015-03-16_0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001_3_4.parquet to /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet
162025 [Executor task launch worker-4] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2015-03-17_c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001_3_5.parquet to /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet
163561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
163561 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
163915 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
163915 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
164044 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
164067 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
164067 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
164594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
164594 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
164713 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 70 for /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet
164756 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet
164756 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 70 results, for file /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet => [00e2da70-1423-489f-82c2-2ac50484553f, 071202af-5fed-4cbb-b2a0-323258a2084c, 07f25b24-0934-4975-84f0-bb487b6fe68c, 090b2f55-22b2-49f7-bee3-adc853210baa, 0be2a084-eb54-4db6-80f1-79031353d123, 0f4a95c4-c26a-4cd9-a1e7-fbeb3bc2e959, 0f915f25-e940-4180-b3b9-30310e10f097, 11bc5b6e-5575-4aaf-ba09-9f2f7ce1f0e5, 161a5321-d8da-4f13-9253-12fb56b4a74d, 197fcdd4-b3da-4507-8006-bf116712c029, 1ff4bcd5-01db-41c6-b45d-1662f46c2377, 2122255b-7d8a-4162-8663-9c7e3cd2a3dc, 254a0477-64c8-4a19-8b9e-aec12bc47a9a, 2a7964f5-3f02-48b3-8974-ed2d52d6253a, 2db46a22-5978-4346-a6fc-a1e14cc50466, 2fa3eb43-6c93-4403-891e-1658a6a70ee1, 33e274b5-4499-4c3d-888c-b0ca234cf14f, 34180013-8d5b-46f2-a01b-3ea549f8e288, 3a809c59-0e04-4e7c-9823-0b011848e284, 3c9c387d-c3e9-45a6-ac78-97458b828ce8, 3f1ef929-ea59-46ec-b43a-495fa399f6bb, 45540ad3-ee15-4789-96f5-c5cb6baae2f7, 4557bad5-fd39-4329-9e07-c2c61cd6c7c4, 48cfdb95-49d7-427a-bbe7-e8f2c3dbce59, 4a8bb684-0e7f-43f2-97bf-89f116a7e52f, 4ab9236b-7091-4cce-9e4a-56aa816c811b, 4b4c5a8b-bdac-4d0b-b38a-273fb293b5d5, 4db0942a-8cd1-4239-9889-98a9d0aa8f8d, 50252020-0340-44ca-9535-48ab107ffe9f, 597d3759-8ff3-49b8-81a7-8478fd67765a, 5ab38049-5f20-4110-b4b3-450e24a265d6, 5b4f8cd0-41a2-4c9e-8eb9-35fbbde4b1e3, 5c5bd11b-2078-42a0-a044-5f07cb6100d1, 646b9af4-874a-4d4c-ae79-d237bbbb456c, 64ce8cbb-8cb0-4a6a-9c02-5c1a4bd30f0d, 65344de2-20d2-4249-b631-b8ffba40f34d, 66ae64c8-891a-4959-b1d8-b2e9d84ba1ef, 6b4b339f-9bff-47bd-bbf6-4a66c149ffd0, 6d471e11-8183-4ea2-8771-f6035300be17, 72c2a646-5066-429a-a953-d934df621b55, 7aa9ce8a-bc75-4a87-807f-5d573befac57, 7e0f0d6b-1ef7-4d18-a1a0-997c4fefbea0, 7f4ff75d-8f2a-4472-aa39-f0a78787c20e, 7fe71cf4-3689-4fa5-b96c-f9cf3bcb1565, 856a0bfd-feb8-4e54-a2be-6cd97eea688e, 873b6f41-7da7-4db4-bc34-da8d3d778c16, 93af0349-ef94-4711-9716-df27e172fe5d, 94f4142d-1c1c-4ff0-a478-22f14c4701c1, 95bd933d-1b47-4d84-a395-4d7d7b98c2f1, 95f5c8c5-fec0-4780-a0ae-4042ce0bb039, 9fa2f5fe-de36-474f-833f-b7384d335a6c, a28431f8-a576-4da0-a2a1-5b542952fc5e, a5390e43-eeb8-4b75-a196-d508fce82669, a78b705c-a042-464d-b93f-f677af82aac0, b0f963b6-fe97-49cc-af1e-72f09ad7fb0c, b77865a9-8114-4cdd-b9ca-c394d88708db, c648c531-ec8a-4f2c-88c6-0403cc5f523f, c6a0fdc1-4ebf-4a7e-88da-7d6168bfbc1f, c82fcf2a-23dc-4eb8-8043-49aa2462b4c9, c937481d-9ef4-4f61-9e6a-b1acd05fb53e, d1db5779-9ea0-4fb6-bebe-e7822d3aedb9, d8a429a7-55cd-4ee3-9567-a013ee49b67c, e6327012-758a-41df-ab99-34480dcef313, e851a5dc-6a00-4f79-8a15-d7130c414216, f09dc47b-868c-4419-b9b6-f5c6d056b46b, f288ab4a-4a7b-4d1f-8db9-ded75625c3a2, f52d1aa8-ba2c-494f-96f6-5639d219ef3c, f9cee427-a157-43c3-a814-a2db8c54c728, fca7fc57-33c8-4d9b-97e0-7dff78ab353c, fd66e623-72df-428c-8019-ca459681693f]
164768 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 54 for /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
164773 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
164773 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 54 results, for file /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet => [001f7328-2544-42ac-ace2-96ed0b6edae0, 01774f5d-ea56-476b-8d35-a4b836750590, 034bdc77-d8a9-444e-8d23-b9ae73859732, 1720fb4d-fc88-4422-a168-feb6c88612b5, 184586bc-9fc8-4821-84e3-db5bb04953f9, 1b27ef87-2702-42db-8444-1d6f7175694e, 1b5e8950-433e-4381-a4aa-5d52920b41b9, 1be329b5-d1f3-4ed2-8d69-9141ab09cef2, 1cafc66b-0fba-4445-8379-5a26113023b2, 1e97b199-b3e5-44e1-8a43-b98861d5df70, 23a3bd40-6431-4f1b-9f25-2b3b1b097d0c, 2ba86e09-79ae-4a3f-9d60-631f8d4a68e9, 2d9878e4-d885-4eca-8137-40760060e951, 30163e4e-2d56-4883-9d3b-fdd2e4319e07, 319fde0d-3b3a-4dd9-a34b-501df597026c, 3cde4424-e03b-4bfc-9731-f8136c64dbb5, 47b36b9d-728c-4e09-b6bf-3295d7736bd1, 58b6550d-d9bb-478b-97b8-38af0424d253, 68759e7b-1e86-49f3-a17b-dfa8c9f31aba, 6981ffab-2e9c-402f-9aca-709ed86bacd6, 782c87a2-4909-42d2-8a6f-ce7f63b38ed1, 81ca6707-9734-43d4-a341-fe7318d43c28, 888ec761-4577-4f56-8462-86cb3341a9bd, 8b439eb0-1c09-4446-bb6e-bf1d1080eb72, 8e387aff-cb12-491d-8437-14af2fececd1, 8fcebecf-8eff-4c70-86a1-f842180089b2, 91fb6465-b351-4612-a5fd-991a485ef28c, a143d680-b5f4-4d7a-91d4-6c242291449d, a2990586-2759-4e13-a5b4-d30e2579de0e, a59ce1ca-4b66-4f68-aa8a-3bc4f55bba2f, a8ede483-b025-4318-9d2f-c3196418a1ac, af1783c9-bb9a-467d-bad2-7479a24b7c8a, af206bea-c002-4442-8965-c3f557c7bbfd, b10d63fb-aae6-4f91-8342-0306b6f7a4ca, b1701083-b02c-4b09-bf1c-4bb4b4f367d9, b2da7f8d-6bb4-415c-b37c-736c13f2af2a, b3486ec4-dfb8-4065-8b7c-b40a6058371e, b3bc25e3-836a-4b0a-adb3-ac0edff1eaab, b886d308-6e1b-41dc-8be8-357b8b0cecd5, b941a0d0-a374-4d54-b4f5-82c52fc0cf6b, c0ae1588-d062-4b16-82c3-0dcb5af2adcf, c9e7075f-0c89-4c85-9ea4-b0b81876bee6, cb4b54db-d98b-4889-8f5c-37a60c589758, cc1ee0f9-cc57-40b7-a0df-87a176279b85, d201b73c-66c5-4ef6-92d3-97ebc52b129c, da08e98b-86ee-4736-8fb4-ccc40c081803, e19ce9c7-5f3c-4b06-9448-6f43699a210e, e1a1eadc-d234-410d-bb16-09f577407fe7, edc77823-e80a-418b-a870-dc5aa4743daa, f1c291cc-ec8d-4cfb-aeed-a2813d930226, f4d79e4c-87b3-4fdf-93c8-28eff393f030, f8155fd0-e0e1-4c05-b9db-4a29f5dd4fe1, fa01833c-a518-446c-a404-d44af6c5bdf2, ff2d7f1d-a5c1-4f33-bbd7-0b141f9da209]
164783 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 76 for /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet
164787 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 76 row keys from /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet
164787 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 76 results, for file /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet => [01d49579-cb26-47d3-a67e-9e1b9e17e6f7, 0b70a277-16f6-4534-94e5-642a1bd4e264, 0f2ede9b-2097-4530-a274-856df808d763, 0fe946a3-2a62-4ed0-8a55-46c7e8a145eb, 105c363d-1a57-43aa-86dc-e99434b23904, 119fa736-aaaf-4faa-9746-c4c1e90bd20e, 16196fb6-3734-466a-9cfb-84c0cc50dd90, 19aa5263-7bdf-4f98-93a9-2077bf4f414b, 1cd9f21e-4f58-4ad7-997a-548b0b69b103, 26ee7ea6-dd35-458f-93c3-69889c663e81, 2e1ece48-571c-4dab-97d3-6c78614543f8, 362324ef-7d71-407b-b039-631eb8f9adaa, 3a267c7f-6d4a-41b1-8a09-4b6d81f0d15a, 3c96565b-6273-4673-96e1-fecc929f155d, 3e816263-5648-4654-8f1f-a5629ffcf247, 4020d37c-0b36-4341-9ec3-5a38cb494587, 4316c5fa-f566-4aa2-96e1-3d11739c174c, 47bceb7f-39bd-421c-8d8c-bc20e84389f7, 4a6350e6-170c-4639-86ed-cf3cae190b74, 4c15f5bf-5cea-47b8-bb05-b96ff0726c7b, 4d354a30-d3f0-4e96-bb49-4804ef71e8b3, 567f5ed3-be12-4d97-b809-ae2554b43a36, 6181869d-8ce1-434f-8473-728e4b8ee2b5, 6913eca8-5bff-4575-a456-776e18b6fa11, 69200bca-474e-496a-a327-01908b265052, 6af02a85-a36e-449a-96be-c7c179a58348, 7119e131-4747-47d5-b2fc-6748c4846954, 71b23b3a-ebc3-404a-9455-81e94230ffb4, 744ccbea-1c6f-4306-8dc4-8c50dc203f99, 7520eb91-a0a1-4766-82ac-46487351ac63, 76010819-c0d9-4323-91a1-92f272a438aa, 780f3852-3d4a-453e-84b7-9fc76494a31d, 7aaa8ef2-d780-46bf-9ade-a07cccb76ef4, 7d197490-25e7-43b5-9bab-82782d935cb1, 7d793a06-995b-461b-bec5-95a687e08b92, 86a25489-ff2f-41b4-86d1-ec3cfe403cdc, 89217ad6-adf7-4719-a32e-0db98ea413d7, 8ec9d311-de33-4f0d-b2cb-707b0447037b, 90ce6d8c-9ba5-43b9-8edc-e31bc3277a18, 97906bf9-5dd6-43b3-9d83-e63f52aec9c4, 97a00efa-0882-4336-8620-ce9e1d5defa4, 98a167a0-958f-4c44-b670-ee21f7acebbb, 98c63745-6cae-4d73-825e-de5a3b075d65, 9d610d6e-30d6-4276-b37c-8d18d3fb1879, 9e95cf7f-f7ce-4f04-b670-eb0e49cc272b, a1203cca-943e-4032-b0ee-2c5c63b4aa45, a34c92ad-7bde-4c85-91ef-a64d804d5ba2, a5d219cf-81fd-4a54-939f-9defccf7d97a, a9a00ab5-05f1-4c5b-a6b2-7b9102b8fdb5, aa63890a-eea5-4783-93ec-733551ab84f1, abf34016-41a8-43e6-befb-7e12244e399a, af192c7e-327a-4a96-8d49-ec982e8a5890, aff11d35-bf5f-421c-828a-b7b302ca131e, b28e6d79-e2ca-43ff-a641-ab061ddd4999, b9d3c7f5-f5a0-409b-ae66-8835f1651cae, ba29aae2-65c6-4c73-b67c-7955ee423e8b, bb14acc6-334c-4294-97d3-07c67b16156b, c233ad66-942a-4ccf-937d-e40c45cb8b18, c33f6ea0-338d-4cab-b70c-587f04b6d0aa, c47f9210-8bb2-4eec-893a-3b18c51a1199, c6c270d0-119d-4eab-8517-4ba2c06ad0ae, c6ea6b83-7787-4ead-a55d-2d48f971eac4, cb889e1f-4373-4f4c-9800-1c252b9ce8f2, cf5640b4-9ed8-4307-84ce-d2b71aa06d39, d369b471-8e8c-4378-b967-d23e4ab7feae, d9decec1-6a27-43f3-9769-7e7463a6088e, e542d5b1-3231-4e83-97c1-088ff2792282, e56dc8af-58fa-4646-b949-f32692246363, e8edd7d3-a2ac-4ae4-ae91-2741c0fd265a, e93fbe48-1528-4770-a0b0-afed63ee7c77, ebb6132f-4509-4ff8-9809-2999c26635db, eca5f89c-87fc-48be-91dd-02eb9eb1ccbc, f23315c7-2fa2-4918-aac6-b5ec3232da93, f76bd067-0c10-4c1a-8141-8ede17f65f84, fac165b7-7ac7-440e-954b-2ebed33c0bc8, fce45c53-f2dd-4112-9aed-6b0a71095da2]
164828 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
165091 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 77
165091 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
165245 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet
165266 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet
165266 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_1_001.parquet => [00e2da70-1423-489f-82c2-2ac50484553f, 090b2f55-22b2-49f7-bee3-adc853210baa, 0be2a084-eb54-4db6-80f1-79031353d123, 0f4a95c4-c26a-4cd9-a1e7-fbeb3bc2e959, 0f915f25-e940-4180-b3b9-30310e10f097, 161a5321-d8da-4f13-9253-12fb56b4a74d, 2122255b-7d8a-4162-8663-9c7e3cd2a3dc, 2a7964f5-3f02-48b3-8974-ed2d52d6253a, 33e274b5-4499-4c3d-888c-b0ca234cf14f, 48cfdb95-49d7-427a-bbe7-e8f2c3dbce59, 4ab9236b-7091-4cce-9e4a-56aa816c811b, 4b4c5a8b-bdac-4d0b-b38a-273fb293b5d5, 4db0942a-8cd1-4239-9889-98a9d0aa8f8d, 50252020-0340-44ca-9535-48ab107ffe9f, 597d3759-8ff3-49b8-81a7-8478fd67765a, 646b9af4-874a-4d4c-ae79-d237bbbb456c, 64ce8cbb-8cb0-4a6a-9c02-5c1a4bd30f0d, 65344de2-20d2-4249-b631-b8ffba40f34d, 6b4b339f-9bff-47bd-bbf6-4a66c149ffd0, 7aa9ce8a-bc75-4a87-807f-5d573befac57, 7fe71cf4-3689-4fa5-b96c-f9cf3bcb1565, 93af0349-ef94-4711-9716-df27e172fe5d, 94f4142d-1c1c-4ff0-a478-22f14c4701c1, 95f5c8c5-fec0-4780-a0ae-4042ce0bb039, a78b705c-a042-464d-b93f-f677af82aac0, c648c531-ec8a-4f2c-88c6-0403cc5f523f, c6a0fdc1-4ebf-4a7e-88da-7d6168bfbc1f, c937481d-9ef4-4f61-9e6a-b1acd05fb53e, d1db5779-9ea0-4fb6-bebe-e7822d3aedb9, d8a429a7-55cd-4ee3-9567-a013ee49b67c, e851a5dc-6a00-4f79-8a15-d7130c414216, fca7fc57-33c8-4d9b-97e0-7dff78ab353c]
165279 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 7 for /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
165295 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
165295 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 7 results, for file /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet => [01774f5d-ea56-476b-8d35-a4b836750590, 1720fb4d-fc88-4422-a168-feb6c88612b5, 2d9878e4-d885-4eca-8137-40760060e951, 3cde4424-e03b-4bfc-9731-f8136c64dbb5, 58b6550d-d9bb-478b-97b8-38af0424d253, 81ca6707-9734-43d4-a341-fe7318d43c28, 91fb6465-b351-4612-a5fd-991a485ef28c]
165324 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 12 for /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
165353 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet
165353 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 12 results, for file /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_0_001.parquet => [a59ce1ca-4b66-4f68-aa8a-3bc4f55bba2f, a8ede483-b025-4318-9d2f-c3196418a1ac, af1783c9-bb9a-467d-bad2-7479a24b7c8a, af206bea-c002-4442-8965-c3f557c7bbfd, c0ae1588-d062-4b16-82c3-0dcb5af2adcf, c9e7075f-0c89-4c85-9ea4-b0b81876bee6, cb4b54db-d98b-4889-8f5c-37a60c589758, cc1ee0f9-cc57-40b7-a0df-87a176279b85, edc77823-e80a-418b-a870-dc5aa4743daa, f4d79e4c-87b3-4fdf-93c8-28eff393f030, fa01833c-a518-446c-a404-d44af6c5bdf2, ff2d7f1d-a5c1-4f33-bbd7-0b141f9da209]
165363 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet
165374 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 76 row keys from /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet
165374 [Executor task launch worker-4] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_2_001.parquet => [0b70a277-16f6-4534-94e5-642a1bd4e264, 16196fb6-3734-466a-9cfb-84c0cc50dd90, 362324ef-7d71-407b-b039-631eb8f9adaa, 47bceb7f-39bd-421c-8d8c-bc20e84389f7, 4c15f5bf-5cea-47b8-bb05-b96ff0726c7b, 567f5ed3-be12-4d97-b809-ae2554b43a36, 6181869d-8ce1-434f-8473-728e4b8ee2b5, 6913eca8-5bff-4575-a456-776e18b6fa11, 7119e131-4747-47d5-b2fc-6748c4846954, 71b23b3a-ebc3-404a-9455-81e94230ffb4, 780f3852-3d4a-453e-84b7-9fc76494a31d, 7d197490-25e7-43b5-9bab-82782d935cb1, 7d793a06-995b-461b-bec5-95a687e08b92, 89217ad6-adf7-4719-a32e-0db98ea413d7, 8ec9d311-de33-4f0d-b2cb-707b0447037b, 9d610d6e-30d6-4276-b37c-8d18d3fb1879, 9e95cf7f-f7ce-4f04-b670-eb0e49cc272b, a1203cca-943e-4032-b0ee-2c5c63b4aa45, af192c7e-327a-4a96-8d49-ec982e8a5890, aff11d35-bf5f-421c-828a-b7b302ca131e, b28e6d79-e2ca-43ff-a641-ab061ddd4999, b9d3c7f5-f5a0-409b-ae66-8835f1651cae, e8edd7d3-a2ac-4ae4-ae91-2741c0fd265a, eca5f89c-87fc-48be-91dd-02eb9eb1ccbc, f76bd067-0c10-4c1a-8141-8ede17f65f84, fac165b7-7ac7-440e-954b-2ebed33c0bc8]
165448 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=77}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=19}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=32}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=26}}}
165498 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6615
165498 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=0cc055c1-0cbb-465d-b947-95c7de2ad2ec}, 1=BucketInfo {bucketType=UPDATE, fileLoc=c94864a8-c50b-4be5-b87a-c8cc752413e2}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4985f45d-8297-40e9-a73f-ade603165b02}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{0cc055c1-0cbb-465d-b947-95c7de2ad2ec=0, c94864a8-c50b-4be5-b87a-c8cc752413e2=1, 4985f45d-8297-40e9-a73f-ade603165b02=2}
165506 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
165506 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
166526 [Executor task launch worker-4] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2015-03-16_0cc055c1-0cbb-465d-b947-95c7de2ad2ec_0_004_43_461.parquet to /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_0_004.parquet
167003 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2015-03-17_c94864a8-c50b-4be5-b87a-c8cc752413e2_1_004_43_462.parquet to /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_1_004.parquet
167476 [Executor task launch worker-3] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Renaming temporary file: /tmp/junit82794037870151337/.hoodie/.temp/2016-03-15_4985f45d-8297-40e9-a73f-ade603165b02_2_004_43_463.parquet to /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_2_004.parquet
169289 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
169289 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
169396 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
169396 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
169508 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
169516 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
169516 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
169758 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 77
169759 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
169875 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 32 for /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_0_004.parquet
169881 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 70 row keys from /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_0_004.parquet
169881 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 32 results, for file /tmp/junit82794037870151337/2015/03/16/0cc055c1-0cbb-465d-b947-95c7de2ad2ec_0_004.parquet => [00e2da70-1423-489f-82c2-2ac50484553f, 090b2f55-22b2-49f7-bee3-adc853210baa, 0be2a084-eb54-4db6-80f1-79031353d123, 0f4a95c4-c26a-4cd9-a1e7-fbeb3bc2e959, 0f915f25-e940-4180-b3b9-30310e10f097, 161a5321-d8da-4f13-9253-12fb56b4a74d, 2122255b-7d8a-4162-8663-9c7e3cd2a3dc, 2a7964f5-3f02-48b3-8974-ed2d52d6253a, 33e274b5-4499-4c3d-888c-b0ca234cf14f, 48cfdb95-49d7-427a-bbe7-e8f2c3dbce59, 4ab9236b-7091-4cce-9e4a-56aa816c811b, 4b4c5a8b-bdac-4d0b-b38a-273fb293b5d5, 4db0942a-8cd1-4239-9889-98a9d0aa8f8d, 50252020-0340-44ca-9535-48ab107ffe9f, 597d3759-8ff3-49b8-81a7-8478fd67765a, 646b9af4-874a-4d4c-ae79-d237bbbb456c, 64ce8cbb-8cb0-4a6a-9c02-5c1a4bd30f0d, 65344de2-20d2-4249-b631-b8ffba40f34d, 6b4b339f-9bff-47bd-bbf6-4a66c149ffd0, 7aa9ce8a-bc75-4a87-807f-5d573befac57, 7fe71cf4-3689-4fa5-b96c-f9cf3bcb1565, 93af0349-ef94-4711-9716-df27e172fe5d, 94f4142d-1c1c-4ff0-a478-22f14c4701c1, 95f5c8c5-fec0-4780-a0ae-4042ce0bb039, a78b705c-a042-464d-b93f-f677af82aac0, c648c531-ec8a-4f2c-88c6-0403cc5f523f, c6a0fdc1-4ebf-4a7e-88da-7d6168bfbc1f, c937481d-9ef4-4f61-9e6a-b1acd05fb53e, d1db5779-9ea0-4fb6-bebe-e7822d3aedb9, d8a429a7-55cd-4ee3-9567-a013ee49b67c, e851a5dc-6a00-4f79-8a15-d7130c414216, fca7fc57-33c8-4d9b-97e0-7dff78ab353c]
169894 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_2_004.parquet
169899 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 54 row keys from /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_2_004.parquet
169899 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit82794037870151337/2016/03/15/4985f45d-8297-40e9-a73f-ade603165b02_2_004.parquet => [01774f5d-ea56-476b-8d35-a4b836750590, 1720fb4d-fc88-4422-a168-feb6c88612b5, 2d9878e4-d885-4eca-8137-40760060e951, 3cde4424-e03b-4bfc-9731-f8136c64dbb5, 58b6550d-d9bb-478b-97b8-38af0424d253, 81ca6707-9734-43d4-a341-fe7318d43c28, 91fb6465-b351-4612-a5fd-991a485ef28c, a59ce1ca-4b66-4f68-aa8a-3bc4f55bba2f, a8ede483-b025-4318-9d2f-c3196418a1ac, af1783c9-bb9a-467d-bad2-7479a24b7c8a, af206bea-c002-4442-8965-c3f557c7bbfd, c0ae1588-d062-4b16-82c3-0dcb5af2adcf, c9e7075f-0c89-4c85-9ea4-b0b81876bee6, cb4b54db-d98b-4889-8f5c-37a60c589758, cc1ee0f9-cc57-40b7-a0df-87a176279b85, edc77823-e80a-418b-a870-dc5aa4743daa, f4d79e4c-87b3-4fdf-93c8-28eff393f030, fa01833c-a518-446c-a404-d44af6c5bdf2, ff2d7f1d-a5c1-4f33-bbd7-0b141f9da209]
169909 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_1_004.parquet
169921 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 76 row keys from /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_1_004.parquet
169922 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit82794037870151337/2015/03/17/c94864a8-c50b-4be5-b87a-c8cc752413e2_1_004.parquet => [0b70a277-16f6-4534-94e5-642a1bd4e264, 16196fb6-3734-466a-9cfb-84c0cc50dd90, 362324ef-7d71-407b-b039-631eb8f9adaa, 47bceb7f-39bd-421c-8d8c-bc20e84389f7, 4c15f5bf-5cea-47b8-bb05-b96ff0726c7b, 567f5ed3-be12-4d97-b809-ae2554b43a36, 6181869d-8ce1-434f-8473-728e4b8ee2b5, 6913eca8-5bff-4575-a456-776e18b6fa11, 7119e131-4747-47d5-b2fc-6748c4846954, 71b23b3a-ebc3-404a-9455-81e94230ffb4, 780f3852-3d4a-453e-84b7-9fc76494a31d, 7d197490-25e7-43b5-9bab-82782d935cb1, 7d793a06-995b-461b-bec5-95a687e08b92, 89217ad6-adf7-4719-a32e-0db98ea413d7, 8ec9d311-de33-4f0d-b2cb-707b0447037b, 9d610d6e-30d6-4276-b37c-8d18d3fb1879, 9e95cf7f-f7ce-4f04-b670-eb0e49cc272b, a1203cca-943e-4032-b0ee-2c5c63b4aa45, af192c7e-327a-4a96-8d49-ec982e8a5890, aff11d35-bf5f-421c-828a-b7b302ca131e, b28e6d79-e2ca-43ff-a641-ab061ddd4999, b9d3c7f5-f5a0-409b-ae66-8835f1651cae, e8edd7d3-a2ac-4ae4-ae91-2741c0fd265a, eca5f89c-87fc-48be-91dd-02eb9eb1ccbc, f76bd067-0c10-4c1a-8141-8ede17f65f84, fac165b7-7ac7-440e-954b-2ebed33c0bc8]
171041 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02, 2016/06/02], with policy KEEP_LATEST_COMMITS
171041 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
171290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195422
171534 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=500, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=187, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=146, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=167, numUpdates=0}}}
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 187, totalInsertBuckets => 1, recordsPerBucket => 500000
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 146, totalInsertBuckets => 1, recordsPerBucket => 500000
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 167, totalInsertBuckets => 1, recordsPerBucket => 500000
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
171572 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
171581 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195422
171581 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195422
171760 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
171779 [pool-1644-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
171866 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
171867 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
171940 [pool-1644-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
171983 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
172002 [pool-1645-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
172065 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
172065 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
172123 [pool-1645-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
172163 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
172184 [pool-1646-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
172223 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
172236 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
172337 [pool-1646-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
172392 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
172392 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
172731 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
172731 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
172838 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
172893 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195422 as complete
172893 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195422
172982 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 9 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
173191 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 13 contains a task of very large size (159 KB). The maximum recommended task size is 100 KB.
173212 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 500
173212 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
173253 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 14 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
173270 [dispatcher-event-loop-1] WARN  org.apache.spark.scheduler.TaskSetManager  - Stage 15 contains a task of very large size (160 KB). The maximum recommended task size is 100 KB.
173359 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 146 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet
173395 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet
173395 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 146 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet => [0004db58-b843-489e-9f8b-644952c44546, 017842aa-9d63-4bf1-b9df-653409f5a9e7, 01a36dec-30ba-418a-90bc-8c080bdeebec, 02ba262f-ee9e-43ab-975c-ae401885a13d, 030999ef-a206-4254-a19e-1f2cf0ffeb2f, 056e809e-11f6-47dc-995d-94e38a79720d, 09240cfb-082d-426b-acac-e9e4184e38a3, 0dce30d9-7d25-47d9-a072-055695037788, 11669b7c-79ca-4430-b257-43940c47bf9f, 129acbea-ea5c-45b4-b5ae-b2985b319ea4, 133f0ca9-0ccc-47a2-9ed2-5d5b4e2d0fa5, 1349a264-908b-49d5-9102-7af6df27b47d, 178e80e5-738c-4775-8d8e-dac93a7598be, 17dc59a8-5fc0-43aa-b90a-3c7fca91de55, 17e51cf4-ac60-454d-8ce0-59206928da27, 18174696-35d4-4d52-b9e5-3594cb9f40dd, 1892a55c-503e-466b-abd6-133472d20913, 196edde8-df45-42fc-b85a-20ea236c9829, 1a2c8eb1-5ed8-4f4f-90b7-d9713baae785, 1c648096-66fd-40d0-ae66-14957bc449ae, 1f090ff3-c1b0-4df2-88fe-2201c5f4adcb, 20bd0922-7c44-4be3-8255-0d584cf2273c, 214c91d0-5538-4dd0-8942-5eb604a099ae, 215d91bd-89b3-4fa7-a64d-6ca89f467950, 248fe026-6314-49c4-b7db-60b51e541ac6, 268fcbbb-9632-4b69-89bb-c26edce6a1c3, 26ab611d-6208-4e91-8e90-7f97a38e291f, 29071a5b-a9ca-4b56-85c7-7bc15efb785b, 2a65d2ee-5286-474c-a499-8b25250fccbf, 2b19e824-6514-4942-a0d3-0ca253fca8c9, 30d1f40a-e1e9-41c2-afea-fb55eb3f4751, 30d26a27-1d7a-4d9f-82a2-c9daa3b2c90a, 3952c7f7-221d-4424-b6c9-1bbf9f67db82, 3e598b8c-5dc7-480d-ae42-a4df0d5831f7, 3f708d91-8f86-4986-bc75-559e29b1ae92, 40f1dec9-b8e9-4100-9288-a25064458f30, 4329713b-0cb7-4a39-a3d9-f766a0e52ec7, 49004182-1557-4aaf-a45f-7eef722371b8, 49f08601-5506-4d51-981b-d54bbcdb441e, 4ed6d68d-a8cf-4478-a963-c0c2f858f1bb, 4fe85051-32cb-45c0-b0be-c3517eb0d3bf, 5139ce7f-a775-4060-9371-dcb0625c2844, 51628f50-8911-414a-b796-df02c976ce2d, 51d04eb7-6148-4118-ab27-5d4f07b3da8e, 5256bff1-8c78-4b91-ac93-30adb15de779, 53f42054-8ea6-4229-bb7e-57783889ab42, 545bcec0-65c8-4b25-ba55-51a2f8c171d5, 54d0e11a-752d-460d-937e-e20111528a24, 5553c469-c62c-4f82-a47f-25f1ca99f0c6, 55f54db3-4b9e-4f06-b3fe-25a19a6a8edb, 5768db08-d80c-4764-942c-a8f0069b420c, 5a4ce07e-c71d-4896-999e-4598c538c961, 5abeff0a-a31a-4395-b281-e52c9b153187, 5c575b62-fb1d-4eb7-bf00-96eccacd8e9f, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 5dec3bee-c8bc-492a-a096-091995617dd7, 5e0bc964-2460-43d9-9043-b527d4abe5db, 667006f5-0fd0-4006-b58c-025fe8ac33b1, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 695d0eec-133a-47e3-9d0c-7c4f223bb1b2, 6a050baf-fa7d-45fc-85dc-c0db630fc27b, 6ba7344a-ea63-4011-9f65-3c51ae4256df, 6d43a2a6-061f-4312-9b37-cc24f38c1c16, 7044ffc5-45fd-425c-8a6f-425296b646ff, 7537c283-de79-444a-b183-eff7e771ce5f, 754cad80-8eaf-4054-b212-f50b6803bbdf, 77a74145-a4cb-44f5-bf56-a5eb0dedf57a, 78a5d9c8-6d3d-4964-84f5-1412dfac8346, 7c03eeb2-034e-4537-9904-c62fc9547baf, 7d69d833-9849-4518-8f27-50aab2d0e5d1, 7d7dca4d-a657-4e49-b770-630f1e3559dc, 7fc09cae-d8d9-4563-93e4-cd9779efcf82, 81c2d2fd-2eef-4cda-a599-79c14b06185a, 8523598d-2e46-4d69-a51a-f22b0fd2b699, 8568448d-1fc7-4181-b40c-87adb7fc40af, 87ac0a53-64e0-4520-a1e5-067421efd27c, 8a1cc4c7-4893-4cd5-885d-3109e30d830d, 8af92b2c-e601-484e-bfe9-2adc060362a3, 8bb7df69-5482-4bda-8266-ed992618a218, 8d4646c7-6b04-4194-bbb3-d9a9562c70d3, 92176651-d67f-47b7-9cd9-6c952adc9bfa, 9861c56b-ce53-4dd3-adf9-e18f4b7bc3c7, 990360ad-70a5-4ffb-940f-59278cd366fc, 99590b3f-f54c-4de7-9c04-0e64bf4f769d, 9a605f00-a29f-47f8-b29c-3a0957581be5, 9ac4a69f-d781-48b2-a5f7-988761fe584a, 9d7f6be6-58e7-46af-b59a-77f25a0f303f, 9ef2fc8f-dce9-475d-a7da-a85742fe7c54, a1494460-ed65-4614-9bc6-70094d424ee3, a1b91bdf-bd2b-4c7a-ab54-961828083278, a355621c-c178-4de8-a5f9-fc01667c5c6d, a41034e2-6135-44f1-ad4d-6a71de909471, a4774175-0279-484c-903b-1473989cc381, a5d38922-1d5e-447f-aea1-ed9651cf4949, a9b33ec7-65b5-463d-8009-a75ab5d38841, a9f9537a-83ff-4398-910c-8bdea3e2a422, aba3570f-7e84-460c-b7c9-271b2ab4add7, abc89782-f43f-4f89-94ba-62fde2f57b8f, b6eba60c-2ab1-409a-8ad1-00257a1bda5d, b8e60966-19b5-494f-bdd5-5880b5376de7, bdd9944c-5767-4054-a610-336cc376f859, c0474c5e-288d-47c0-833f-0dd68eefa29f, c062cf4e-5c5b-4af5-8b57-67006ce51dd4, c118b025-5816-4abf-b176-cc4a17289c0f, c1499be1-3e6d-4bc5-ba58-c1e09cb89622, c23ef669-0bd4-4a9b-ac40-73f6f4654c4c, c2813043-268e-4118-a3e3-e2365164c26d, c4c67410-a14c-48a9-9815-da023aa874e3, c5cdb2af-6ff9-47fe-9b8b-4f393b36db39, ca87648b-f916-4ad2-a28f-ce4bb03e6bf6, ce34786d-9ca7-4f3d-ac1f-6861af309e3e, ceadd625-5a0e-4e63-a8e2-8431ca9fc0f4, d0d2897f-689d-4b7a-87d2-c370984a2cc4, d29b67d9-44fc-4df9-bcba-0fe13c9344c2, d29ea52f-8bbc-4ad7-9e4a-6a62d0dc3ce3, d51be0c9-7865-472a-9bd5-3fec5a3d0f31, d897a141-eac2-4e26-ae06-5c0daf3c773a, db033b2b-0587-43b8-b475-27e6f26fb369, db173160-96bf-4805-8fd5-3a3d9310764c, db2453fc-071a-43f6-83d8-c8e7068fc596, dc12707b-a265-4d84-ac26-bdf86f4204d0, df76930f-b8a2-4ab4-8950-c942a3414504, e155d7c8-93ad-4a61-8eb2-6a289e317811, e160fcba-ce59-44c1-961d-c42a1d2638f8, e272902d-898c-4c9a-8db7-d254a7a1e026, e4318c37-dfb3-4ccd-8a10-e8ffc11b70ab, e4d921d8-336b-4d6e-b065-b172bc484a81, e4f1ba66-d2b6-4c91-a8a5-073e9e583d2b, e7d75d12-35c8-442c-b19d-b9da41e0a0df, e7e7dbf3-14cb-440c-971a-e2c3f0a1829e, e8ebede2-b310-4399-b76d-dd841c52aad5, e912a426-97ed-47f6-a015-9d62cb66371c, ead061e2-5b23-460e-845a-1a0d81cdde9a, ed250813-60a1-409e-8eec-6b6dae682148, ed816151-b8c8-4051-9bd9-b5809fdde18e, ef7e863f-9780-4f82-861f-9be106f2ac10, f0f6ca3f-0e36-47aa-adb7-b41e6ae490fd, f13b3d80-671e-4fc7-8e54-76b252b190f7, f393468e-731f-4594-befc-bac0fac6c66b, f3e446e8-5686-4f82-8f10-29f2a634f6e6, f4036c9d-bf5c-4efa-9a7b-afcddc7064f5, f7aaca79-90a8-4977-89cf-818919b9dd38, f7b777be-d581-482c-8532-d9375fefff1d, fa45e310-ddcb-4c43-8d01-d7b77f64001b, fd82bb16-5857-40c4-a967-7b93b0fede52, fdec1d4b-451e-494a-9d6e-f2f3941fdb07]
173405 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 167 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
173432 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
173432 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 167 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet => [0399174a-e6d8-4b10-9bab-313b5865902c, 03cf03dd-7401-4ff7-9718-4595f3bb33a8, 045596b1-c6d4-4505-bd30-53f2e26a38d7, 0ae33ffe-73c5-4a58-a7fc-55c7f83ca7ef, 0e0c77f4-9934-4a53-9efd-bfc66cbb3de2, 0e608d43-82f0-4deb-b407-f560496e1315, 0f02a21c-5705-4af7-a895-0f6e3a08b58c, 0fea4ca5-4cde-4d59-b304-56396e015fbf, 1410433f-7e3d-4b8f-837b-9f715dc82905, 151ec79b-fa39-46a1-86ac-743cc8286f6f, 15913463-f3ab-47b2-a5d5-75719350d2c7, 1635e52b-4d73-43a3-ab13-b50445bc73bd, 176b2686-150a-4283-896a-502ed68c9d94, 17cd6a9f-f3f0-4c40-9384-b10fdea261ee, 1a7b11ad-5cb7-4998-b50c-a09f39aa8ee6, 1aaf70dc-1f6d-44b2-b8eb-e67f25fa09a3, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 1de5b679-e077-4950-b0ab-b03042e6969e, 1e3db419-fa04-489a-b560-ededb3f71c10, 1f01cfb6-6811-4926-9c20-c692bd63dfeb, 20ce385a-4ef4-4905-9cf7-0b6d5eab4304, 22c74b61-2dc1-4e9c-b149-55ad165c7db0, 251ed43e-f00d-46c2-9eae-578b8dad7d8a, 2665bbc0-3f91-418b-8d01-c5d859f461c9, 2b670ba6-606c-42f0-981f-3615917d7574, 2bc512b1-f3fa-4875-900b-8c16e2ef1392, 2bea04c7-e5b1-486a-8a3e-5fd2e0174f9a, 2c187a4f-eabb-43f2-b1f4-2679197cd734, 2d5ee102-e378-44fa-9680-18f66b1f403e, 2e74360b-b3ff-46bd-8bf0-99c8cdaecfb8, 2e8ed220-8e90-4276-a06f-9866abf6168b, 3043a3d1-fd37-46bb-9689-84c579bc5ed0, 307fc873-0570-4d15-be79-16c4a11f0f8a, 33878fb7-3765-4548-b556-50e097d16897, 34ffce7c-6e6c-41d0-a6fa-19a30b2d7fd5, 3862ff44-b148-41b9-a1e6-9c718e79a705, 3922d525-2a33-493a-9bf0-cf10bc3d96a3, 3ace0130-93f5-4c15-b1c0-10906d59f83c, 3ba7e662-9521-4544-8438-79c7cbb5d20b, 3de736a4-23ff-48c8-8b48-e7110b3a31de, 3f3067ee-c6ad-41d6-bb0b-d677729a95c3, 46b64df5-6089-48e5-b8db-cfee2bfd96c9, 480991ea-d960-425f-9754-d565d103a40d, 48944b93-9618-498b-bb4f-e411983b39fb, 48d923d7-56f2-4aa7-91fd-469ed4a58cb0, 4b4866aa-1291-4709-9a38-952a223bdefb, 4d2d886c-a71e-4c8f-823c-4368e04dd2aa, 4e78f4a9-fd59-4d59-b74f-8e8448eca2f2, 4eb154cb-9a2a-4046-a5bf-01d4c87ace52, 518f3f36-9c22-4665-b5e9-b6bb3d624232, 51b35d7b-a4f1-465f-8fcc-d902e9c490d5, 5265ef4b-16e5-401a-9380-c059a01c98e7, 5294327e-b696-4239-8a44-b1e215589d16, 53d6e47f-9d01-4d52-a1fa-ca29925ffc04, 5544d194-c65c-4d5b-9c97-866e5bf7228a, 5c573f1a-ad21-4dd1-a8e7-cd4204c53d1a, 5e648bd4-4895-469c-9b7c-20b92a692bc2, 5ebf7e4e-e130-466e-84f7-ac9059535a9e, 6148d989-7047-466c-ba18-7d090de9b6e7, 618d0474-3f27-4ac8-8932-60b16a9217a5, 62d72d04-57f1-45b8-84b0-0f33eb18c588, 630f4883-3184-4b69-bbb9-3de3ff30d6d1, 64d56151-3e69-4eb8-97ef-996e4b891699, 69d82d3d-c576-4608-a35e-3bdbebf85ddd, 6b17aa08-8de4-4fe8-be45-f8afcc5f8ccc, 6bc3f70b-f058-40cf-8dee-6d46a1f13a7a, 6c31cd40-145f-4022-91da-652367f4ddd2, 6c402960-d417-4e87-8d18-cb81f3915953, 6c62db05-a087-4c1a-a4a3-2e3aeb43325f, 6d41d46e-efd7-4be9-af9a-59a6fbbce18d, 706ebfa9-470c-415d-b96c-856c240c7392, 71442073-953c-49f8-9e2d-cc01c7e046e0, 7489dda2-2f07-484b-8e9b-37255936b070, 7499225f-3dc5-42c1-bea6-e69543d0aacc, 77105a57-bddc-4a3c-b120-4ac2e0a827db, 783c54c2-055d-41bd-b3d7-91084323d879, 785b4084-740b-46c7-931d-13ba78c12ce3, 785d4981-8453-45da-ac92-ab40630608b9, 79bbb762-6f5f-4b8d-b158-c0b058fb58f2, 7a0fc42f-a4c6-456c-aca1-94636d4bb91e, 8310449d-d2e9-4473-9712-556609394cc3, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2, 84939911-b1c5-47dc-8af7-cbc757d90993, 8bc0109f-d17f-4051-a652-f7d04022e203, 8f6bcee4-b9e3-46a4-a593-b4684eacbd82, 8fa256b9-8a0f-4bab-8554-8b4427afa86b, 907831d7-3ddd-44ff-8a04-99cdc41d0575, 90dc1cc9-aded-4911-9ee4-65a39f70371b, 91112eab-b778-46f9-97bf-950ef35b5601, 91cc567e-9493-4468-b8c9-2e8f689cfc31, 93aa9cd9-09e3-4085-916a-fa2b9242a992, 9426cd67-722c-4c7d-8a20-da64a770caf3, 9560fbb4-35e6-425b-ac7a-418c0f0f963c, 96f0ac8e-a0ac-48bd-90d4-2f7a0ff43265, 97822b50-f841-4192-a95e-090c6c27d91f, 9782db1b-c724-4e7f-a41f-6ab99e1fd9c1, 9848a8ed-e93a-43b6-b3a9-fd4b0e5bc522, 9a348f6b-f394-43c2-8405-6822c27e0522, 9c6b819f-900e-4010-8740-d91be64cae89, 9eb452a1-8d86-47c4-934c-81023ac8515c, a1e24c1b-cf87-47ae-9329-b0c224cb804a, a1f4c76f-deab-4a3b-8b46-b26511231e76, a2528032-c34b-4b0f-b871-18a1271a5920, a40fe54d-a5b5-4c4c-b313-9ec4c12f3b6e, a52fbb9b-5972-4700-9f95-2df90f010863, a734ba20-0a97-4902-b8ce-c3b8faea1253, ab2f75be-6df0-4192-81be-b311ee99efec, ae3f145e-fd1c-4aeb-8c9d-0a537cf45066, af0f63e9-2231-44c8-acfc-de4b8cb1a495, af41611a-2259-4610-a66b-e43b97958d0f, b3127ab6-133c-4161-8bb9-23043f660017, b45bfeff-9b8c-45e7-8b89-c621457059f6, b46fcf47-6f72-4e38-aade-1732e33d65df, b52115af-2758-4b0c-93da-cc77a1b8b409, b5a41f8a-a3c6-4c50-9fb5-1c7e30f903b1, b5abdfc3-06a3-4012-957c-4d1a138d2d31, b8752044-d968-4067-ae9d-fbb4a73739e3, b8fa6a0a-0aa3-44ba-9b58-781b3d450e7e, b9387c93-1bd0-496a-8813-57755f09d0e3, ba26e4d3-6ca5-4770-972d-cbc470ccba61, bb2c9f2a-624a-4cd2-829c-d034979ed761, bf32b32b-298f-4422-aaf5-57ffcb55144f, c1b1e69d-cabd-4895-b9a3-7a47a22b36b6, c34e7298-582c-4031-8b5b-8235b9c9c521, c3f2d83c-661d-4393-859c-ac1d719c8d29, c8f14c57-3173-4cbc-8949-a04cf76e4850, c9ee3c1c-281d-474a-b1d3-f49420e17527, c9f54d0e-3072-48e8-99cf-124c5a5994b5, ca057c50-36da-491a-9cb1-ebe7fa54e8cc, cb262672-537a-44ec-a622-fa600f539456, cc609e5c-f1be-4b41-b7d7-5b1310816448, ccca3827-0ad9-4775-9e35-b246467a5aca, cd7b2d74-7ebd-45c9-83b9-256206376eb2, cdc03c01-a15f-4033-8fc0-9bc72cb1ac59, d0e64d65-7b74-436a-8288-bc62fae72aec, d20b2409-fcc9-4f50-bfdf-2162308d137e, d571f779-c54c-4d0b-9db3-33523e692ca1, d5b98e9d-c0af-405c-b977-d2afbf72bc8b, d5e04adf-206a-4770-b401-af239b8b9c10, d8c49b7c-e0b8-4615-8e4a-259967d630a7, dba389c4-d911-4555-bfd5-f9a5738d05e5, dcfa6a36-99d2-4a28-a171-8d2d21477bf5, dd334dd2-a314-436b-8c3f-9a1761f084aa, deaeca31-e65d-4291-96ec-b5449f4e627e, e364f237-5a51-45d2-86e1-ec66f13a3086, e4036ed3-922f-4397-a08b-2149f50c3869, e70eb84c-5703-4ac4-8fe7-aacb15f3a0cd, e9c82bc1-47e1-46cb-905e-94dde0953381, ebdf3e96-c538-49fe-8bfd-4888e8325ef1, ec1764c7-4067-4231-8df2-e2c5f3741879, ece25393-46b6-440b-8ab1-23757abd911f, eda10b43-41d6-440d-8fd2-04e3be8cd85b, ee0bfdd4-0e25-4733-a4af-7383a76f7599, ef017b9e-6f4f-4e77-9961-58c92f2409fe, f021dc63-fe37-4403-810d-20de8a771408, f1e9368d-e1d0-4510-b6f1-20a1376ed10e, f5211b9c-5c7a-4340-9a52-a3fedb7f7f43, f63ee4c5-5d34-450a-8963-2f7865d0141e, f8ae2142-85ac-48d1-b3b6-c9b7817cb6ab, fb175f56-ee3e-4b86-b29a-3b75ebb632a3, fb1ea6fb-41ef-4c14-b901-5527a9158679, fc697ea8-832d-4337-aeb8-bcf736f032cc, fd208c31-e733-40df-8b46-0fa9fb066c61, fd6f1e0d-9e73-4e11-b8d2-84e0cb528d66, fe5332a2-116b-4c6a-8d46-cf04fe28070a, fefc8220-fa6e-4cfb-baaf-7b8479cf0902, ff4219b9-de03-44d9-8b8b-5633442c93ee]
173441 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 187 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet
173471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet
173471 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 187 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet => [01512de1-8ed9-4bed-bf4e-69ca99ec56d1, 01a218f0-b417-4af6-b953-92a30c23bc30, 02464d1d-9801-41bc-b0cd-2c3999eb3a6d, 0364a8fa-ccb5-4701-bb35-53ec115709da, 03ab3e0a-9a6d-4379-b2e6-54b48bf404b8, 0447dfdd-f159-4791-abd9-c7625acee5a6, 05464664-15f2-4b79-82d9-d44c9ef41d75, 05db492d-0620-403f-bbba-129244b7f51b, 061a1061-b1fe-4952-b918-96ade75604bf, 0a7d5812-d573-432a-9721-142e71396b06, 0b0a7c49-6376-42b6-8bd4-77ba445fc233, 0b755d5f-19ed-46ad-983d-f3355023f56f, 0d425b8c-d077-4b27-ad0f-727cdb70b748, 0de278eb-96f0-44af-9459-6ea1930230d3, 0e6a54bb-94c1-4ff2-a26f-ee25e6168eb4, 104677ec-da2e-4c40-9233-b38e8758d01b, 104bda19-2910-40db-aa52-13349424f24f, 10ee7fda-8515-4638-aaad-810c16c0fd8e, 115fa76b-bd6b-4856-98f3-4abe101d784a, 11f5a22a-64de-41bf-baa1-0c5ca74ce638, 1402f23e-31df-4a75-a203-d96b4325a0c6, 15cbdd52-ccb5-448b-be60-91c840beda3e, 164240d3-977d-4bef-816b-02d707c575ae, 16d41bdf-322b-47ab-98b0-d7cd114bad58, 173142b1-6a0f-48fe-878f-b213094f4465, 17b1d61a-7163-4ac8-a650-f1035eb72bc4, 18cbee7b-d3b3-4f84-8eb9-17f0fe475d17, 18dde726-5fe4-408e-8fd0-cce2a0536dec, 1ae814a0-2cba-454d-ac96-988d0b97a941, 1d1268cf-0fd8-4642-81f6-f4e7cfb621e9, 1e29d1eb-b86e-4fa0-8fb0-eb3209031e21, 1eab9338-29c1-42b7-bad9-ec4a5c590057, 202f7945-7129-49e5-b4f0-e75010487085, 21ae6afa-e9cd-4a3b-8c1d-a1f475037a9b, 234c8c6a-c58f-4a03-8ca9-5443b053b398, 24c7902d-c70a-42b1-9245-96cbade6bb7e, 2727b590-29a8-4582-8597-bc34e9c1ef98, 27d3a16a-8572-4716-9252-d4dde9cc62ca, 293dcaa1-68b4-4e5f-bb91-cafe1266b35c, 2b12412e-0a5a-4d80-b124-5d0635e14c93, 2db9271c-b77e-42a0-bc5f-39d66adc1649, 30e7a7de-3694-453c-a13d-99c46ee2a393, 33d256a2-ba0f-40b5-86c2-491b526a15cc, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 37883702-6ed1-4cdb-8935-09788186c539, 3a4d853e-f805-4507-8adf-ca0e72f41ee5, 3add04e3-7d1c-4caa-a834-4143f91a0695, 3b87201b-3e02-4647-b758-e380cccdb55c, 3c0c7809-3170-4ba3-9db7-b34bfe7d6f00, 3d342613-fde4-4f10-ae25-5b84edffc055, 3ddc6222-f7e9-4986-9bb4-4f0c7659b125, 3fa4d3b4-8a62-45e0-8202-3e8dd146147a, 40545381-defb-491d-957b-ad0d83cf803d, 42042347-5cbf-4758-b3e8-a8381506635f, 43910807-eba8-479c-b696-a38e25e98cf3, 4a91673d-d511-468f-9d35-c43190d572cc, 4b467642-4ae2-438a-9cba-5800b28f3d74, 4b7e9778-94ab-4180-9955-469a35555602, 4e541e57-1b04-4a8a-86b0-db9675f0c590, 4fcb0d5a-4e0f-4d01-bdd7-9ee3ddb098c7, 4fdb95af-f3db-409f-8e01-e656b2755ecb, 50566024-5b7b-4633-bb83-7bc8ba6a1478, 52f6ff56-47a2-417f-87e0-0f797c24be5a, 53a7f1f5-2fd1-48a0-9c30-b24124a1dadf, 5567d833-c832-40cf-86b8-a58a8203bfa0, 56d785d6-62bc-45cb-930c-2ac086065871, 5903081e-4156-46a7-8226-cee987598b48, 59f1f6d6-5fb6-4230-8c0a-c3388be95a60, 5cf2193e-6ec6-42ff-b4a6-794ec9185ec2, 5ee45a16-73e0-4337-866f-a0ee20f4beac, 5f2dcc8e-0e05-4a92-b223-1b1d2edf1ea2, 6044d313-3c8c-422b-b2ee-27d8328a789b, 6340a74f-9e6a-4c08-a56e-29d1d2547141, 64912811-f837-4e95-b7ab-152d4bfd70fd, 64e4001d-2264-4964-a06f-862bc52c9806, 674c6473-40a0-4a7b-b6c2-49f76a1a615c, 6754c350-3a20-475b-a70d-110675feba81, 6b6220e5-72a2-4854-b044-b67de954d635, 6e63ab50-4e8d-4e96-b48e-f4643967680b, 72f614d6-b994-464b-a1fb-a6064628b2db, 72ff13e0-b94c-45f7-ae73-72f003203d9c, 75aaa81f-b91a-44eb-a3c0-cceda4d34022, 75afc7b6-0bc8-4c9c-bc9e-f08251dc7c24, 75ef6414-590c-41ab-b755-b7c314f5cc5d, 7d343007-fc22-4ee2-85b7-622224f9c6d8, 7ecdd3f7-b185-471a-9eb7-feb549300ca0, 7ff832c5-86ae-4cb6-980f-4e2858e18be9, 8228cb88-6f38-431a-baec-0f92ee567ad4, 83309d65-bd8e-49d1-b330-f2482f0b6052, 8395e375-4db9-407f-aad7-62d57d27a732, 83da791a-ae77-401e-8ddc-bcbcd969b9af, 84f4d7ec-942d-4600-92c5-2fea263c56ee, 8b66485d-2ecd-4948-bb99-1246463b6bfd, 8bf7396e-5514-453f-a824-964947126eb0, 8c96f50f-c411-4c2c-9cd9-20929ff32312, 8d1cc486-edd3-468f-9d31-f891989b86a5, 8dfdaee0-bb98-4336-a563-eedfb5b24555, 8fe14b54-a1a6-4507-b408-b93b5847e0a5, 9390394f-68c5-471a-ad9b-c58c67c0652e, 93c75b4a-c4c7-45d3-8d70-1cc72186e8d5, 98989271-7587-4fda-aff7-88eab6a5fe72, 9948c10c-59eb-4f0d-b9a8-02a162aed679, 9a0ddb9b-776d-4efd-8211-236ea139e693, 9a9ecd96-b67e-48ed-9fd4-e9a36a7b7bc0, 9b7dc4d4-7162-48fb-a56c-a441953258fe, 9d2eb146-ca9e-4c6c-8ea6-8240639d66f0, 9f39283b-d2df-4f90-8bc6-0d17f2bbabe4, 9f617d01-c6b6-4bbe-908e-4bc20e28581c, 9fa2f132-6eb2-43d4-b134-68e76c99dd16, a199891d-5d17-473b-9ebe-ba21591b7f6b, a294e581-1966-458d-b33e-5bccbccb925e, a3d03133-eb03-48a8-9ec4-cd27da5d603b, a3f27831-413f-416a-ba58-cd1887f501c0, a5827aef-0ddb-4bb9-a421-660b1b457f4c, aa23bb3f-3fef-46d2-917e-b36dfa1888e1, aa34367b-16dc-4931-b7db-69ba905f8cbb, abffe849-7922-413d-aefa-e8095d2873f3, acff3468-f5b5-408d-bf50-50e48b040429, adaba54a-9aca-412a-a5dd-d028e024350f, aef24ed8-2290-4631-869d-90d31c91a347, b06fea13-bea3-46e5-bad4-9483a49fce2b, b199eb32-886f-4103-bb33-b0fdfe9ac138, b24b3fb2-c8f9-4e87-8fa0-ab73febd0ec5, b2b22a64-8285-4c5c-b599-687405fc386f, b2e39bec-dd5c-47f1-a001-8ef9daa65115, b68908c6-7304-4256-a45a-29cd9abc826c, b73f67e7-59d5-4579-8fc3-69ba009af453, b786942e-f5f5-4bc2-8c8f-d1f1b43231e5, b794171e-6ce7-4aba-81a6-5a70651cb1a0, ba649618-807e-4cef-98b2-5da0382d88d5, bab43408-91e0-41bd-a302-a3519f410c5d, bb1c04ff-1a89-4a56-9153-fedf331c60c5, bb8ed0aa-0dae-46b3-ac3d-bb0edc2a7a13, bb90e5f9-3951-4531-804e-17b9fa29a1ec, bca28757-ea19-428c-ab4b-98ea79ee26fe, bcfdfd5e-66a0-44de-adac-834f13ef6ceb, c0017f26-2826-4ed9-8874-301597374df9, c3d6b256-a3a5-4712-ace5-a90e2dc0555a, c43f5955-e6b4-48c6-a38a-d4ce748f5547, c558f0be-ac94-463f-b58c-58440e6de5b5, c5fc4430-1120-434b-b6f5-357284971829, c79061e9-96ec-4451-ba5d-745fc7e58e24, c8b90ac2-5d77-463a-85d6-4276ea12aafd, c8bd0fb3-83da-4a8d-ae24-48e1f75fb93e, ca6bf156-a0ae-4a6d-8a11-66e648f720fe, cb1b1172-3090-43f6-b49c-67593dc26245, cbe119e2-40f7-46aa-a7b4-0594799d5921, ce39871f-1f8e-4fd8-b691-38e6e80d8bec, ce8c0040-e46f-4df2-81c4-bdbe5c3d579c, d07973e2-a721-4743-bbdd-4170d6b87ea5, d46281e8-6e09-497c-9146-39d352557805, d4700132-d150-416b-bbac-b9a1071e1a0e, d48edb4f-d99b-4730-9087-cb65c861a877, d52b9240-49ef-455b-9be4-b5162a5662cf, d9cc4b41-2a9f-483b-8ed4-0787dbce6dba, daadc1b6-8d1b-45c8-ae37-68c83f345c73, dda8c37a-117b-4f01-b15d-dfb1a0b4e78a, dde92cc6-f23f-48dc-be63-7e5dd0bb0370, e1e460b2-1b5c-404d-82ec-11bd3e2a9caf, e2d68a7a-74c3-4427-8b9b-62701771e89c, e42eeab6-e9b9-46b3-92b0-12a3aaa6cf69, e494c00a-7199-4a21-828a-762bfbf8eed9, e587dc4b-8be5-4688-91a1-c7b2030c2fcf, e6a29185-1710-4bc3-9127-884ce0e885d3, e7883d11-694a-40e5-beea-c492b0349ced, e81f69e7-1820-468a-92a8-091913d52f39, e87934ff-6de4-41f4-a04f-fa0af88f8c8f, e884ea4b-6551-4e73-b023-a075c0a6b724, e9a6679f-bcdb-42cc-a57d-ee72d7da4d8d, ea2e1964-d8f7-41ff-8571-ace0111c679e, eb130a09-c83c-4378-8b3c-dd96ed5fd558, eb4c9cd9-b84c-41cf-a13d-5840ba5bd271, ec964995-6121-4738-b89f-ec7c80be05c5, f2850232-8178-456a-a4b0-b40ccf9b659d, f51cbc46-57b1-4107-9155-16fb137f3ee1, f57a98ea-b83f-4003-b562-ca72070fe5ac, f5b53eba-e913-4ea7-a0a6-f1be6fa8dab8, f62d392c-8196-460d-9552-22e50b5b3d00, f7072f27-5552-4628-8f2b-649fed0cdfce, fa3181ef-b3cc-4ec2-bfb5-e1c43bab4dc9, fb518ab9-1c91-4ab7-9ea7-7b12311a0283, fb886a63-a39f-4fa2-adfa-1a61ec03dd7f, fd332c8b-c4bd-47f8-a739-f5f48cc9a2b5, fdaf0271-616b-4733-967c-363fe0ac40f9, fddd96ed-87f0-42cf-9f88-9bb6773aee16, fde04fef-10ee-460f-8c21-1f3f33ed2da5, fe7aefee-cc61-41dd-8333-4371e9adbc73]
174671 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195425
174921 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 96
174921 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
175061 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet
175066 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet
175066 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_1_20200319195422.parquet => [01a36dec-30ba-418a-90bc-8c080bdeebec, 056e809e-11f6-47dc-995d-94e38a79720d, 11669b7c-79ca-4430-b257-43940c47bf9f, 1892a55c-503e-466b-abd6-133472d20913, 1c648096-66fd-40d0-ae66-14957bc449ae, 29071a5b-a9ca-4b56-85c7-7bc15efb785b, 3e598b8c-5dc7-480d-ae42-a4df0d5831f7, 5256bff1-8c78-4b91-ac93-30adb15de779, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 5dec3bee-c8bc-492a-a096-091995617dd7, 6d43a2a6-061f-4312-9b37-cc24f38c1c16, 7044ffc5-45fd-425c-8a6f-425296b646ff, 77a74145-a4cb-44f5-bf56-a5eb0dedf57a, 7c03eeb2-034e-4537-9904-c62fc9547baf, 81c2d2fd-2eef-4cda-a599-79c14b06185a, 8d4646c7-6b04-4194-bbb3-d9a9562c70d3, 92176651-d67f-47b7-9cd9-6c952adc9bfa, a1b91bdf-bd2b-4c7a-ab54-961828083278, a41034e2-6135-44f1-ad4d-6a71de909471, a5d38922-1d5e-447f-aea1-ed9651cf4949, a9f9537a-83ff-4398-910c-8bdea3e2a422, c062cf4e-5c5b-4af5-8b57-67006ce51dd4, c1499be1-3e6d-4bc5-ba58-c1e09cb89622, ce34786d-9ca7-4f3d-ac1f-6861af309e3e, df76930f-b8a2-4ab4-8950-c942a3414504, e7d75d12-35c8-442c-b19d-b9da41e0a0df, e8ebede2-b310-4399-b76d-dd841c52aad5, e912a426-97ed-47f6-a015-9d62cb66371c, ed816151-b8c8-4051-9bd9-b5809fdde18e, f0f6ca3f-0e36-47aa-adb7-b41e6ae490fd, f4036c9d-bf5c-4efa-9a7b-afcddc7064f5]
175076 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
175081 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
175081 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet => [0ae33ffe-73c5-4a58-a7fc-55c7f83ca7ef, 0e0c77f4-9934-4a53-9efd-bfc66cbb3de2, 0e608d43-82f0-4deb-b407-f560496e1315, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 1de5b679-e077-4950-b0ab-b03042e6969e, 2d5ee102-e378-44fa-9680-18f66b1f403e, 3043a3d1-fd37-46bb-9689-84c579bc5ed0, 33878fb7-3765-4548-b556-50e097d16897, 3922d525-2a33-493a-9bf0-cf10bc3d96a3, 3f3067ee-c6ad-41d6-bb0b-d677729a95c3, 48d923d7-56f2-4aa7-91fd-469ed4a58cb0, 4e78f4a9-fd59-4d59-b74f-8e8448eca2f2, 5c573f1a-ad21-4dd1-a8e7-cd4204c53d1a, 5ebf7e4e-e130-466e-84f7-ac9059535a9e, 62d72d04-57f1-45b8-84b0-0f33eb18c588, 6bc3f70b-f058-40cf-8dee-6d46a1f13a7a, 706ebfa9-470c-415d-b96c-856c240c7392]
175110 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
175115 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet
175115 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_2_20200319195422.parquet => [71442073-953c-49f8-9e2d-cc01c7e046e0, 91cc567e-9493-4468-b8c9-2e8f689cfc31, 97822b50-f841-4192-a95e-090c6c27d91f, 9848a8ed-e93a-43b6-b3a9-fd4b0e5bc522, a1f4c76f-deab-4a3b-8b46-b26511231e76, ab2f75be-6df0-4192-81be-b311ee99efec, af41611a-2259-4610-a66b-e43b97958d0f, b8752044-d968-4067-ae9d-fbb4a73739e3, bb2c9f2a-624a-4cd2-829c-d034979ed761, c34e7298-582c-4031-8b5b-8235b9c9c521, cdc03c01-a15f-4033-8fc0-9bc72cb1ac59, d20b2409-fcc9-4f50-bfdf-2162308d137e, d8c49b7c-e0b8-4615-8e4a-259967d630a7, dba389c4-d911-4555-bfd5-f9a5738d05e5, e9c82bc1-47e1-46cb-905e-94dde0953381, f1e9368d-e1d0-4510-b6f1-20a1376ed10e, fd6f1e0d-9e73-4e11-b8d2-84e0cb528d66, fe5332a2-116b-4c6a-8d46-cf04fe28070a, fefc8220-fa6e-4cfb-baaf-7b8479cf0902]
175125 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet
175129 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet
175129 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195422.parquet => [02464d1d-9801-41bc-b0cd-2c3999eb3a6d, 0447dfdd-f159-4791-abd9-c7625acee5a6, 061a1061-b1fe-4952-b918-96ade75604bf, 0d425b8c-d077-4b27-ad0f-727cdb70b748, 0e6a54bb-94c1-4ff2-a26f-ee25e6168eb4, 164240d3-977d-4bef-816b-02d707c575ae, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 3c0c7809-3170-4ba3-9db7-b34bfe7d6f00, 4b467642-4ae2-438a-9cba-5800b28f3d74, 53a7f1f5-2fd1-48a0-9c30-b24124a1dadf, 75aaa81f-b91a-44eb-a3c0-cceda4d34022, 93c75b4a-c4c7-45d3-8d70-1cc72186e8d5, 9948c10c-59eb-4f0d-b9a8-02a162aed679, 9a9ecd96-b67e-48ed-9fd4-e9a36a7b7bc0, 9d2eb146-ca9e-4c6c-8ea6-8240639d66f0, 9f39283b-d2df-4f90-8bc6-0d17f2bbabe4, 9fa2f132-6eb2-43d4-b134-68e76c99dd16, b24b3fb2-c8f9-4e87-8fa0-ab73febd0ec5, bb8ed0aa-0dae-46b3-ac3d-bb0edc2a7a13, c3d6b256-a3a5-4712-ace5-a90e2dc0555a, c43f5955-e6b4-48c6-a38a-d4ce748f5547, c5fc4430-1120-434b-b6f5-357284971829, c79061e9-96ec-4451-ba5d-745fc7e58e24, ce8c0040-e46f-4df2-81c4-bdbe5c3d579c, d4700132-d150-416b-bbac-b9a1071e1a0e, e6a29185-1710-4bc3-9127-884ce0e885d3, e87934ff-6de4-41f4-a04f-fa0af88f8c8f, eb4c9cd9-b84c-41cf-a13d-5840ba5bd271, fd332c8b-c4bd-47f8-a739-f5f48cc9a2b5]
175204 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=96}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=31}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=36}}}
175228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2699
175228 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
175237 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195425
175237 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195425
175597 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
175597 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
175687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
175687 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
175781 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
175789 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195425 as complete
175789 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195425
176981 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195428
177202 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 92
177202 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
177351 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195425.parquet
177355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195425.parquet
177355 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195425.parquet => [056e809e-11f6-47dc-995d-94e38a79720d, 09240cfb-082d-426b-acac-e9e4184e38a3, 4329713b-0cb7-4a39-a3d9-f766a0e52ec7, 49f08601-5506-4d51-981b-d54bbcdb441e, 4ed6d68d-a8cf-4478-a963-c0c2f858f1bb, 4fe85051-32cb-45c0-b0be-c3517eb0d3bf, 5c575b62-fb1d-4eb7-bf00-96eccacd8e9f, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 6ba7344a-ea63-4011-9f65-3c51ae4256df, 7c03eeb2-034e-4537-9904-c62fc9547baf, 7d69d833-9849-4518-8f27-50aab2d0e5d1, 87ac0a53-64e0-4520-a1e5-067421efd27c, 8a1cc4c7-4893-4cd5-885d-3109e30d830d, 8d4646c7-6b04-4194-bbb3-d9a9562c70d3, 9861c56b-ce53-4dd3-adf9-e18f4b7bc3c7, a1494460-ed65-4614-9bc6-70094d424ee3, a1b91bdf-bd2b-4c7a-ab54-961828083278, a4774175-0279-484c-903b-1473989cc381, a5d38922-1d5e-447f-aea1-ed9651cf4949, aba3570f-7e84-460c-b7c9-271b2ab4add7, c0474c5e-288d-47c0-833f-0dd68eefa29f, ceadd625-5a0e-4e63-a8e2-8431ca9fc0f4, d897a141-eac2-4e26-ae06-5c0daf3c773a, db033b2b-0587-43b8-b475-27e6f26fb369, ed816151-b8c8-4051-9bd9-b5809fdde18e, f4036c9d-bf5c-4efa-9a7b-afcddc7064f5, fd82bb16-5857-40c4-a967-7b93b0fede52]
177366 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet
177377 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet
177377 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet => [17cd6a9f-f3f0-4c40-9384-b10fdea261ee, 1a7b11ad-5cb7-4998-b50c-a09f39aa8ee6, 307fc873-0570-4d15-be79-16c4a11f0f8a, 48944b93-9618-498b-bb4f-e411983b39fb, 4b4866aa-1291-4709-9a38-952a223bdefb, 4d2d886c-a71e-4c8f-823c-4368e04dd2aa, 51b35d7b-a4f1-465f-8fcc-d902e9c490d5, 618d0474-3f27-4ac8-8932-60b16a9217a5, 630f4883-3184-4b69-bbb9-3de3ff30d6d1, 71442073-953c-49f8-9e2d-cc01c7e046e0, 7499225f-3dc5-42c1-bea6-e69543d0aacc, 785b4084-740b-46c7-931d-13ba78c12ce3, 8bc0109f-d17f-4051-a652-f7d04022e203, 90dc1cc9-aded-4911-9ee4-65a39f70371b, 9426cd67-722c-4c7d-8a20-da64a770caf3, 9782db1b-c724-4e7f-a41f-6ab99e1fd9c1, 9848a8ed-e93a-43b6-b3a9-fd4b0e5bc522, 9eb452a1-8d86-47c4-934c-81023ac8515c]
177398 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet
177404 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet
177404 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195425.parquet => [a52fbb9b-5972-4700-9f95-2df90f010863, b3127ab6-133c-4161-8bb9-23043f660017, c9ee3c1c-281d-474a-b1d3-f49420e17527, cd7b2d74-7ebd-45c9-83b9-256206376eb2, e70eb84c-5703-4ac4-8fe7-aacb15f3a0cd, e9c82bc1-47e1-46cb-905e-94dde0953381, ebdf3e96-c538-49fe-8bfd-4888e8325ef1, ec1764c7-4067-4231-8df2-e2c5f3741879, f021dc63-fe37-4403-810d-20de8a771408, fe5332a2-116b-4c6a-8d46-cf04fe28070a, ff4219b9-de03-44d9-8b8b-5633442c93ee]
177422 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 35 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195425.parquet
177427 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195425.parquet
177427 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 35 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195425.parquet => [01a218f0-b417-4af6-b953-92a30c23bc30, 0364a8fa-ccb5-4701-bb35-53ec115709da, 0b0a7c49-6376-42b6-8bd4-77ba445fc233, 104bda19-2910-40db-aa52-13349424f24f, 18cbee7b-d3b3-4f84-8eb9-17f0fe475d17, 18dde726-5fe4-408e-8fd0-cce2a0536dec, 202f7945-7129-49e5-b4f0-e75010487085, 234c8c6a-c58f-4a03-8ca9-5443b053b398, 2727b590-29a8-4582-8597-bc34e9c1ef98, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 3b87201b-3e02-4647-b758-e380cccdb55c, 43910807-eba8-479c-b696-a38e25e98cf3, 4b467642-4ae2-438a-9cba-5800b28f3d74, 56d785d6-62bc-45cb-930c-2ac086065871, 5903081e-4156-46a7-8226-cee987598b48, 6044d313-3c8c-422b-b2ee-27d8328a789b, 6754c350-3a20-475b-a70d-110675feba81, 84f4d7ec-942d-4600-92c5-2fea263c56ee, 8d1cc486-edd3-468f-9d31-f891989b86a5, 98989271-7587-4fda-aff7-88eab6a5fe72, 9948c10c-59eb-4f0d-b9a8-02a162aed679, 9fa2f132-6eb2-43d4-b134-68e76c99dd16, aa23bb3f-3fef-46d2-917e-b36dfa1888e1, acff3468-f5b5-408d-bf50-50e48b040429, b68908c6-7304-4256-a45a-29cd9abc826c, b794171e-6ce7-4aba-81a6-5a70651cb1a0, ba649618-807e-4cef-98b2-5da0382d88d5, c43f5955-e6b4-48c6-a38a-d4ce748f5547, d4700132-d150-416b-bbac-b9a1071e1a0e, ea2e1964-d8f7-41ff-8571-ace0111c679e, f2850232-8178-456a-a4b0-b40ccf9b659d, f51cbc46-57b1-4107-9155-16fb137f3ee1, f7072f27-5552-4628-8f2b-649fed0cdfce, fdaf0271-616b-4733-967c-363fe0ac40f9, fddd96ed-87f0-42cf-9f88-9bb6773aee16]
177490 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=92}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=35}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=29}}}
177500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2700
177500 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
177510 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195428
177510 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195428
177861 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
177861 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
178383 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
178383 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
178494 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
178544 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195428 as complete
178544 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195428
179726 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195430
179970 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 93
179971 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
180110 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195428.parquet
180115 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195428.parquet
180115 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195428.parquet => [0004db58-b843-489e-9f8b-644952c44546, 017842aa-9d63-4bf1-b9df-653409f5a9e7, 02ba262f-ee9e-43ab-975c-ae401885a13d, 09240cfb-082d-426b-acac-e9e4184e38a3, 2a65d2ee-5286-474c-a499-8b25250fccbf, 30d1f40a-e1e9-41c2-afea-fb55eb3f4751, 30d26a27-1d7a-4d9f-82a2-c9daa3b2c90a, 51d04eb7-6148-4118-ab27-5d4f07b3da8e, 54d0e11a-752d-460d-937e-e20111528a24, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 667006f5-0fd0-4006-b58c-025fe8ac33b1, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 6d43a2a6-061f-4312-9b37-cc24f38c1c16, 78a5d9c8-6d3d-4964-84f5-1412dfac8346, 8bb7df69-5482-4bda-8266-ed992618a218, 9ef2fc8f-dce9-475d-a7da-a85742fe7c54, a1b91bdf-bd2b-4c7a-ab54-961828083278, aba3570f-7e84-460c-b7c9-271b2ab4add7, c118b025-5816-4abf-b176-cc4a17289c0f, c1499be1-3e6d-4bc5-ba58-c1e09cb89622, d29ea52f-8bbc-4ad7-9e4a-6a62d0dc3ce3, d51be0c9-7865-472a-9bd5-3fec5a3d0f31, e4318c37-dfb3-4ccd-8a10-e8ffc11b70ab, e7d75d12-35c8-442c-b19d-b9da41e0a0df, f13b3d80-671e-4fc7-8e54-76b252b190f7, f393468e-731f-4594-befc-bac0fac6c66b, f7aaca79-90a8-4977-89cf-818919b9dd38, fa45e310-ddcb-4c43-8d01-d7b77f64001b]
180125 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet
180129 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet
180129 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet => [045596b1-c6d4-4505-bd30-53f2e26a38d7, 0e608d43-82f0-4deb-b407-f560496e1315, 15913463-f3ab-47b2-a5d5-75719350d2c7, 2b670ba6-606c-42f0-981f-3615917d7574, 3922d525-2a33-493a-9bf0-cf10bc3d96a3, 46b64df5-6089-48e5-b8db-cfee2bfd96c9, 518f3f36-9c22-4665-b5e9-b6bb3d624232, 5265ef4b-16e5-401a-9380-c059a01c98e7, 53d6e47f-9d01-4d52-a1fa-ca29925ffc04, 5e648bd4-4895-469c-9b7c-20b92a692bc2, 5ebf7e4e-e130-466e-84f7-ac9059535a9e, 69d82d3d-c576-4608-a35e-3bdbebf85ddd, 6bc3f70b-f058-40cf-8dee-6d46a1f13a7a, 706ebfa9-470c-415d-b96c-856c240c7392, 79bbb762-6f5f-4b8d-b158-c0b058fb58f2, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2, 8bc0109f-d17f-4051-a652-f7d04022e203, 91cc567e-9493-4468-b8c9-2e8f689cfc31, 9560fbb4-35e6-425b-ac7a-418c0f0f963c]
180147 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet
180151 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet
180151 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195428.parquet => [a2528032-c34b-4b0f-b871-18a1271a5920, cd7b2d74-7ebd-45c9-83b9-256206376eb2, d571f779-c54c-4d0b-9db3-33523e692ca1, d8c49b7c-e0b8-4615-8e4a-259967d630a7, deaeca31-e65d-4291-96ec-b5449f4e627e, ef017b9e-6f4f-4e77-9961-58c92f2409fe, f021dc63-fe37-4403-810d-20de8a771408, f8ae2142-85ac-48d1-b3b6-c9b7817cb6ab]
180162 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195428.parquet
180166 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195428.parquet
180166 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195428.parquet => [02464d1d-9801-41bc-b0cd-2c3999eb3a6d, 0a7d5812-d573-432a-9721-142e71396b06, 11f5a22a-64de-41bf-baa1-0c5ca74ce638, 18cbee7b-d3b3-4f84-8eb9-17f0fe475d17, 1ae814a0-2cba-454d-ac96-988d0b97a941, 1eab9338-29c1-42b7-bad9-ec4a5c590057, 202f7945-7129-49e5-b4f0-e75010487085, 27d3a16a-8572-4716-9252-d4dde9cc62ca, 2b12412e-0a5a-4d80-b124-5d0635e14c93, 33d256a2-ba0f-40b5-86c2-491b526a15cc, 3add04e3-7d1c-4caa-a834-4143f91a0695, 3c0c7809-3170-4ba3-9db7-b34bfe7d6f00, 4fcb0d5a-4e0f-4d01-bdd7-9ee3ddb098c7, 4fdb95af-f3db-409f-8e01-e656b2755ecb, 56d785d6-62bc-45cb-930c-2ac086065871, 5cf2193e-6ec6-42ff-b4a6-794ec9185ec2, 5ee45a16-73e0-4337-866f-a0ee20f4beac, 64e4001d-2264-4964-a06f-862bc52c9806, 6e63ab50-4e8d-4e96-b48e-f4643967680b, 75afc7b6-0bc8-4c9c-bc9e-f08251dc7c24, 7d343007-fc22-4ee2-85b7-622224f9c6d8, 9948c10c-59eb-4f0d-b9a8-02a162aed679, aa23bb3f-3fef-46d2-917e-b36dfa1888e1, aa34367b-16dc-4931-b7db-69ba905f8cbb, b199eb32-886f-4103-bb33-b0fdfe9ac138, bb1c04ff-1a89-4a56-9153-fedf331c60c5, c0017f26-2826-4ed9-8874-301597374df9, c8b90ac2-5d77-463a-85d6-4276ea12aafd, c8bd0fb3-83da-4a8d-ae24-48e1f75fb93e, d07973e2-a721-4743-bbdd-4170d6b87ea5, dda8c37a-117b-4f01-b15d-dfb1a0b4e78a, dde92cc6-f23f-48dc-be63-7e5dd0bb0370, e42eeab6-e9b9-46b3-92b0-12a3aaa6cf69, e494c00a-7199-4a21-828a-762bfbf8eed9, e587dc4b-8be5-4688-91a1-c7b2030c2fcf, e9a6679f-bcdb-42cc-a57d-ee72d7da4d8d, f2850232-8178-456a-a4b0-b40ccf9b659d, fb886a63-a39f-4fa2-adfa-1a61ec03dd7f]
180245 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=93}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=38}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
180258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
180258 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
180267 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195430
180267 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195430
180565 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
180565 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
180681 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
180681 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
180793 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
180800 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195430 as complete
180800 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195430
182001 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195433
182262 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 89
182262 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
182392 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 22 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195430.parquet
182396 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195430.parquet
182396 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 22 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195430.parquet => [0dce30d9-7d25-47d9-a072-055695037788, 133f0ca9-0ccc-47a2-9ed2-5d5b4e2d0fa5, 178e80e5-738c-4775-8d8e-dac93a7598be, 17dc59a8-5fc0-43aa-b90a-3c7fca91de55, 3e598b8c-5dc7-480d-ae42-a4df0d5831f7, 4fe85051-32cb-45c0-b0be-c3517eb0d3bf, 545bcec0-65c8-4b25-ba55-51a2f8c171d5, 5abeff0a-a31a-4395-b281-e52c9b153187, 667006f5-0fd0-4006-b58c-025fe8ac33b1, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 7d69d833-9849-4518-8f27-50aab2d0e5d1, 8523598d-2e46-4d69-a51a-f22b0fd2b699, 8568448d-1fc7-4181-b40c-87adb7fc40af, 87ac0a53-64e0-4520-a1e5-067421efd27c, 990360ad-70a5-4ffb-940f-59278cd366fc, a1494460-ed65-4614-9bc6-70094d424ee3, c2813043-268e-4118-a3e3-e2365164c26d, c5cdb2af-6ff9-47fe-9b8b-4f393b36db39, d29ea52f-8bbc-4ad7-9e4a-6a62d0dc3ce3, dc12707b-a265-4d84-ac26-bdf86f4204d0, f393468e-731f-4594-befc-bac0fac6c66b, fdec1d4b-451e-494a-9d6e-f2f3941fdb07]
182407 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 23 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet
182410 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet
182410 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 23 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet => [0ae33ffe-73c5-4a58-a7fc-55c7f83ca7ef, 0e0c77f4-9934-4a53-9efd-bfc66cbb3de2, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 3043a3d1-fd37-46bb-9689-84c579bc5ed0, 3de736a4-23ff-48c8-8b48-e7110b3a31de, 3f3067ee-c6ad-41d6-bb0b-d677729a95c3, 4eb154cb-9a2a-4046-a5bf-01d4c87ace52, 51b35d7b-a4f1-465f-8fcc-d902e9c490d5, 53d6e47f-9d01-4d52-a1fa-ca29925ffc04, 5c573f1a-ad21-4dd1-a8e7-cd4204c53d1a, 630f4883-3184-4b69-bbb9-3de3ff30d6d1, 64d56151-3e69-4eb8-97ef-996e4b891699, 6d41d46e-efd7-4be9-af9a-59a6fbbce18d, 79bbb762-6f5f-4b8d-b158-c0b058fb58f2, 7a0fc42f-a4c6-456c-aca1-94636d4bb91e, 8310449d-d2e9-4473-9712-556609394cc3, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2, 8f6bcee4-b9e3-46a4-a593-b4684eacbd82, 91112eab-b778-46f9-97bf-950ef35b5601, 9eb452a1-8d86-47c4-934c-81023ac8515c, a52fbb9b-5972-4700-9f95-2df90f010863, b5a41f8a-a3c6-4c50-9fb5-1c7e30f903b1, b8752044-d968-4067-ae9d-fbb4a73739e3]
182440 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet
182463 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet
182463 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195430.parquet => [b8fa6a0a-0aa3-44ba-9b58-781b3d450e7e, bb2c9f2a-624a-4cd2-829c-d034979ed761, d20b2409-fcc9-4f50-bfdf-2162308d137e, d5b98e9d-c0af-405c-b977-d2afbf72bc8b, d8c49b7c-e0b8-4615-8e4a-259967d630a7, ec1764c7-4067-4231-8df2-e2c5f3741879, ef017b9e-6f4f-4e77-9961-58c92f2409fe, fb175f56-ee3e-4b86-b29a-3b75ebb632a3, fb1ea6fb-41ef-4c14-b901-5527a9158679, fefc8220-fa6e-4cfb-baaf-7b8479cf0902]
182474 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195430.parquet
182478 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195430.parquet
182478 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195430.parquet => [0d425b8c-d077-4b27-ad0f-727cdb70b748, 104677ec-da2e-4c40-9233-b38e8758d01b, 104bda19-2910-40db-aa52-13349424f24f, 16d41bdf-322b-47ab-98b0-d7cd114bad58, 1e29d1eb-b86e-4fa0-8fb0-eb3209031e21, 24c7902d-c70a-42b1-9245-96cbade6bb7e, 2727b590-29a8-4582-8597-bc34e9c1ef98, 33d256a2-ba0f-40b5-86c2-491b526a15cc, 3a4d853e-f805-4507-8adf-ca0e72f41ee5, 3ddc6222-f7e9-4986-9bb4-4f0c7659b125, 40545381-defb-491d-957b-ad0d83cf803d, 50566024-5b7b-4633-bb83-7bc8ba6a1478, 56d785d6-62bc-45cb-930c-2ac086065871, 59f1f6d6-5fb6-4230-8c0a-c3388be95a60, 674c6473-40a0-4a7b-b6c2-49f76a1a615c, 6b6220e5-72a2-4854-b044-b67de954d635, 6e63ab50-4e8d-4e96-b48e-f4643967680b, 8d1cc486-edd3-468f-9d31-f891989b86a5, 9390394f-68c5-471a-ad9b-c58c67c0652e, 9948c10c-59eb-4f0d-b9a8-02a162aed679, adaba54a-9aca-412a-a5dd-d028e024350f, b2b22a64-8285-4c5c-b599-687405fc386f, c0017f26-2826-4ed9-8874-301597374df9, c5fc4430-1120-434b-b6f5-357284971829, c8bd0fb3-83da-4a8d-ae24-48e1f75fb93e, ce8c0040-e46f-4df2-81c4-bdbe5c3d579c, d46281e8-6e09-497c-9146-39d352557805, d4700132-d150-416b-bbac-b9a1071e1a0e, d52b9240-49ef-455b-9be4-b5162a5662cf, dda8c37a-117b-4f01-b15d-dfb1a0b4e78a, e9a6679f-bcdb-42cc-a57d-ee72d7da4d8d, eb130a09-c83c-4378-8b3c-dd96ed5fd558, eb4c9cd9-b84c-41cf-a13d-5840ba5bd271, fdaf0271-616b-4733-967c-363fe0ac40f9]
182548 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=89}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=22}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=33}}}
182566 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
182566 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
182574 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195433
182574 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195433
183101 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
183101 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
183286 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
183286 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
183425 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
183433 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195433 as complete
183433 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195433
184648 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195435
184898 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 83
184898 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
185020 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195433.parquet
185025 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195433.parquet
185025 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195433.parquet => [01a36dec-30ba-418a-90bc-8c080bdeebec, 02ba262f-ee9e-43ab-975c-ae401885a13d, 056e809e-11f6-47dc-995d-94e38a79720d, 11669b7c-79ca-4430-b257-43940c47bf9f, 3e598b8c-5dc7-480d-ae42-a4df0d5831f7, 4fe85051-32cb-45c0-b0be-c3517eb0d3bf, 51628f50-8911-414a-b796-df02c976ce2d, 5256bff1-8c78-4b91-ac93-30adb15de779, 53f42054-8ea6-4229-bb7e-57783889ab42, 545bcec0-65c8-4b25-ba55-51a2f8c171d5, 54d0e11a-752d-460d-937e-e20111528a24, 55f54db3-4b9e-4f06-b3fe-25a19a6a8edb, 5c575b62-fb1d-4eb7-bf00-96eccacd8e9f, 754cad80-8eaf-4054-b212-f50b6803bbdf, 8af92b2c-e601-484e-bfe9-2adc060362a3, 990360ad-70a5-4ffb-940f-59278cd366fc, a1494460-ed65-4614-9bc6-70094d424ee3, c1499be1-3e6d-4bc5-ba58-c1e09cb89622, c4c67410-a14c-48a9-9815-da023aa874e3, d51be0c9-7865-472a-9bd5-3fec5a3d0f31, e272902d-898c-4c9a-8db7-d254a7a1e026, e7d75d12-35c8-442c-b19d-b9da41e0a0df, ead061e2-5b23-460e-845a-1a0d81cdde9a, f393468e-731f-4594-befc-bac0fac6c66b]
185036 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 18 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet
185040 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet
185040 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 18 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet => [0e608d43-82f0-4deb-b407-f560496e1315, 1410433f-7e3d-4b8f-837b-9f715dc82905, 1635e52b-4d73-43a3-ab13-b50445bc73bd, 20ce385a-4ef4-4905-9cf7-0b6d5eab4304, 2e74360b-b3ff-46bd-8bf0-99c8cdaecfb8, 307fc873-0570-4d15-be79-16c4a11f0f8a, 3ace0130-93f5-4c15-b1c0-10906d59f83c, 46b64df5-6089-48e5-b8db-cfee2bfd96c9, 51b35d7b-a4f1-465f-8fcc-d902e9c490d5, 5544d194-c65c-4d5b-9c97-866e5bf7228a, 5c573f1a-ad21-4dd1-a8e7-cd4204c53d1a, 62d72d04-57f1-45b8-84b0-0f33eb18c588, 706ebfa9-470c-415d-b96c-856c240c7392, 7489dda2-2f07-484b-8e9b-37255936b070, 77105a57-bddc-4a3c-b120-4ac2e0a827db, 79bbb762-6f5f-4b8d-b158-c0b058fb58f2, 8310449d-d2e9-4473-9712-556609394cc3, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2]
185066 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet
185070 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet
185070 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195433.parquet => [8bc0109f-d17f-4051-a652-f7d04022e203, 90dc1cc9-aded-4911-9ee4-65a39f70371b, 9426cd67-722c-4c7d-8a20-da64a770caf3, 9848a8ed-e93a-43b6-b3a9-fd4b0e5bc522, a734ba20-0a97-4902-b8ce-c3b8faea1253, b46fcf47-6f72-4e38-aade-1732e33d65df, b52115af-2758-4b0c-93da-cc77a1b8b409, b8752044-d968-4067-ae9d-fbb4a73739e3, b8fa6a0a-0aa3-44ba-9b58-781b3d450e7e, bb2c9f2a-624a-4cd2-829c-d034979ed761, ccca3827-0ad9-4775-9e35-b246467a5aca, cdc03c01-a15f-4033-8fc0-9bc72cb1ac59, d5e04adf-206a-4770-b401-af239b8b9c10, ebdf3e96-c538-49fe-8bfd-4888e8325ef1, ee0bfdd4-0e25-4733-a4af-7383a76f7599, f5211b9c-5c7a-4340-9a52-a3fedb7f7f43, fb1ea6fb-41ef-4c14-b901-5527a9158679]
185080 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 24 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195433.parquet
185100 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195433.parquet
185100 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 24 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195433.parquet => [05db492d-0620-403f-bbba-129244b7f51b, 18cbee7b-d3b3-4f84-8eb9-17f0fe475d17, 234c8c6a-c58f-4a03-8ca9-5443b053b398, 37883702-6ed1-4cdb-8935-09788186c539, 52f6ff56-47a2-417f-87e0-0f797c24be5a, 64e4001d-2264-4964-a06f-862bc52c9806, 75aaa81f-b91a-44eb-a3c0-cceda4d34022, 75ef6414-590c-41ab-b755-b7c314f5cc5d, 83309d65-bd8e-49d1-b330-f2482f0b6052, 8b66485d-2ecd-4948-bb99-1246463b6bfd, 9390394f-68c5-471a-ad9b-c58c67c0652e, 9f39283b-d2df-4f90-8bc6-0d17f2bbabe4, a199891d-5d17-473b-9ebe-ba21591b7f6b, a5827aef-0ddb-4bb9-a421-660b1b457f4c, acff3468-f5b5-408d-bf50-50e48b040429, b68908c6-7304-4256-a45a-29cd9abc826c, c0017f26-2826-4ed9-8874-301597374df9, c43f5955-e6b4-48c6-a38a-d4ce748f5547, c558f0be-ac94-463f-b58c-58440e6de5b5, d52b9240-49ef-455b-9be4-b5162a5662cf, e587dc4b-8be5-4688-91a1-c7b2030c2fcf, e87934ff-6de4-41f4-a04f-fa0af88f8c8f, ec964995-6121-4738-b89f-ec7c80be05c5, fa3181ef-b3cc-4ec2-bfb5-e1c43bab4dc9]
185170 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=83}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=24}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=24}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=35}}}
185238 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2701
185238 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
185247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195435
185247 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195435
185588 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
185588 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
185723 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
185723 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
185833 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
185841 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195435 as complete
185841 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195435
187078 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195438
187310 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 90
187310 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
187465 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 26 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195435.parquet
187497 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195435.parquet
187497 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 26 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195435.parquet => [0dce30d9-7d25-47d9-a072-055695037788, 11669b7c-79ca-4430-b257-43940c47bf9f, 133f0ca9-0ccc-47a2-9ed2-5d5b4e2d0fa5, 1349a264-908b-49d5-9102-7af6df27b47d, 215d91bd-89b3-4fa7-a64d-6ca89f467950, 29071a5b-a9ca-4b56-85c7-7bc15efb785b, 30d1f40a-e1e9-41c2-afea-fb55eb3f4751, 40f1dec9-b8e9-4100-9288-a25064458f30, 4fe85051-32cb-45c0-b0be-c3517eb0d3bf, 51d04eb7-6148-4118-ab27-5d4f07b3da8e, 55f54db3-4b9e-4f06-b3fe-25a19a6a8edb, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 5e0bc964-2460-43d9-9043-b527d4abe5db, 77a74145-a4cb-44f5-bf56-a5eb0dedf57a, 81c2d2fd-2eef-4cda-a599-79c14b06185a, 8523598d-2e46-4d69-a51a-f22b0fd2b699, 8568448d-1fc7-4181-b40c-87adb7fc40af, 8a1cc4c7-4893-4cd5-885d-3109e30d830d, a41034e2-6135-44f1-ad4d-6a71de909471, c0474c5e-288d-47c0-833f-0dd68eefa29f, ca87648b-f916-4ad2-a28f-ce4bb03e6bf6, db033b2b-0587-43b8-b475-27e6f26fb369, df76930f-b8a2-4ab4-8950-c942a3414504, ead061e2-5b23-460e-845a-1a0d81cdde9a, f4036c9d-bf5c-4efa-9a7b-afcddc7064f5, fd82bb16-5857-40c4-a967-7b93b0fede52]
187507 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet
187511 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet
187511 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet => [0ae33ffe-73c5-4a58-a7fc-55c7f83ca7ef, 0e608d43-82f0-4deb-b407-f560496e1315, 1aaf70dc-1f6d-44b2-b8eb-e67f25fa09a3, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 46b64df5-6089-48e5-b8db-cfee2bfd96c9, 48944b93-9618-498b-bb4f-e411983b39fb, 4d2d886c-a71e-4c8f-823c-4368e04dd2aa, 5265ef4b-16e5-401a-9380-c059a01c98e7, 6b17aa08-8de4-4fe8-be45-f8afcc5f8ccc, 6bc3f70b-f058-40cf-8dee-6d46a1f13a7a, 8bc0109f-d17f-4051-a652-f7d04022e203, 8fa256b9-8a0f-4bab-8554-8b4427afa86b, 91cc567e-9493-4468-b8c9-2e8f689cfc31, 9782db1b-c724-4e7f-a41f-6ab99e1fd9c1, 9848a8ed-e93a-43b6-b3a9-fd4b0e5bc522, 9a348f6b-f394-43c2-8405-6822c27e0522, a52fbb9b-5972-4700-9f95-2df90f010863, ab2f75be-6df0-4192-81be-b311ee99efec, af41611a-2259-4610-a66b-e43b97958d0f]
187537 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 8 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet
187541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet
187541 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 8 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195435.parquet => [cdc03c01-a15f-4033-8fc0-9bc72cb1ac59, d571f779-c54c-4d0b-9db3-33523e692ca1, dcfa6a36-99d2-4a28-a171-8d2d21477bf5, dd334dd2-a314-436b-8c3f-9a1761f084aa, e70eb84c-5703-4ac4-8fe7-aacb15f3a0cd, e9c82bc1-47e1-46cb-905e-94dde0953381, ebdf3e96-c538-49fe-8bfd-4888e8325ef1, f5211b9c-5c7a-4340-9a52-a3fedb7f7f43]
187551 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 37 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195435.parquet
187555 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195435.parquet
187555 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 37 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195435.parquet => [0a7d5812-d573-432a-9721-142e71396b06, 104677ec-da2e-4c40-9233-b38e8758d01b, 1d1268cf-0fd8-4642-81f6-f4e7cfb621e9, 30e7a7de-3694-453c-a13d-99c46ee2a393, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 3a4d853e-f805-4507-8adf-ca0e72f41ee5, 4b467642-4ae2-438a-9cba-5800b28f3d74, 50566024-5b7b-4633-bb83-7bc8ba6a1478, 52f6ff56-47a2-417f-87e0-0f797c24be5a, 56d785d6-62bc-45cb-930c-2ac086065871, 5903081e-4156-46a7-8226-cee987598b48, 5ee45a16-73e0-4337-866f-a0ee20f4beac, 75afc7b6-0bc8-4c9c-bc9e-f08251dc7c24, 7ff832c5-86ae-4cb6-980f-4e2858e18be9, 8228cb88-6f38-431a-baec-0f92ee567ad4, 8c96f50f-c411-4c2c-9cd9-20929ff32312, 8d1cc486-edd3-468f-9d31-f891989b86a5, 9948c10c-59eb-4f0d-b9a8-02a162aed679, 9d2eb146-ca9e-4c6c-8ea6-8240639d66f0, 9f617d01-c6b6-4bbe-908e-4bc20e28581c, a5827aef-0ddb-4bb9-a421-660b1b457f4c, aa23bb3f-3fef-46d2-917e-b36dfa1888e1, acff3468-f5b5-408d-bf50-50e48b040429, b06fea13-bea3-46e5-bad4-9483a49fce2b, b73f67e7-59d5-4579-8fc3-69ba009af453, bb8ed0aa-0dae-46b3-ac3d-bb0edc2a7a13, bca28757-ea19-428c-ab4b-98ea79ee26fe, c8b90ac2-5d77-463a-85d6-4276ea12aafd, d52b9240-49ef-455b-9be4-b5162a5662cf, e1e460b2-1b5c-404d-82ec-11bd3e2a9caf, e42eeab6-e9b9-46b3-92b0-12a3aaa6cf69, e6a29185-1710-4bc3-9127-884ce0e885d3, e7883d11-694a-40e5-beea-c492b0349ced, e81f69e7-1820-468a-92a8-091913d52f39, e9a6679f-bcdb-42cc-a57d-ee72d7da4d8d, f62d392c-8196-460d-9552-22e50b5b3d00, fa3181ef-b3cc-4ec2-bfb5-e1c43bab4dc9]
187603 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=90}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=37}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=26}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
187644 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
187644 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
187655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195438
187655 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195438
188232 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
188232 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
188620 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
188620 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
188745 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
188753 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195438 as complete
188753 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195438
189965 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195441
190231 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 91
190231 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
190359 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195438.parquet
190398 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195438.parquet
190398 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195438.parquet => [017842aa-9d63-4bf1-b9df-653409f5a9e7, 01a36dec-30ba-418a-90bc-8c080bdeebec, 02ba262f-ee9e-43ab-975c-ae401885a13d, 056e809e-11f6-47dc-995d-94e38a79720d, 09240cfb-082d-426b-acac-e9e4184e38a3, 0dce30d9-7d25-47d9-a072-055695037788, 1892a55c-503e-466b-abd6-133472d20913, 26ab611d-6208-4e91-8e90-7f97a38e291f, 2b19e824-6514-4942-a0d3-0ca253fca8c9, 49004182-1557-4aaf-a45f-7eef722371b8, 53f42054-8ea6-4229-bb7e-57783889ab42, 545bcec0-65c8-4b25-ba55-51a2f8c171d5, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 6a050baf-fa7d-45fc-85dc-c0db630fc27b, 754cad80-8eaf-4054-b212-f50b6803bbdf, 77a74145-a4cb-44f5-bf56-a5eb0dedf57a, 8523598d-2e46-4d69-a51a-f22b0fd2b699, 87ac0a53-64e0-4520-a1e5-067421efd27c, 8af92b2c-e601-484e-bfe9-2adc060362a3, 92176651-d67f-47b7-9cd9-6c952adc9bfa, a5d38922-1d5e-447f-aea1-ed9651cf4949, c23ef669-0bd4-4a9b-ac40-73f6f4654c4c, ca87648b-f916-4ad2-a28f-ce4bb03e6bf6, df76930f-b8a2-4ab4-8950-c942a3414504, e7d75d12-35c8-442c-b19d-b9da41e0a0df, e8ebede2-b310-4399-b76d-dd841c52aad5, ed816151-b8c8-4051-9bd9-b5809fdde18e, f3e446e8-5686-4f82-8f10-29f2a634f6e6, fdec1d4b-451e-494a-9d6e-f2f3941fdb07]
190410 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet
190431 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet
190431 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet => [0f02a21c-5705-4af7-a895-0f6e3a08b58c, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 1e3db419-fa04-489a-b560-ededb3f71c10, 2bc512b1-f3fa-4875-900b-8c16e2ef1392, 2e8ed220-8e90-4276-a06f-9866abf6168b, 3922d525-2a33-493a-9bf0-cf10bc3d96a3, 3ace0130-93f5-4c15-b1c0-10906d59f83c, 3f3067ee-c6ad-41d6-bb0b-d677729a95c3, 4e78f4a9-fd59-4d59-b74f-8e8448eca2f2, 53d6e47f-9d01-4d52-a1fa-ca29925ffc04, 5e648bd4-4895-469c-9b7c-20b92a692bc2, 6148d989-7047-466c-ba18-7d090de9b6e7, 7499225f-3dc5-42c1-bea6-e69543d0aacc, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2, 93aa9cd9-09e3-4085-916a-fa2b9242a992, af0f63e9-2231-44c8-acfc-de4b8cb1a495, c34e7298-582c-4031-8b5b-8235b9c9c521]
190469 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet
190498 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet
190499 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195438.parquet => [c3f2d83c-661d-4393-859c-ac1d719c8d29, cc609e5c-f1be-4b41-b7d7-5b1310816448, d571f779-c54c-4d0b-9db3-33523e692ca1, d5e04adf-206a-4770-b401-af239b8b9c10, e9c82bc1-47e1-46cb-905e-94dde0953381, ee0bfdd4-0e25-4733-a4af-7383a76f7599, fb175f56-ee3e-4b86-b29a-3b75ebb632a3, fc697ea8-832d-4337-aeb8-bcf736f032cc, fd208c31-e733-40df-8b46-0fa9fb066c61, fefc8220-fa6e-4cfb-baaf-7b8479cf0902, ff4219b9-de03-44d9-8b8b-5633442c93ee]
190510 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195438.parquet
190531 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195438.parquet
190531 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195438.parquet => [0447dfdd-f159-4791-abd9-c7625acee5a6, 0b0a7c49-6376-42b6-8bd4-77ba445fc233, 11f5a22a-64de-41bf-baa1-0c5ca74ce638, 1e29d1eb-b86e-4fa0-8fb0-eb3209031e21, 2b12412e-0a5a-4d80-b124-5d0635e14c93, 30e7a7de-3694-453c-a13d-99c46ee2a393, 33d256a2-ba0f-40b5-86c2-491b526a15cc, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 37883702-6ed1-4cdb-8935-09788186c539, 4b467642-4ae2-438a-9cba-5800b28f3d74, 4fcb0d5a-4e0f-4d01-bdd7-9ee3ddb098c7, 50566024-5b7b-4633-bb83-7bc8ba6a1478, 5cf2193e-6ec6-42ff-b4a6-794ec9185ec2, 674c6473-40a0-4a7b-b6c2-49f76a1a615c, 75ef6414-590c-41ab-b755-b7c314f5cc5d, 84f4d7ec-942d-4600-92c5-2fea263c56ee, 93c75b4a-c4c7-45d3-8d70-1cc72186e8d5, 98989271-7587-4fda-aff7-88eab6a5fe72, 9a9ecd96-b67e-48ed-9fd4-e9a36a7b7bc0, a199891d-5d17-473b-9ebe-ba21591b7f6b, aa23bb3f-3fef-46d2-917e-b36dfa1888e1, aa34367b-16dc-4931-b7db-69ba905f8cbb, b199eb32-886f-4103-bb33-b0fdfe9ac138, bab43408-91e0-41bd-a302-a3519f410c5d, bb90e5f9-3951-4531-804e-17b9fa29a1ec, c5fc4430-1120-434b-b6f5-357284971829, c8b90ac2-5d77-463a-85d6-4276ea12aafd, ca6bf156-a0ae-4a6d-8a11-66e648f720fe, ce39871f-1f8e-4fd8-b691-38e6e80d8bec, d46281e8-6e09-497c-9146-39d352557805, d52b9240-49ef-455b-9be4-b5162a5662cf, e494c00a-7199-4a21-828a-762bfbf8eed9, e7883d11-694a-40e5-beea-c492b0349ced, f62d392c-8196-460d-9552-22e50b5b3d00]
190602 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=91}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=29}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=28}}}
190670 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
190670 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
190679 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195441
190679 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195441
191285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
191285 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
191554 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
191555 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
191669 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
191676 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195441 as complete
191676 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195441
192920 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195444
193131 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 89
193131 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
193246 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 28 for /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195441.parquet
193253 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 146 row keys from /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195441.parquet
193253 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 28 results, for file /tmp/junit9010204236062209327/2015/03/16/4444375d-6671-46e9-8776-1b74f4a2c140_2_20200319195441.parquet => [0004db58-b843-489e-9f8b-644952c44546, 017842aa-9d63-4bf1-b9df-653409f5a9e7, 02ba262f-ee9e-43ab-975c-ae401885a13d, 11669b7c-79ca-4430-b257-43940c47bf9f, 133f0ca9-0ccc-47a2-9ed2-5d5b4e2d0fa5, 214c91d0-5538-4dd0-8942-5eb604a099ae, 30d26a27-1d7a-4d9f-82a2-c9daa3b2c90a, 3e598b8c-5dc7-480d-ae42-a4df0d5831f7, 4329713b-0cb7-4a39-a3d9-f766a0e52ec7, 5256bff1-8c78-4b91-ac93-30adb15de779, 54d0e11a-752d-460d-937e-e20111528a24, 5d42732d-161b-40a8-a628-ba8dcce9f3ee, 66b46659-34a0-44d9-a9dc-da1e7d3e38d7, 7044ffc5-45fd-425c-8a6f-425296b646ff, 8af92b2c-e601-484e-bfe9-2adc060362a3, a9f9537a-83ff-4398-910c-8bdea3e2a422, aba3570f-7e84-460c-b7c9-271b2ab4add7, b6eba60c-2ab1-409a-8ad1-00257a1bda5d, c0474c5e-288d-47c0-833f-0dd68eefa29f, ceadd625-5a0e-4e63-a8e2-8431ca9fc0f4, db2453fc-071a-43f6-83d8-c8e7068fc596, e155d7c8-93ad-4a61-8eb2-6a289e317811, e4f1ba66-d2b6-4c91-a8a5-073e9e583d2b, e7e7dbf3-14cb-440c-971a-e2c3f0a1829e, e912a426-97ed-47f6-a015-9d62cb66371c, ed816151-b8c8-4051-9bd9-b5809fdde18e, ef7e863f-9780-4f82-861f-9be106f2ac10, f7aaca79-90a8-4977-89cf-818919b9dd38]
193263 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 17 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet
193268 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet
193268 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 17 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet => [0ae33ffe-73c5-4a58-a7fc-55c7f83ca7ef, 0e0c77f4-9934-4a53-9efd-bfc66cbb3de2, 1410433f-7e3d-4b8f-837b-9f715dc82905, 17cd6a9f-f3f0-4c40-9384-b10fdea261ee, 1bb43c1e-c78b-4993-be40-39c4d23a306b, 2bc512b1-f3fa-4875-900b-8c16e2ef1392, 3922d525-2a33-493a-9bf0-cf10bc3d96a3, 3ace0130-93f5-4c15-b1c0-10906d59f83c, 4e78f4a9-fd59-4d59-b74f-8e8448eca2f2, 5c573f1a-ad21-4dd1-a8e7-cd4204c53d1a, 62d72d04-57f1-45b8-84b0-0f33eb18c588, 6b17aa08-8de4-4fe8-be45-f8afcc5f8ccc, 6bc3f70b-f058-40cf-8dee-6d46a1f13a7a, 785d4981-8453-45da-ac92-ab40630608b9, 79bbb762-6f5f-4b8d-b158-c0b058fb58f2, 83125865-3ffa-4473-8b9a-6e70ecf9e4d2, 91cc567e-9493-4468-b8c9-2e8f689cfc31]
193293 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 10 for /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet
193297 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 167 row keys from /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet
193297 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 10 results, for file /tmp/junit9010204236062209327/2015/03/17/6e46e78e-edbe-4a56-8dc2-24ea08588172_1_20200319195441.parquet => [9560fbb4-35e6-425b-ac7a-418c0f0f963c, 96f0ac8e-a0ac-48bd-90d4-2f7a0ff43265, 97822b50-f841-4192-a95e-090c6c27d91f, 9eb452a1-8d86-47c4-934c-81023ac8515c, b5abdfc3-06a3-4012-957c-4d1a138d2d31, cb262672-537a-44ec-a622-fa600f539456, cc609e5c-f1be-4b41-b7d7-5b1310816448, d8c49b7c-e0b8-4615-8e4a-259967d630a7, dba389c4-d911-4555-bfd5-f9a5738d05e5, fb175f56-ee3e-4b86-b29a-3b75ebb632a3]
193307 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 34 for /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195441.parquet
193311 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 187 row keys from /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195441.parquet
193311 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 34 results, for file /tmp/junit9010204236062209327/2016/03/15/ba827dab-2f0f-47fc-9e04-14bb37335117_0_20200319195441.parquet => [0447dfdd-f159-4791-abd9-c7625acee5a6, 05464664-15f2-4b79-82d9-d44c9ef41d75, 0b0a7c49-6376-42b6-8bd4-77ba445fc233, 115fa76b-bd6b-4856-98f3-4abe101d784a, 164240d3-977d-4bef-816b-02d707c575ae, 173142b1-6a0f-48fe-878f-b213094f4465, 21ae6afa-e9cd-4a3b-8c1d-a1f475037a9b, 35a761cf-e97a-4000-8fcb-6517d7a50d19, 3add04e3-7d1c-4caa-a834-4143f91a0695, 3fa4d3b4-8a62-45e0-8202-3e8dd146147a, 42042347-5cbf-4758-b3e8-a8381506635f, 4a91673d-d511-468f-9d35-c43190d572cc, 4b7e9778-94ab-4180-9955-469a35555602, 4e541e57-1b04-4a8a-86b0-db9675f0c590, 53a7f1f5-2fd1-48a0-9c30-b24124a1dadf, 75afc7b6-0bc8-4c9c-bc9e-f08251dc7c24, 7ff832c5-86ae-4cb6-980f-4e2858e18be9, 84f4d7ec-942d-4600-92c5-2fea263c56ee, 8bf7396e-5514-453f-a824-964947126eb0, 8c96f50f-c411-4c2c-9cd9-20929ff32312, a3f27831-413f-416a-ba58-cd1887f501c0, a5827aef-0ddb-4bb9-a421-660b1b457f4c, b786942e-f5f5-4bc2-8c8f-d1f1b43231e5, bab43408-91e0-41bd-a302-a3519f410c5d, c8bd0fb3-83da-4a8d-ae24-48e1f75fb93e, ce39871f-1f8e-4fd8-b691-38e6e80d8bec, ce8c0040-e46f-4df2-81c4-bdbe5c3d579c, d4700132-d150-416b-bbac-b9a1071e1a0e, e6a29185-1710-4bc3-9127-884ce0e885d3, eb130a09-c83c-4378-8b3c-dd96ed5fd558, eb4c9cd9-b84c-41cf-a13d-5840ba5bd271, f51cbc46-57b1-4107-9155-16fb137f3ee1, f5b53eba-e913-4ea7-a0a6-f1be6fa8dab8, fdaf0271-616b-4733-967c-363fe0ac40f9]
193355 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=89}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=34}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=28}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=27}}}
193375 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 2702
193375 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=ba827dab-2f0f-47fc-9e04-14bb37335117}, 1=BucketInfo {bucketType=UPDATE, fileLoc=6e46e78e-edbe-4a56-8dc2-24ea08588172}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4444375d-6671-46e9-8776-1b74f4a2c140}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{ba827dab-2f0f-47fc-9e04-14bb37335117=0, 6e46e78e-edbe-4a56-8dc2-24ea08588172=1, 4444375d-6671-46e9-8776-1b74f4a2c140=2}
193384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195444
193384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195444
193675 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
193676 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
193837 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_FILE_VERSIONS
193837 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
193944 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
193952 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195444 as complete
193952 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195444
194236 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
194236 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
194367 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
194368 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/01/id31_1_20160506030611.parquet	true
194369 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
194370 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/02/id32_1_20160506030611.parquet	true
194370 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
194370 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/06/id33_1_20160506030611.parquet	true
194373 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
194373 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
194384 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
194384 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
194384 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
194506 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
194506 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
194506 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
194514 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
194514 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
194521 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
194522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
194522 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
194631 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
194631 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/01/id21_1_20160502020601.parquet	true
194631 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
194632 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/02/id22_1_20160502020601.parquet	true
194632 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
194632 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/06/id23_1_20160502020601.parquet	true
194635 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
194635 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
194644 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
194645 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
194645 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
194762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
194762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/01/id21_1_20160502020601.parquet	true
194762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
194762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/02/id22_1_20160502020601.parquet	true
194762 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
194763 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/06/id23_1_20160502020601.parquet	true
194766 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
194766 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
194775 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
194776 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160501010101]
194776 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160501010101]
194871 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
194871 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/01/id11_1_20160501010101.parquet	true
194872 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
194872 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/02/id12_1_20160501010101.parquet	true
194872 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
194872 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit7387979668502070933/2016/05/06/id13_1_20160501010101.parquet	true
194876 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160501010101]
194876 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160501010101]
194887 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160501010101] rollback is complete
195020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
195262 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
195262 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
195570 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=300, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=107, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=91, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=102, numUpdates=0}}}
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 107, totalInsertBuckets => 1, recordsPerBucket => 500000
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 91, totalInsertBuckets => 1, recordsPerBucket => 500000
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 102, totalInsertBuckets => 1, recordsPerBucket => 500000
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
195578 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
195586 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
195586 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
195672 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
195712 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
195712 [pool-1795-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
195713 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
195797 [pool-1795-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
195808 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
195821 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
195822 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
195821 [pool-1796-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
196086 [pool-1796-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
196100 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
196117 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
196118 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
196117 [pool-1797-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
196157 [pool-1797-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
196178 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
196178 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
196247 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
196247 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
196361 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
196370 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
196370 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
196814 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
196815 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
196897 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 74 for /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet
196902 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet
196902 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 74 results, for file /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet => [01897bb1-fbb5-4584-be1a-bf6798425b8d, 03930a95-7231-42d6-b723-b60953724065, 101ac562-98e6-4fbc-8014-bd4484bbe90a, 12944d5f-65b2-42fe-b453-e0ababf0bc18, 1441bad0-a178-45ce-9b1f-7c84f10f8240, 155d4bea-96dd-4a32-9035-f26dace008ba, 1bed9384-bc77-4731-a2de-49a1eebfa783, 1e44310b-52e6-4811-937f-555da26fe033, 2182b37b-9225-48d8-a727-e2da81367983, 23d40df6-a36f-4821-9d47-52ed954472a2, 2ac6b6d9-a004-42d6-840f-08b8087da991, 2aeca867-5e12-4908-9369-420146241c00, 2b2d2e39-ad7a-4f98-9104-09c3b6a48824, 2d6b143b-2fb4-4f51-805a-6d9c3f0e532d, 35458d05-8af0-44de-9300-e9021b0710e1, 380cc176-21f1-43c8-8b15-33e4c7ce217a, 3de7275d-ccae-4e1c-8be9-cfb6de2055be, 433e31de-15a4-4180-8780-3c63d3b36fd7, 465f3dc6-1457-4f81-a525-ce05fa9b23e7, 486d8a2f-bd8e-41b5-9e57-18c9036ca4f9, 4cd0d9c5-73be-44e4-87e7-b16403eda5f1, 584eadf6-a58b-4798-888e-8020f9d7b7de, 5e4e3271-6382-440e-b88f-992b5fdfde83, 5f824f44-389c-4df8-8909-b7f9e5477ea4, 62e91c05-9613-4217-a9fd-86aa92fb2ee3, 67b16459-0c72-4a62-b564-0ff45402894b, 69618008-29a2-4f6c-94e1-de69ecd09565, 6cdb1b38-b291-4c8a-998d-1cbf57cd0d40, 6ea7615c-5fec-498d-86e8-1fe5674fc546, 7319c797-b7d2-48c9-8f67-399baaf01d2d, 79836979-cfbd-4693-9463-4409379bf1cc, 80593663-135d-45c5-9709-fb215dcba7e8, 85ab1932-630c-4705-8d1c-79b2d155146a, 87d272fe-eda1-420b-9069-511300803409, 880110aa-393d-490f-8d3b-60fb2e27c1b8, 89281d2b-a903-4cc4-ba7c-91746f49f661, 8c319ea6-3138-44e7-9d5a-7f869d0701ab, 8d25697b-551c-47d2-817f-a2a55bd5b812, 8e3ed27c-45b0-4461-a334-0b01c924b2aa, 959ca3e6-1b4f-4f2a-9022-8d061d01e98a, 98bb9837-8f7b-406d-a38b-bd8cc0a3278e, 99833563-7639-4447-bc32-48ebe4269715, 99ce3a61-40f7-4b5c-85c7-daf71ead7880, 9b5c4f68-b386-4c32-b02a-491334aee43c, 9c473564-3e47-4680-8d52-e0658e34d3d5, 9ca881a9-31f2-4beb-a9ff-15d00a0833fc, a04814f6-f598-40e2-9c1f-b0cfbad1fbaf, a12630cf-7e41-4cae-90e3-c1ae454a84cd, ad77de4f-0ea1-4466-96da-32f4bbcd511a, ae10d188-cff7-4ef5-9c65-a07bcd8bcc5e, b1dbbd0b-cc9f-4295-af37-ea1dd0650c7c, b510c8e6-1b71-448d-b4d4-9b46518516dc, b5d1e98b-c8da-4319-8eff-9105894ec28e, b8d69b3d-b4d9-4040-b444-4e79e896d258, ba0df596-30bc-47f3-a3b5-31531d575704, bf7c3c29-e4a7-4288-be47-8b2c6d8d8491, c21c6a57-5c74-490f-b6dc-387858ee3ba4, c2454c57-318a-4d5f-b389-d58648c94c3b, c888b8c5-0872-488a-9787-11220515c7ba, c9904daf-5ea2-4906-b8a1-e0da418dce1d, d3f6eab7-a4cc-4899-a51e-df151d04c55a, d65330a6-ecf0-45cb-8e3c-de2761dfbd4e, dc3d0b92-94a0-4f9b-a908-b59ab817413d, dd9b6265-a98e-4e88-a7a2-00a5e81f3024, dfc67340-5134-410b-9a8d-fd82a9e96433, e2c6203e-4d96-4b61-a613-a6c9cd5afd11, ea2c2486-cfaf-4682-9310-669579fb17ce, ed18bdc7-cd2b-412e-b93a-9bed93632cc7, f079496d-0ed2-44eb-9114-a74fe71a1fe0, f20801fb-8187-465e-b7f3-1c3e4417e947, f873879c-b0ec-48a7-9c8b-f1859c7e425b, fb26626c-e760-474b-82e7-ca539e4ddef4, fc8bf3b7-205c-466e-bbb9-39b7cda383a8, fc9247b9-e03d-4056-808b-2a8a811f2865]
196912 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 60 for /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
196916 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
196916 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 60 results, for file /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet => [0293ed22-647d-4a45-bd7b-afb8c458d992, 045b3ec5-028d-4d6a-afc4-025da6a73a5a, 0754659f-d813-422d-92b7-01ff63179392, 0b25a064-12e2-4716-b685-c42779efa580, 0c0d2d62-5dca-408c-b4a2-df00c7a985c3, 0c50985a-9b1e-4695-9f6e-c57ce96e693e, 0d4aaf98-be54-4f79-b7af-df7b505619c8, 11ceda06-5f52-4f72-9ffe-b0f87a888070, 15313894-6335-42f2-b4b9-3440efcd511e, 1abca76a-658e-4cf7-a356-084cb2ca9624, 2271ec43-7fb9-4652-923b-8aa4b57ed1ed, 22e5b12b-47d0-41d0-ac57-2fef5f20316f, 249120e1-a5d5-4256-b5ba-72bd8cada8f8, 2d928105-c24c-4474-854e-5fdd10917c6b, 302d269f-ad34-4733-9ed4-6f10edf119d3, 318fe878-396e-452d-8837-b4d54b24b9eb, 35f33156-4f20-424e-945f-29ac6adc49ff, 3a8cdf8b-e22c-49c1-9cf9-7cf3bbac1b1c, 3bfb4221-bfaf-4973-b4cc-d61dfc4de734, 40c582e8-17df-48b2-aa9e-4d9d398b1054, 498c9ed7-db35-4aa0-8d04-dec284b205d5, 4eb033c2-5825-4654-a278-da11a9a11ab7, 572d26bf-68a5-4646-b2db-f614dad84b43, 5b1d2ade-6639-4495-b815-901277a7ba1c, 5bc69f97-055d-455a-ab60-b78c1c1e10a5, 5d076ad2-059a-485e-afe9-5bb77dc64380, 5ed5d973-6ef5-48a9-af5e-8a1613bcdc89, 5fd044d1-62b4-4783-8495-bab0caba8d8b, 6894e9c0-0992-4797-a240-b69eb8ae20cd, 6e333bbf-6c74-456e-9c15-befb889c2899, 704f23af-1f82-439c-b658-6e05f96313cc, 7a08ea21-064d-4de8-bab4-59a3cb90e09a, 866183cb-bdbc-4c14-9447-ff2b4192fac8, 8dc197fb-8329-4fa4-ac72-9b111e713974, 8fdb2b67-28fa-4de5-936a-4d263cea8e3c, 9686f793-b80b-44c2-88fe-cf858b69fdd1, 99e9f32b-8b06-45db-9794-0944ddddfcbf, 9d506819-df09-417f-bdf5-e9d82961b1d0, a388fdd7-3134-42db-8dd2-b26e33b6778b, a3cce6cc-2e80-4234-927d-01e70556a336, a4a65687-e59a-49fd-b6d0-179c141d7f58, ba3ebc63-9d2f-41b4-81b5-20cc8e4fc0c5, be58c27f-1d16-41e1-a149-d77d23ab51b4, c620303b-d141-43fc-9430-f63bb5a49ed7, c6505e69-8305-4cce-b308-fe1d7cd841a6, ca758b1c-96a1-4c81-b4f3-04fe2d75cbed, ccafb39a-c6da-47c6-844e-19986b5c02ab, d017d5af-396a-4297-a78d-5fe7ec83d904, d10c1d51-f501-4084-91ff-c3ffe9f8c6e9, d1e96731-1a78-4528-b210-6de68a406287, d714eea6-ef53-46ba-8d32-22acc32cc5c1, d77eff2b-0691-4557-bbc8-53d0a2b2a2a2, daf48fdd-7cb4-48ba-a401-51140a405bd5, e1a081af-73e2-4090-9f9c-958e48b6a20c, e351bebd-8ce7-4b27-aa0c-e004c8418ea3, edb41f18-39cd-49e5-8179-cd072bb1998d, f1810359-ef44-45f3-aec9-172683b3e224, f1b1bc24-07e1-40d5-b566-9b2315000685, f4c66cc9-4b18-4f67-9f8d-64b68cd6a405, f6f360bf-b541-400e-b7f3-e30d667defd3]
196925 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 66 for /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet
196935 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet
196935 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 66 results, for file /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet => [025770a9-649e-4cea-9010-efbcb552f0b2, 06c0ee8e-9ac9-48ef-87b2-fbeca00f2e9c, 06c6c7ac-48aa-4f3c-a69e-59bff7adcc81, 0c4b364f-007e-41b7-a672-aa6a0c6e8fff, 0e5bb9b0-d217-4d52-8e9b-a14caff42468, 0f91f786-ec30-4926-9a87-17a4af58d786, 12b31777-ee4d-4613-a44f-4cbfc04700b4, 1ee55f23-9e8c-4d75-b9ca-ba095b2f9d24, 27c8fc33-c94a-4465-a1d5-21fc456bc121, 2fadb683-9d23-4573-851e-bcb0d6c981bc, 3098a9fc-4353-4577-b956-35d3701f357c, 30b1bafd-a8ad-4463-a49a-fa79ca24fd52, 3318b695-9fe8-4087-9f40-dd53acae09e6, 34bcf2e0-a9a5-4634-998f-12fe88eae2eb, 352df239-183a-4fc1-8bcd-5d1a76f0df0b, 39e1d7ce-953f-4f42-a0e1-afffa43add58, 40a5a377-866e-40bc-a4d7-90bba32f966d, 488e611e-2eb5-4cc3-a637-81cf8f4189bc, 48d46a9c-3e78-4632-8498-02a59b819450, 49eb3d2e-6343-445a-9a6c-1b715ca6b76a, 4aad30e8-26a6-45f9-b3d5-54e9d267d262, 531fe39f-1ff2-484c-a39a-9acae0124eb6, 60d0400d-f5f8-499b-a0e9-8ff35b25f699, 66187c52-4755-49fb-9be7-19434a62a631, 673989cb-630b-4b37-a6bf-aecd16fea09b, 6aa063a5-22de-4345-ad24-e561d04c8569, 6f4d15cb-cfa4-40bb-85ef-f72b21379e00, 7058a322-563d-4ef5-ae95-c85808c308a2, 71b1acef-af99-42fe-a76a-1ad45fd46684, 72d30d52-6f75-4d6c-a901-3d4b8f3a4d8d, 741e4d89-5734-4598-b6c0-06f845c192da, 78a5e834-c5aa-4fc6-b098-62a398aa81c7, 7c1a9fcc-d616-4366-8c0c-18f70a3440a7, 80099729-a1b5-4dd8-bef6-99427037ed0f, 82914d38-b329-49c4-8435-866e29bbca66, 85d1fa53-69f4-41c7-89e6-704ea0098bb8, 896659d4-2bb5-42fb-b175-fa1d31b430fb, 950c46c0-7017-43a0-a6cc-e9454318d7db, 96e3c02b-4c91-48cb-b406-b6283fc9e88e, 9d6b07eb-0be4-4374-a763-59251d7cbe08, 9d8abc57-315d-4aac-8774-4ded85336e85, 9ea8b0b1-4d59-49f3-aa75-67f6be6aa41f, a1382b35-d573-4803-a9f9-7e0ac1f545a3, a3d9743b-4c6d-401c-a731-0f3f1f9d8b26, a4942d5f-1013-429c-b52f-c242c416c89f, a59421d3-cb11-49f4-a64a-0576d9954291, b09b4228-d32e-43ca-8db3-b923da9d3f97, b4466db5-42e1-45d8-8e7a-1e8aa9142e61, bbdf8586-9e0e-47b3-82c2-f75637fd16fe, c05a7521-6332-4ae7-9bb4-ed25f0deb048, c16a766f-f4d4-4fbd-99ab-42ec3fc9c81c, c55e76c9-0c46-4175-bb17-be0c0e42bb65, c59cb503-ee4d-4a9b-b044-bcd71b089a39, c669c8b1-8cde-4055-954c-9d82f2ac0d85, c76f0058-7ecd-4cc4-a3ec-4b0440c8b87d, cd347aa8-65ea-48a2-bc2d-fee6af879fb3, d073b91a-5780-4d4d-8ef1-6ae71bd8964c, d19260e0-a21b-4862-8ddc-c56d5d5396f5, d346069f-7250-490e-ac24-797d4ad09ff1, d78c0b49-9917-49b7-a9df-990d150dee89, da72314e-8454-4fd4-b44f-7340744e478e, ea0d48da-ad26-4ec9-ac46-7c74f90e4d78, f1b777ec-e257-47a0-b58a-eaa953cc4613, f353558e-b8b5-44be-a5c7-8d3b9ea04607, f725a1e4-81c6-45e0-acb6-ef0dac634ce0, fcc6091a-1c50-4f6b-a20a-979bdc0b39fa]
196982 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
197225 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 100
197225 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
197359 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 39 for /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet
197364 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 74 row keys from /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet
197364 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 39 results, for file /tmp/junit1926547621124415169/2016/03/15/5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b_0_001.parquet => [01897bb1-fbb5-4584-be1a-bf6798425b8d, 1441bad0-a178-45ce-9b1f-7c84f10f8240, 1bed9384-bc77-4731-a2de-49a1eebfa783, 1e44310b-52e6-4811-937f-555da26fe033, 2182b37b-9225-48d8-a727-e2da81367983, 23d40df6-a36f-4821-9d47-52ed954472a2, 2b2d2e39-ad7a-4f98-9104-09c3b6a48824, 2d6b143b-2fb4-4f51-805a-6d9c3f0e532d, 35458d05-8af0-44de-9300-e9021b0710e1, 380cc176-21f1-43c8-8b15-33e4c7ce217a, 465f3dc6-1457-4f81-a525-ce05fa9b23e7, 5f824f44-389c-4df8-8909-b7f9e5477ea4, 69618008-29a2-4f6c-94e1-de69ecd09565, 6cdb1b38-b291-4c8a-998d-1cbf57cd0d40, 6ea7615c-5fec-498d-86e8-1fe5674fc546, 7319c797-b7d2-48c9-8f67-399baaf01d2d, 79836979-cfbd-4693-9463-4409379bf1cc, 8c319ea6-3138-44e7-9d5a-7f869d0701ab, 8e3ed27c-45b0-4461-a334-0b01c924b2aa, 959ca3e6-1b4f-4f2a-9022-8d061d01e98a, 9b5c4f68-b386-4c32-b02a-491334aee43c, 9c473564-3e47-4680-8d52-e0658e34d3d5, a04814f6-f598-40e2-9c1f-b0cfbad1fbaf, ad77de4f-0ea1-4466-96da-32f4bbcd511a, ae10d188-cff7-4ef5-9c65-a07bcd8bcc5e, b510c8e6-1b71-448d-b4d4-9b46518516dc, ba0df596-30bc-47f3-a3b5-31531d575704, c21c6a57-5c74-490f-b6dc-387858ee3ba4, c2454c57-318a-4d5f-b389-d58648c94c3b, c9904daf-5ea2-4906-b8a1-e0da418dce1d, d3f6eab7-a4cc-4899-a51e-df151d04c55a, dd9b6265-a98e-4e88-a7a2-00a5e81f3024, dfc67340-5134-410b-9a8d-fd82a9e96433, e2c6203e-4d96-4b61-a613-a6c9cd5afd11, ed18bdc7-cd2b-412e-b93a-9bed93632cc7, f20801fb-8187-465e-b7f3-1c3e4417e947, f873879c-b0ec-48a7-9c8b-f1859c7e425b, fb26626c-e760-474b-82e7-ca539e4ddef4, fc9247b9-e03d-4056-808b-2a8a811f2865]
197374 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 11 for /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
197406 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
197406 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 11 results, for file /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet => [0293ed22-647d-4a45-bd7b-afb8c458d992, 0754659f-d813-422d-92b7-01ff63179392, 0c0d2d62-5dca-408c-b4a2-df00c7a985c3, 0c50985a-9b1e-4695-9f6e-c57ce96e693e, 0d4aaf98-be54-4f79-b7af-df7b505619c8, 11ceda06-5f52-4f72-9ffe-b0f87a888070, 15313894-6335-42f2-b4b9-3440efcd511e, 1abca76a-658e-4cf7-a356-084cb2ca9624, 2271ec43-7fb9-4652-923b-8aa4b57ed1ed, 249120e1-a5d5-4256-b5ba-72bd8cada8f8, 2d928105-c24c-4474-854e-5fdd10917c6b]
197427 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 19 for /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
197431 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 60 row keys from /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet
197431 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 19 results, for file /tmp/junit1926547621124415169/2015/03/16/618ab248-87ee-4d03-8168-106c6db02688_1_001.parquet => [302d269f-ad34-4733-9ed4-6f10edf119d3, 35f33156-4f20-424e-945f-29ac6adc49ff, 3a8cdf8b-e22c-49c1-9cf9-7cf3bbac1b1c, 3bfb4221-bfaf-4973-b4cc-d61dfc4de734, 5bc69f97-055d-455a-ab60-b78c1c1e10a5, 5d076ad2-059a-485e-afe9-5bb77dc64380, 5ed5d973-6ef5-48a9-af5e-8a1613bcdc89, 6e333bbf-6c74-456e-9c15-befb889c2899, 866183cb-bdbc-4c14-9447-ff2b4192fac8, 9686f793-b80b-44c2-88fe-cf858b69fdd1, 99e9f32b-8b06-45db-9794-0944ddddfcbf, a388fdd7-3134-42db-8dd2-b26e33b6778b, a4a65687-e59a-49fd-b6d0-179c141d7f58, ba3ebc63-9d2f-41b4-81b5-20cc8e4fc0c5, ca758b1c-96a1-4c81-b4f3-04fe2d75cbed, ccafb39a-c6da-47c6-844e-19986b5c02ab, d10c1d51-f501-4084-91ff-c3ffe9f8c6e9, d714eea6-ef53-46ba-8d32-22acc32cc5c1, f1810359-ef44-45f3-aec9-172683b3e224]
197441 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 31 for /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet
197445 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 66 row keys from /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet
197445 [Executor task launch worker-2] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 31 results, for file /tmp/junit1926547621124415169/2015/03/17/96557156-7b04-4e60-96e6-3170080e8d0e_2_001.parquet => [06c0ee8e-9ac9-48ef-87b2-fbeca00f2e9c, 12b31777-ee4d-4613-a44f-4cbfc04700b4, 30b1bafd-a8ad-4463-a49a-fa79ca24fd52, 3318b695-9fe8-4087-9f40-dd53acae09e6, 34bcf2e0-a9a5-4634-998f-12fe88eae2eb, 39e1d7ce-953f-4f42-a0e1-afffa43add58, 40a5a377-866e-40bc-a4d7-90bba32f966d, 531fe39f-1ff2-484c-a39a-9acae0124eb6, 673989cb-630b-4b37-a6bf-aecd16fea09b, 6f4d15cb-cfa4-40bb-85ef-f72b21379e00, 7058a322-563d-4ef5-ae95-c85808c308a2, 78a5e834-c5aa-4fc6-b098-62a398aa81c7, 82914d38-b329-49c4-8435-866e29bbca66, 85d1fa53-69f4-41c7-89e6-704ea0098bb8, 950c46c0-7017-43a0-a6cc-e9454318d7db, 96e3c02b-4c91-48cb-b406-b6283fc9e88e, 9d8abc57-315d-4aac-8774-4ded85336e85, 9ea8b0b1-4d59-49f3-aa75-67f6be6aa41f, a1382b35-d573-4803-a9f9-7e0ac1f545a3, a3d9743b-4c6d-401c-a731-0f3f1f9d8b26, a4942d5f-1013-429c-b52f-c242c416c89f, c05a7521-6332-4ae7-9bb4-ed25f0deb048, cd347aa8-65ea-48a2-bc2d-fee6af879fb3, d073b91a-5780-4d4d-8ef1-6ae71bd8964c, d346069f-7250-490e-ac24-797d4ad09ff1, d78c0b49-9917-49b7-a9df-990d150dee89, da72314e-8454-4fd4-b44f-7340744e478e, ea0d48da-ad26-4ec9-ac46-7c74f90e4d78, f1b777ec-e257-47a0-b58a-eaa953cc4613, f725a1e4-81c6-45e0-acb6-ef0dac634ce0, fcc6091a-1c50-4f6b-a20a-979bdc0b39fa]
197518 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=100}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=39}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=30}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=31}}}
197561 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
197561 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=618ab248-87ee-4d03-8168-106c6db02688}, 1=BucketInfo {bucketType=UPDATE, fileLoc=5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b}, 2=BucketInfo {bucketType=UPDATE, fileLoc=96557156-7b04-4e60-96e6-3170080e8d0e}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{618ab248-87ee-4d03-8168-106c6db02688=0, 5b69e5c2-b3a8-4fa2-a1ab-b9b53616ba3b=1, 96557156-7b04-4e60-96e6-3170080e8d0e=2}
197578 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
197580 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
197990 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
197990 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
198408 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
198408 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
198509 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
198564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
198564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
199598 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
199598 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
199704 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
199704 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
199787 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
199787 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
199867 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
199868 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
199945 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
199945 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 2
200155 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 001
200387 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
200388 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
200621 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=75, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=58, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=67, numUpdates=0}}}
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 75, totalInsertBuckets => 1, recordsPerBucket => 500000
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 58, totalInsertBuckets => 1, recordsPerBucket => 500000
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 67, totalInsertBuckets => 1, recordsPerBucket => 500000
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
200629 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
200637 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 001
200637 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
200736 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
200756 [pool-1853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
200764 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
200765 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
200844 [pool-1853-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
200855 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
200867 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
200867 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
200867 [pool-1854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
200947 [pool-1854-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
200972 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
200994 [pool-1855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
201004 [Executor task launch worker-2] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
201004 [Executor task launch worker-2] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
201081 [pool-1855-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
201141 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
201141 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
201308 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
201308 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
201392 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
201401 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
201401 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
201482 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 002
201743 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
201743 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
201956 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 58 for /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_1_001.parquet
201962 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_1_001.parquet
201962 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 58 results, for file /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_1_001.parquet => [000c1dee-b84e-40a5-b92d-c45b29b6d4ec, 02d937a8-7b00-46a5-acf7-c48f18aa342a, 080b4786-93e9-41db-b8c1-24a64231313c, 0ca82551-10cc-4914-8443-466d00e01b9c, 0f84cf42-2d0e-4a5a-bf6d-71f7b15ef75e, 13565b5c-db73-4b4f-8842-8384f8b2792a, 14a57b73-a37c-412c-a8ba-9aa48abd6b81, 15e53124-207a-4051-a64a-2d9a8482741b, 1ab0b986-43ea-4616-8b87-a14c35be07ed, 1bbd9daa-cee8-4f70-82ca-42a437659f29, 220a73d5-c5f5-456e-ae2f-0f4572ad014c, 25dbbe60-993d-4ec7-ad5c-7a033d51d3bd, 272ee05a-cbe3-4858-a2fb-d0862f093e5a, 3176ed08-ffdd-4558-bd91-7582e9517f16, 35bf40ec-752b-4476-abcd-b306dcd9cebc, 3ed84a85-6521-4fc3-bd83-fae1260940ca, 453c9afa-19fd-4300-9840-664cb6385da2, 4f73388d-ecd2-4469-bbbd-4486c86e8e1b, 5b8b5206-41f3-4773-8b19-f216ec036cf7, 5fdadf31-0ea5-4c59-84a5-a63041871af4, 62f8ee65-5e03-4df8-b0c0-f58e7ce689ad, 7594c85d-ce15-4b96-b500-b4be23ae1d5e, 7d0841ba-6e7c-4152-b79e-8f5849ce87d1, 7d27a4e1-32ba-4136-b4d8-a6d3a78b76f0, 83d2f4d6-6146-4e8e-addd-485630e7f045, 85d8c268-7b2e-4164-8aa1-aec8cce6975c, 85ee699e-db63-4dc6-8b71-afaaf3933103, 917991e9-01ab-44f3-aae5-5818470367ca, 926d9e04-1ef8-4df3-a65f-e4408d016acc, 92b16dda-c462-452a-ae24-f9c94fa98c74, 9b7e1169-f54a-49fb-8ed2-70d17de0b67d, 9ebfba7e-c73d-4699-92fa-7afbd0e4edfb, a004b8ce-7263-4d59-9753-375ce3b9e198, a05cd884-592c-41c1-a667-755091cafcaf, a6e1af65-03a5-451d-8823-93fb1b472f18, ab56c3e6-717d-470d-9dd6-d6d93e37f8b3, ad29706c-0f5e-495e-867b-fd142e3a66a0, ae1e6d3e-7976-4528-bfec-cbb2899a31ba, b596a15d-806a-42f1-88ee-6e2cc6ba1853, c281352b-1913-4d79-98ad-d04c1d7c384d, c7ab69a4-93ea-4150-8d20-5c7404b8261f, c8a0b36c-1f0e-4087-8851-d062bf51fad6, c8aa8ee4-3c60-4fbb-a0d1-26e76f7d787e, cb036f4b-f862-4cae-aa54-82350c88929a, cbe73b8b-a6a6-4f08-a78d-45e78ec22cf1, cbf688e4-565b-4c28-bb36-7bbca160f601, ce6e9a96-ff04-494c-a947-4682140746bb, dc99ae53-7fcf-4a10-ae7c-36c23405656b, dccce64e-71d7-4f91-90c0-a0435f7da599, e277e0b1-a648-42d2-927e-5115aef59c1b, e94fd0a1-02ce-4511-b160-6ff7a98d38fd, f05f1606-0a01-49e7-9a78-0ffe438dadcf, f0821ec8-1069-47be-ac56-042be2d57d21, f284b919-ceee-4618-8e5e-0df2e2fcda55, f3541553-9aa1-4f72-a3e4-a67c28489bfb, f4b9d90a-44b1-4153-93c7-7cde3547d965, f6d8a806-dfa4-427f-a803-dd912017c372, fe8a3e0d-15f4-4f4c-9a31-599952847f88]
201973 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 47 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet
201977 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet
201977 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 47 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet => [02d1be34-1b5a-4ce3-b347-065184cf9da3, 03cad7f8-16e2-41bc-87a6-a2fae04e116c, 043a12a1-0dc4-4db1-a155-83536a6e71a2, 04560301-63ab-40cc-93b4-b4e0c91ac8a0, 078c2108-e9b1-4cfc-9cfb-d5d0bfca4070, 07c41b72-b626-4bc7-b706-c57d4a45a9d1, 0806d770-a4e4-4bd8-8779-dbad988b5382, 0b918373-ffc6-4cef-8d14-a71716c94d65, 0e180d31-10d8-42cd-82dc-6fffa7c84acf, 111744e3-a30e-42a0-abed-9c02c890e1f7, 1351bd92-e9c7-4f64-9638-8cc61a29cf03, 158ecd88-94dc-481e-acf9-4f3fab20f166, 1923df69-3c21-4a8c-b6c4-27f2af9c444d, 19d9bcf7-ddd4-491d-a212-a24240cffeb1, 1c18f64c-4925-41fe-bf10-70da98497514, 1d26e7a4-269b-4284-9634-671435e338bc, 2661aea8-f69b-4105-b9e6-d6b22bdf2a8f, 2ad30686-96d1-4de1-aaf6-a7bb8e8a8b79, 2f861b96-9c20-4845-93ae-83ee75f4ea33, 379c5fad-e704-4c97-a14d-d1e40d909326, 46be8a71-643e-4fb9-8aa8-3350f9560954, 47767838-2f87-4b02-a5f1-9f400a42856a, 4ad09344-1162-425a-82f1-90c9ee75ee74, 4cfc6224-da4e-4629-a0cb-6137d02b0ef4, 4f782600-abae-42f1-8be2-4876726b76c5, 52ec9f62-a1a5-4652-b3bf-8215337814ed, 53cb0098-ab78-493a-92d3-57db82c0d280, 5501944f-bdf7-4f74-85eb-269ecf1d7f03, 55835955-909d-4373-8f1e-e7162ef3e9db, 5be1511c-ab28-4f2e-b3c4-04052b8bddc4, 5db2da7a-c4fb-440a-86be-8005b80fb300, 632ea77a-9d6b-4275-905f-a184396e813b, 67c49ebf-86c2-49f6-b95f-d375dc22797d, 6a76ac16-0406-45e8-a473-17d6b2e701da, 7219101e-c604-4894-8a97-5a9a076eddfd, 73ff1f71-6997-49fc-a160-982c092f847f, 89eb81c3-a1d4-4ff2-b8a0-f3d5cf5404ab, 8d1217ba-169f-4898-8e8d-04c60389b29d, 8e01fcbf-3a0c-42c3-99d5-db741d39943c, 8e3abe35-c802-468a-9166-0522af4943f2, 9c244c61-41bc-4c3d-8533-1f61e0466891, 9dab9c30-43ae-49da-8035-47dcacc4eeb2, 9f1480a8-eef7-4a27-9f42-d94807fdc137, 9f193724-5615-43dc-bed6-3ce9b528b063, a1cbb142-e04d-4869-a9c0-a10a8eb015db, a5b858fc-776e-43c8-a5a5-64952118f60e, aa1ec677-6e7b-4fef-af6b-072de91894ca]
202000 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet
202004 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet
202004 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_2_001.parquet => [b1dd06f2-532c-4d97-8494-10c7c286ba67, b4132744-882a-4295-855d-76e56bf364ea, b5ecfb7f-069b-402a-9751-a5913e52f2df, b7d19603-f09a-4b2b-b1b3-7795de0efbca, c375ef3e-df82-46ae-820c-7899a0444199, c64bf482-2948-4938-9f48-e2404d6a2d80, c6d87681-4c63-4197-b9f0-22c105a89761, cdfbd619-ec0b-44e4-a4ac-8ce76d4d9513, db6dcd89-61e2-4b97-86dc-246546245f27, df14abcc-fa54-4c60-a9ac-7a27d78d4d94, e0fa4539-cb42-4136-9021-7c0535837de8, e4cff470-da17-4b40-82b0-81789477e037, e8e7c6a6-5882-4c72-9627-1e44c2753244, e98dfdc4-70b6-40f9-8928-7908b4bd68ae, ea3f7fc0-fff7-4e76-b7ff-327dd50bfecb, ec7e788f-d193-46db-8514-95f14e27d7fc, ef3d9099-7633-487a-ade3-a3bb020bb889, f198fb4c-3ded-4e57-97da-6fa40a4fb35a, ff4851b5-7b1f-48c0-80e8-80eca41a8ec1, ff6365e6-cd19-49db-ba63-6885ce05d8d1]
202014 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_0_001.parquet
202019 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_0_001.parquet
202020 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_0_001.parquet => [0196c132-2f01-44a7-90f2-fa75b9746a29, 05077806-e863-4ad6-809a-94847264aec3, 05bf5689-dd0c-48e0-97d6-4ca759f61f18, 0688b391-a7e1-49a3-87b2-eaa3de287c30, 0fa08d5d-f81b-4f70-a7b7-55abeb0b1922, 11b3ddad-9e82-493a-95b3-3c001f7e93ae, 16aaa0ba-8129-44ee-9fd5-5540f8dd0dcb, 179ffdb6-512e-495e-9d10-8430bce4615a, 1a4cf1b7-d5c6-4ca6-8cd1-560ba4ba3e53, 1b8f86ac-a14f-487c-bd48-3cdb29782227, 1d74f77a-9c0f-41de-9db7-d46f643a4b2a, 1e1c0838-6747-46ff-a1be-6a213c09991d, 1fce8476-7fc9-465a-bfbe-dadf6a98625e, 21185614-6097-42cc-b040-c1005b2e489d, 271ad7f0-25c9-4cd6-b20a-9fa479234903, 27a8d46f-cfc6-43d7-95e7-17adb99020a9, 2eff8d6c-413e-4e59-adcc-2d90647cb2f8, 308a148d-940e-48aa-bd4c-f804be243d2d, 32b93a2f-b2f5-4569-a6c9-2bae1322d115, 38f0c189-36a4-4c23-af31-837ee433d128, 3b7451f2-e0bf-4f99-85e4-fc2deb54213a, 3de56032-fa07-4c5e-a2b0-16f99ade19ca, 3e8548e8-70ef-455a-b868-faf1c8105a08, 3fd03699-94a4-4cba-a4f7-cd31814f0f74, 4113357e-f13c-41e3-8831-7cb14f17aa6c, 49bd4dd2-87f4-41f3-8a9c-102d84683a19, 4fec8b68-841e-42a0-923e-d3b27faee7f8, 52022913-85fa-4806-9539-fd3b88c6b546, 55bf2915-a80d-414c-a89c-207da57e4a4f, 57b86878-54ab-4eee-a9d7-c546b4a8ed20, 6e31edad-8981-4920-944c-9678ff2d67ec, 6ee7506d-237e-4e26-8baa-1fafa1f893d2, 78f920a5-2bb9-4860-9349-bbd75be9ac59, 7b400dc4-60ca-41cf-8f43-3485ca408648, 7e739f1b-9fa2-4e89-b9d6-ccca3b0b2b74, 82d4e2d5-b840-44b9-8ba5-3b713b7c09c7, 83c2ebcb-6804-48bf-8aaf-53dd71d6f3a7, 83d72e80-b963-4129-a797-c621b39e848b, 843920f0-b499-48a0-acad-cf6f181573d7, 84640fca-a9b4-404f-ae37-b649431e585f, 87664584-6a35-4a87-9bc9-3468277b12e9, 8b317fce-09c8-4afd-9b0c-4d25b6963fb3, 8bc8f242-2682-4075-bb77-24a150bcab0c, 8dcbb942-8686-4dcb-bf44-44a5a8083590, 94f1f225-fe0b-4fa5-ab88-3f1fc5a2f70c, 9d2ac0e9-fa19-41cb-977b-7ccb510ad767, a12157ac-370f-48c7-8db5-be305e634fc3, a36f0ffd-0d05-4822-bd87-6e4a29f53526, a55ff461-1852-4a9b-81d4-2d9dc0cca9fb, a5abbd48-0009-43a1-9e1b-3ded612cf7f6, a884f195-5cec-4a6f-ae66-46e449c39a5f, a9480e61-4be7-4407-b03e-0a1534cb7fe1, aa4123f9-5cc9-4906-9896-75b74b74184a, b1de67f3-afba-4add-ae60-f8bf68e6e702, b8f80ff9-3d6e-4ae7-829c-498c4e7ef8b9, bac222ef-5b61-449d-9076-b04e244b3fd0, bd18f29d-15a9-4f7c-9710-b6875a10ce3e, bf4544e6-254f-4ca5-900f-fb285145ac16, c86e131a-ffcf-4741-85a2-da7914c1c050, d20a0e99-8f4e-433a-8b5b-83df5ad2c5ec, d40f6e07-d8c3-4d7b-a45d-3f1cfa9e6219, d611d3ea-47d7-4d41-a27c-6a692b3f7eaa, d8ae9913-9c90-4291-853e-559866fcb7ea, d9712a12-f4e6-4097-8c58-77bb1e7e60d0, de942f4c-ea17-4ab5-8a2d-d38956e15a73, e1eb790a-85f6-44ec-adb8-d0527edf5206, e9360656-2d8f-4789-9a4e-b4230b23aaf0, ef980cfc-6f00-4fbc-aea0-d6d936a135e9, efb73e24-2d48-4f90-903d-46ce11cbd006, f1753e28-9ef7-4763-afe0-5201e43c9190, f511f8c6-ec01-4774-a198-bb176e2df028, fc633abe-b08a-44c8-b482-5fc73132dda2, fd4fc9b4-4eae-45b0-9692-9dbb6854ca02, ff0b95e6-f30f-4f51-ac48-cd36097a64dd, ff4c0fa8-4a05-4f5c-8ad4-f50d2f9098fa]
202153 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=75}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=58}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=67}}}
202190 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
202190 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=6b0837f9-c8b5-4b39-a03f-dd461cca6f2e}, 1=BucketInfo {bucketType=UPDATE, fileLoc=e900712b-1a33-4830-a458-0ed5ffd59f7f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4eceecc3-309f-46c2-be01-323a266dc2c3}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{6b0837f9-c8b5-4b39-a03f-dd461cca6f2e=0, e900712b-1a33-4830-a458-0ed5ffd59f7f=1, 4eceecc3-309f-46c2-be01-323a266dc2c3=2}
202198 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 002
202198 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 002
202765 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
202765 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
202923 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
202924 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
203032 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
203048 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 002 as complete
203048 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 002
203103 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepointing latest commit 002
203262 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2016/03/15
203266 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/16
203266 [Executor task launch worker-0] INFO  com.uber.hoodie.HoodieWriteClient  - Collecting latest files in partition path 2015/03/17
203275 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Savepoint 002 created
203275 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 003
203572 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
203572 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
203752 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 58 for /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_002.parquet
203792 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_002.parquet
203792 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 58 results, for file /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_002.parquet => [000c1dee-b84e-40a5-b92d-c45b29b6d4ec, 02d937a8-7b00-46a5-acf7-c48f18aa342a, 080b4786-93e9-41db-b8c1-24a64231313c, 0ca82551-10cc-4914-8443-466d00e01b9c, 0f84cf42-2d0e-4a5a-bf6d-71f7b15ef75e, 13565b5c-db73-4b4f-8842-8384f8b2792a, 14a57b73-a37c-412c-a8ba-9aa48abd6b81, 15e53124-207a-4051-a64a-2d9a8482741b, 1ab0b986-43ea-4616-8b87-a14c35be07ed, 1bbd9daa-cee8-4f70-82ca-42a437659f29, 220a73d5-c5f5-456e-ae2f-0f4572ad014c, 25dbbe60-993d-4ec7-ad5c-7a033d51d3bd, 272ee05a-cbe3-4858-a2fb-d0862f093e5a, 3176ed08-ffdd-4558-bd91-7582e9517f16, 35bf40ec-752b-4476-abcd-b306dcd9cebc, 3ed84a85-6521-4fc3-bd83-fae1260940ca, 453c9afa-19fd-4300-9840-664cb6385da2, 4f73388d-ecd2-4469-bbbd-4486c86e8e1b, 5b8b5206-41f3-4773-8b19-f216ec036cf7, 5fdadf31-0ea5-4c59-84a5-a63041871af4, 62f8ee65-5e03-4df8-b0c0-f58e7ce689ad, 7594c85d-ce15-4b96-b500-b4be23ae1d5e, 7d0841ba-6e7c-4152-b79e-8f5849ce87d1, 7d27a4e1-32ba-4136-b4d8-a6d3a78b76f0, 83d2f4d6-6146-4e8e-addd-485630e7f045, 85d8c268-7b2e-4164-8aa1-aec8cce6975c, 85ee699e-db63-4dc6-8b71-afaaf3933103, 917991e9-01ab-44f3-aae5-5818470367ca, 926d9e04-1ef8-4df3-a65f-e4408d016acc, 92b16dda-c462-452a-ae24-f9c94fa98c74, 9b7e1169-f54a-49fb-8ed2-70d17de0b67d, 9ebfba7e-c73d-4699-92fa-7afbd0e4edfb, a004b8ce-7263-4d59-9753-375ce3b9e198, a05cd884-592c-41c1-a667-755091cafcaf, a6e1af65-03a5-451d-8823-93fb1b472f18, ab56c3e6-717d-470d-9dd6-d6d93e37f8b3, ad29706c-0f5e-495e-867b-fd142e3a66a0, ae1e6d3e-7976-4528-bfec-cbb2899a31ba, b596a15d-806a-42f1-88ee-6e2cc6ba1853, c281352b-1913-4d79-98ad-d04c1d7c384d, c7ab69a4-93ea-4150-8d20-5c7404b8261f, c8a0b36c-1f0e-4087-8851-d062bf51fad6, c8aa8ee4-3c60-4fbb-a0d1-26e76f7d787e, cb036f4b-f862-4cae-aa54-82350c88929a, cbe73b8b-a6a6-4f08-a78d-45e78ec22cf1, cbf688e4-565b-4c28-bb36-7bbca160f601, ce6e9a96-ff04-494c-a947-4682140746bb, dc99ae53-7fcf-4a10-ae7c-36c23405656b, dccce64e-71d7-4f91-90c0-a0435f7da599, e277e0b1-a648-42d2-927e-5115aef59c1b, e94fd0a1-02ce-4511-b160-6ff7a98d38fd, f05f1606-0a01-49e7-9a78-0ffe438dadcf, f0821ec8-1069-47be-ac56-042be2d57d21, f284b919-ceee-4618-8e5e-0df2e2fcda55, f3541553-9aa1-4f72-a3e4-a67c28489bfb, f4b9d90a-44b1-4153-93c7-7cde3547d965, f6d8a806-dfa4-427f-a803-dd912017c372, fe8a3e0d-15f4-4f4c-9a31-599952847f88]
203802 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 47 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet
203822 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet
203822 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 47 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet => [02d1be34-1b5a-4ce3-b347-065184cf9da3, 03cad7f8-16e2-41bc-87a6-a2fae04e116c, 043a12a1-0dc4-4db1-a155-83536a6e71a2, 04560301-63ab-40cc-93b4-b4e0c91ac8a0, 078c2108-e9b1-4cfc-9cfb-d5d0bfca4070, 07c41b72-b626-4bc7-b706-c57d4a45a9d1, 0806d770-a4e4-4bd8-8779-dbad988b5382, 0b918373-ffc6-4cef-8d14-a71716c94d65, 0e180d31-10d8-42cd-82dc-6fffa7c84acf, 111744e3-a30e-42a0-abed-9c02c890e1f7, 1351bd92-e9c7-4f64-9638-8cc61a29cf03, 158ecd88-94dc-481e-acf9-4f3fab20f166, 1923df69-3c21-4a8c-b6c4-27f2af9c444d, 19d9bcf7-ddd4-491d-a212-a24240cffeb1, 1c18f64c-4925-41fe-bf10-70da98497514, 1d26e7a4-269b-4284-9634-671435e338bc, 2661aea8-f69b-4105-b9e6-d6b22bdf2a8f, 2ad30686-96d1-4de1-aaf6-a7bb8e8a8b79, 2f861b96-9c20-4845-93ae-83ee75f4ea33, 379c5fad-e704-4c97-a14d-d1e40d909326, 46be8a71-643e-4fb9-8aa8-3350f9560954, 47767838-2f87-4b02-a5f1-9f400a42856a, 4ad09344-1162-425a-82f1-90c9ee75ee74, 4cfc6224-da4e-4629-a0cb-6137d02b0ef4, 4f782600-abae-42f1-8be2-4876726b76c5, 52ec9f62-a1a5-4652-b3bf-8215337814ed, 53cb0098-ab78-493a-92d3-57db82c0d280, 5501944f-bdf7-4f74-85eb-269ecf1d7f03, 55835955-909d-4373-8f1e-e7162ef3e9db, 5be1511c-ab28-4f2e-b3c4-04052b8bddc4, 5db2da7a-c4fb-440a-86be-8005b80fb300, 632ea77a-9d6b-4275-905f-a184396e813b, 67c49ebf-86c2-49f6-b95f-d375dc22797d, 6a76ac16-0406-45e8-a473-17d6b2e701da, 7219101e-c604-4894-8a97-5a9a076eddfd, 73ff1f71-6997-49fc-a160-982c092f847f, 89eb81c3-a1d4-4ff2-b8a0-f3d5cf5404ab, 8d1217ba-169f-4898-8e8d-04c60389b29d, 8e01fcbf-3a0c-42c3-99d5-db741d39943c, 8e3abe35-c802-468a-9166-0522af4943f2, 9c244c61-41bc-4c3d-8533-1f61e0466891, 9dab9c30-43ae-49da-8035-47dcacc4eeb2, 9f1480a8-eef7-4a27-9f42-d94807fdc137, 9f193724-5615-43dc-bed6-3ce9b528b063, a1cbb142-e04d-4869-a9c0-a10a8eb015db, a5b858fc-776e-43c8-a5a5-64952118f60e, aa1ec677-6e7b-4fef-af6b-072de91894ca]
203867 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 20 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet
203890 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet
203890 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 20 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_002.parquet => [b1dd06f2-532c-4d97-8494-10c7c286ba67, b4132744-882a-4295-855d-76e56bf364ea, b5ecfb7f-069b-402a-9751-a5913e52f2df, b7d19603-f09a-4b2b-b1b3-7795de0efbca, c375ef3e-df82-46ae-820c-7899a0444199, c64bf482-2948-4938-9f48-e2404d6a2d80, c6d87681-4c63-4197-b9f0-22c105a89761, cdfbd619-ec0b-44e4-a4ac-8ce76d4d9513, db6dcd89-61e2-4b97-86dc-246546245f27, df14abcc-fa54-4c60-a9ac-7a27d78d4d94, e0fa4539-cb42-4136-9021-7c0535837de8, e4cff470-da17-4b40-82b0-81789477e037, e8e7c6a6-5882-4c72-9627-1e44c2753244, e98dfdc4-70b6-40f9-8928-7908b4bd68ae, ea3f7fc0-fff7-4e76-b7ff-327dd50bfecb, ec7e788f-d193-46db-8514-95f14e27d7fc, ef3d9099-7633-487a-ade3-a3bb020bb889, f198fb4c-3ded-4e57-97da-6fa40a4fb35a, ff4851b5-7b1f-48c0-80e8-80eca41a8ec1, ff6365e6-cd19-49db-ba63-6885ce05d8d1]
203901 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_002.parquet
203918 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_002.parquet
203918 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_002.parquet => [0196c132-2f01-44a7-90f2-fa75b9746a29, 05077806-e863-4ad6-809a-94847264aec3, 05bf5689-dd0c-48e0-97d6-4ca759f61f18, 0688b391-a7e1-49a3-87b2-eaa3de287c30, 0fa08d5d-f81b-4f70-a7b7-55abeb0b1922, 11b3ddad-9e82-493a-95b3-3c001f7e93ae, 16aaa0ba-8129-44ee-9fd5-5540f8dd0dcb, 179ffdb6-512e-495e-9d10-8430bce4615a, 1a4cf1b7-d5c6-4ca6-8cd1-560ba4ba3e53, 1b8f86ac-a14f-487c-bd48-3cdb29782227, 1d74f77a-9c0f-41de-9db7-d46f643a4b2a, 1e1c0838-6747-46ff-a1be-6a213c09991d, 1fce8476-7fc9-465a-bfbe-dadf6a98625e, 21185614-6097-42cc-b040-c1005b2e489d, 271ad7f0-25c9-4cd6-b20a-9fa479234903, 27a8d46f-cfc6-43d7-95e7-17adb99020a9, 2eff8d6c-413e-4e59-adcc-2d90647cb2f8, 308a148d-940e-48aa-bd4c-f804be243d2d, 32b93a2f-b2f5-4569-a6c9-2bae1322d115, 38f0c189-36a4-4c23-af31-837ee433d128, 3b7451f2-e0bf-4f99-85e4-fc2deb54213a, 3de56032-fa07-4c5e-a2b0-16f99ade19ca, 3e8548e8-70ef-455a-b868-faf1c8105a08, 3fd03699-94a4-4cba-a4f7-cd31814f0f74, 4113357e-f13c-41e3-8831-7cb14f17aa6c, 49bd4dd2-87f4-41f3-8a9c-102d84683a19, 4fec8b68-841e-42a0-923e-d3b27faee7f8, 52022913-85fa-4806-9539-fd3b88c6b546, 55bf2915-a80d-414c-a89c-207da57e4a4f, 57b86878-54ab-4eee-a9d7-c546b4a8ed20, 6e31edad-8981-4920-944c-9678ff2d67ec, 6ee7506d-237e-4e26-8baa-1fafa1f893d2, 78f920a5-2bb9-4860-9349-bbd75be9ac59, 7b400dc4-60ca-41cf-8f43-3485ca408648, 7e739f1b-9fa2-4e89-b9d6-ccca3b0b2b74, 82d4e2d5-b840-44b9-8ba5-3b713b7c09c7, 83c2ebcb-6804-48bf-8aaf-53dd71d6f3a7, 83d72e80-b963-4129-a797-c621b39e848b, 843920f0-b499-48a0-acad-cf6f181573d7, 84640fca-a9b4-404f-ae37-b649431e585f, 87664584-6a35-4a87-9bc9-3468277b12e9, 8b317fce-09c8-4afd-9b0c-4d25b6963fb3, 8bc8f242-2682-4075-bb77-24a150bcab0c, 8dcbb942-8686-4dcb-bf44-44a5a8083590, 94f1f225-fe0b-4fa5-ab88-3f1fc5a2f70c, 9d2ac0e9-fa19-41cb-977b-7ccb510ad767, a12157ac-370f-48c7-8db5-be305e634fc3, a36f0ffd-0d05-4822-bd87-6e4a29f53526, a55ff461-1852-4a9b-81d4-2d9dc0cca9fb, a5abbd48-0009-43a1-9e1b-3ded612cf7f6, a884f195-5cec-4a6f-ae66-46e449c39a5f, a9480e61-4be7-4407-b03e-0a1534cb7fe1, aa4123f9-5cc9-4906-9896-75b74b74184a, b1de67f3-afba-4add-ae60-f8bf68e6e702, b8f80ff9-3d6e-4ae7-829c-498c4e7ef8b9, bac222ef-5b61-449d-9076-b04e244b3fd0, bd18f29d-15a9-4f7c-9710-b6875a10ce3e, bf4544e6-254f-4ca5-900f-fb285145ac16, c86e131a-ffcf-4741-85a2-da7914c1c050, d20a0e99-8f4e-433a-8b5b-83df5ad2c5ec, d40f6e07-d8c3-4d7b-a45d-3f1cfa9e6219, d611d3ea-47d7-4d41-a27c-6a692b3f7eaa, d8ae9913-9c90-4291-853e-559866fcb7ea, d9712a12-f4e6-4097-8c58-77bb1e7e60d0, de942f4c-ea17-4ab5-8a2d-d38956e15a73, e1eb790a-85f6-44ec-adb8-d0527edf5206, e9360656-2d8f-4789-9a4e-b4230b23aaf0, ef980cfc-6f00-4fbc-aea0-d6d936a135e9, efb73e24-2d48-4f90-903d-46ce11cbd006, f1753e28-9ef7-4763-afe0-5201e43c9190, f511f8c6-ec01-4774-a198-bb176e2df028, fc633abe-b08a-44c8-b482-5fc73132dda2, fd4fc9b4-4eae-45b0-9692-9dbb6854ca02, ff0b95e6-f30f-4f51-ac48-cd36097a64dd, ff4c0fa8-4a05-4f5c-8ad4-f50d2f9098fa]
204038 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=75}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=58}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=67}}}
204110 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
204110 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=6b0837f9-c8b5-4b39-a03f-dd461cca6f2e}, 1=BucketInfo {bucketType=UPDATE, fileLoc=e900712b-1a33-4830-a458-0ed5ffd59f7f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4eceecc3-309f-46c2-be01-323a266dc2c3}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{6b0837f9-c8b5-4b39-a03f-dd461cca6f2e=0, e900712b-1a33-4830-a458-0ed5ffd59f7f=1, 4eceecc3-309f-46c2-be01-323a266dc2c3=2}
204125 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 003
204125 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 003
204560 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
204560 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
204673 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
204673 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
204790 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 3 files
204798 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 003 as complete
204798 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 003
204964 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 004
205246 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 200
205247 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
205430 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 58 for /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_003.parquet
205435 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 58 row keys from /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_003.parquet
205435 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 58 results, for file /tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_003.parquet => [000c1dee-b84e-40a5-b92d-c45b29b6d4ec, 02d937a8-7b00-46a5-acf7-c48f18aa342a, 080b4786-93e9-41db-b8c1-24a64231313c, 0ca82551-10cc-4914-8443-466d00e01b9c, 0f84cf42-2d0e-4a5a-bf6d-71f7b15ef75e, 13565b5c-db73-4b4f-8842-8384f8b2792a, 14a57b73-a37c-412c-a8ba-9aa48abd6b81, 15e53124-207a-4051-a64a-2d9a8482741b, 1ab0b986-43ea-4616-8b87-a14c35be07ed, 1bbd9daa-cee8-4f70-82ca-42a437659f29, 220a73d5-c5f5-456e-ae2f-0f4572ad014c, 25dbbe60-993d-4ec7-ad5c-7a033d51d3bd, 272ee05a-cbe3-4858-a2fb-d0862f093e5a, 3176ed08-ffdd-4558-bd91-7582e9517f16, 35bf40ec-752b-4476-abcd-b306dcd9cebc, 3ed84a85-6521-4fc3-bd83-fae1260940ca, 453c9afa-19fd-4300-9840-664cb6385da2, 4f73388d-ecd2-4469-bbbd-4486c86e8e1b, 5b8b5206-41f3-4773-8b19-f216ec036cf7, 5fdadf31-0ea5-4c59-84a5-a63041871af4, 62f8ee65-5e03-4df8-b0c0-f58e7ce689ad, 7594c85d-ce15-4b96-b500-b4be23ae1d5e, 7d0841ba-6e7c-4152-b79e-8f5849ce87d1, 7d27a4e1-32ba-4136-b4d8-a6d3a78b76f0, 83d2f4d6-6146-4e8e-addd-485630e7f045, 85d8c268-7b2e-4164-8aa1-aec8cce6975c, 85ee699e-db63-4dc6-8b71-afaaf3933103, 917991e9-01ab-44f3-aae5-5818470367ca, 926d9e04-1ef8-4df3-a65f-e4408d016acc, 92b16dda-c462-452a-ae24-f9c94fa98c74, 9b7e1169-f54a-49fb-8ed2-70d17de0b67d, 9ebfba7e-c73d-4699-92fa-7afbd0e4edfb, a004b8ce-7263-4d59-9753-375ce3b9e198, a05cd884-592c-41c1-a667-755091cafcaf, a6e1af65-03a5-451d-8823-93fb1b472f18, ab56c3e6-717d-470d-9dd6-d6d93e37f8b3, ad29706c-0f5e-495e-867b-fd142e3a66a0, ae1e6d3e-7976-4528-bfec-cbb2899a31ba, b596a15d-806a-42f1-88ee-6e2cc6ba1853, c281352b-1913-4d79-98ad-d04c1d7c384d, c7ab69a4-93ea-4150-8d20-5c7404b8261f, c8a0b36c-1f0e-4087-8851-d062bf51fad6, c8aa8ee4-3c60-4fbb-a0d1-26e76f7d787e, cb036f4b-f862-4cae-aa54-82350c88929a, cbe73b8b-a6a6-4f08-a78d-45e78ec22cf1, cbf688e4-565b-4c28-bb36-7bbca160f601, ce6e9a96-ff04-494c-a947-4682140746bb, dc99ae53-7fcf-4a10-ae7c-36c23405656b, dccce64e-71d7-4f91-90c0-a0435f7da599, e277e0b1-a648-42d2-927e-5115aef59c1b, e94fd0a1-02ce-4511-b160-6ff7a98d38fd, f05f1606-0a01-49e7-9a78-0ffe438dadcf, f0821ec8-1069-47be-ac56-042be2d57d21, f284b919-ceee-4618-8e5e-0df2e2fcda55, f3541553-9aa1-4f72-a3e4-a67c28489bfb, f4b9d90a-44b1-4153-93c7-7cde3547d965, f6d8a806-dfa4-427f-a803-dd912017c372, fe8a3e0d-15f4-4f4c-9a31-599952847f88]
205459 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 38 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet
205477 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet
205477 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 38 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet => [02d1be34-1b5a-4ce3-b347-065184cf9da3, 03cad7f8-16e2-41bc-87a6-a2fae04e116c, 043a12a1-0dc4-4db1-a155-83536a6e71a2, 04560301-63ab-40cc-93b4-b4e0c91ac8a0, 078c2108-e9b1-4cfc-9cfb-d5d0bfca4070, 07c41b72-b626-4bc7-b706-c57d4a45a9d1, 0806d770-a4e4-4bd8-8779-dbad988b5382, 0b918373-ffc6-4cef-8d14-a71716c94d65, 0e180d31-10d8-42cd-82dc-6fffa7c84acf, 111744e3-a30e-42a0-abed-9c02c890e1f7, 1351bd92-e9c7-4f64-9638-8cc61a29cf03, 158ecd88-94dc-481e-acf9-4f3fab20f166, 1923df69-3c21-4a8c-b6c4-27f2af9c444d, 19d9bcf7-ddd4-491d-a212-a24240cffeb1, 1c18f64c-4925-41fe-bf10-70da98497514, 1d26e7a4-269b-4284-9634-671435e338bc, 2661aea8-f69b-4105-b9e6-d6b22bdf2a8f, 2ad30686-96d1-4de1-aaf6-a7bb8e8a8b79, 2f861b96-9c20-4845-93ae-83ee75f4ea33, 379c5fad-e704-4c97-a14d-d1e40d909326, 46be8a71-643e-4fb9-8aa8-3350f9560954, 47767838-2f87-4b02-a5f1-9f400a42856a, 4ad09344-1162-425a-82f1-90c9ee75ee74, 4cfc6224-da4e-4629-a0cb-6137d02b0ef4, 4f782600-abae-42f1-8be2-4876726b76c5, 52ec9f62-a1a5-4652-b3bf-8215337814ed, 53cb0098-ab78-493a-92d3-57db82c0d280, 5501944f-bdf7-4f74-85eb-269ecf1d7f03, 55835955-909d-4373-8f1e-e7162ef3e9db, 5be1511c-ab28-4f2e-b3c4-04052b8bddc4, 5db2da7a-c4fb-440a-86be-8005b80fb300, 632ea77a-9d6b-4275-905f-a184396e813b, 67c49ebf-86c2-49f6-b95f-d375dc22797d, 6a76ac16-0406-45e8-a473-17d6b2e701da, 7219101e-c604-4894-8a97-5a9a076eddfd, 73ff1f71-6997-49fc-a160-982c092f847f, 89eb81c3-a1d4-4ff2-b8a0-f3d5cf5404ab, 8d1217ba-169f-4898-8e8d-04c60389b29d]
205505 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 29 for /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet
205510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 67 row keys from /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet
205510 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 29 results, for file /tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet => [8e01fcbf-3a0c-42c3-99d5-db741d39943c, 8e3abe35-c802-468a-9166-0522af4943f2, 9c244c61-41bc-4c3d-8533-1f61e0466891, 9dab9c30-43ae-49da-8035-47dcacc4eeb2, 9f1480a8-eef7-4a27-9f42-d94807fdc137, 9f193724-5615-43dc-bed6-3ce9b528b063, a1cbb142-e04d-4869-a9c0-a10a8eb015db, a5b858fc-776e-43c8-a5a5-64952118f60e, aa1ec677-6e7b-4fef-af6b-072de91894ca, b1dd06f2-532c-4d97-8494-10c7c286ba67, b4132744-882a-4295-855d-76e56bf364ea, b5ecfb7f-069b-402a-9751-a5913e52f2df, b7d19603-f09a-4b2b-b1b3-7795de0efbca, c375ef3e-df82-46ae-820c-7899a0444199, c64bf482-2948-4938-9f48-e2404d6a2d80, c6d87681-4c63-4197-b9f0-22c105a89761, cdfbd619-ec0b-44e4-a4ac-8ce76d4d9513, db6dcd89-61e2-4b97-86dc-246546245f27, df14abcc-fa54-4c60-a9ac-7a27d78d4d94, e0fa4539-cb42-4136-9021-7c0535837de8, e4cff470-da17-4b40-82b0-81789477e037, e8e7c6a6-5882-4c72-9627-1e44c2753244, e98dfdc4-70b6-40f9-8928-7908b4bd68ae, ea3f7fc0-fff7-4e76-b7ff-327dd50bfecb, ec7e788f-d193-46db-8514-95f14e27d7fc, ef3d9099-7633-487a-ade3-a3bb020bb889, f198fb4c-3ded-4e57-97da-6fa40a4fb35a, ff4851b5-7b1f-48c0-80e8-80eca41a8ec1, ff6365e6-cd19-49db-ba63-6885ce05d8d1]
205520 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 75 for /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_003.parquet
205528 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 75 row keys from /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_003.parquet
205528 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 75 results, for file /tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_003.parquet => [0196c132-2f01-44a7-90f2-fa75b9746a29, 05077806-e863-4ad6-809a-94847264aec3, 05bf5689-dd0c-48e0-97d6-4ca759f61f18, 0688b391-a7e1-49a3-87b2-eaa3de287c30, 0fa08d5d-f81b-4f70-a7b7-55abeb0b1922, 11b3ddad-9e82-493a-95b3-3c001f7e93ae, 16aaa0ba-8129-44ee-9fd5-5540f8dd0dcb, 179ffdb6-512e-495e-9d10-8430bce4615a, 1a4cf1b7-d5c6-4ca6-8cd1-560ba4ba3e53, 1b8f86ac-a14f-487c-bd48-3cdb29782227, 1d74f77a-9c0f-41de-9db7-d46f643a4b2a, 1e1c0838-6747-46ff-a1be-6a213c09991d, 1fce8476-7fc9-465a-bfbe-dadf6a98625e, 21185614-6097-42cc-b040-c1005b2e489d, 271ad7f0-25c9-4cd6-b20a-9fa479234903, 27a8d46f-cfc6-43d7-95e7-17adb99020a9, 2eff8d6c-413e-4e59-adcc-2d90647cb2f8, 308a148d-940e-48aa-bd4c-f804be243d2d, 32b93a2f-b2f5-4569-a6c9-2bae1322d115, 38f0c189-36a4-4c23-af31-837ee433d128, 3b7451f2-e0bf-4f99-85e4-fc2deb54213a, 3de56032-fa07-4c5e-a2b0-16f99ade19ca, 3e8548e8-70ef-455a-b868-faf1c8105a08, 3fd03699-94a4-4cba-a4f7-cd31814f0f74, 4113357e-f13c-41e3-8831-7cb14f17aa6c, 49bd4dd2-87f4-41f3-8a9c-102d84683a19, 4fec8b68-841e-42a0-923e-d3b27faee7f8, 52022913-85fa-4806-9539-fd3b88c6b546, 55bf2915-a80d-414c-a89c-207da57e4a4f, 57b86878-54ab-4eee-a9d7-c546b4a8ed20, 6e31edad-8981-4920-944c-9678ff2d67ec, 6ee7506d-237e-4e26-8baa-1fafa1f893d2, 78f920a5-2bb9-4860-9349-bbd75be9ac59, 7b400dc4-60ca-41cf-8f43-3485ca408648, 7e739f1b-9fa2-4e89-b9d6-ccca3b0b2b74, 82d4e2d5-b840-44b9-8ba5-3b713b7c09c7, 83c2ebcb-6804-48bf-8aaf-53dd71d6f3a7, 83d72e80-b963-4129-a797-c621b39e848b, 843920f0-b499-48a0-acad-cf6f181573d7, 84640fca-a9b4-404f-ae37-b649431e585f, 87664584-6a35-4a87-9bc9-3468277b12e9, 8b317fce-09c8-4afd-9b0c-4d25b6963fb3, 8bc8f242-2682-4075-bb77-24a150bcab0c, 8dcbb942-8686-4dcb-bf44-44a5a8083590, 94f1f225-fe0b-4fa5-ab88-3f1fc5a2f70c, 9d2ac0e9-fa19-41cb-977b-7ccb510ad767, a12157ac-370f-48c7-8db5-be305e634fc3, a36f0ffd-0d05-4822-bd87-6e4a29f53526, a55ff461-1852-4a9b-81d4-2d9dc0cca9fb, a5abbd48-0009-43a1-9e1b-3ded612cf7f6, a884f195-5cec-4a6f-ae66-46e449c39a5f, a9480e61-4be7-4407-b03e-0a1534cb7fe1, aa4123f9-5cc9-4906-9896-75b74b74184a, b1de67f3-afba-4add-ae60-f8bf68e6e702, b8f80ff9-3d6e-4ae7-829c-498c4e7ef8b9, bac222ef-5b61-449d-9076-b04e244b3fd0, bd18f29d-15a9-4f7c-9710-b6875a10ce3e, bf4544e6-254f-4ca5-900f-fb285145ac16, c86e131a-ffcf-4741-85a2-da7914c1c050, d20a0e99-8f4e-433a-8b5b-83df5ad2c5ec, d40f6e07-d8c3-4d7b-a45d-3f1cfa9e6219, d611d3ea-47d7-4d41-a27c-6a692b3f7eaa, d8ae9913-9c90-4291-853e-559866fcb7ea, d9712a12-f4e6-4097-8c58-77bb1e7e60d0, de942f4c-ea17-4ab5-8a2d-d38956e15a73, e1eb790a-85f6-44ec-adb8-d0527edf5206, e9360656-2d8f-4789-9a4e-b4230b23aaf0, ef980cfc-6f00-4fbc-aea0-d6d936a135e9, efb73e24-2d48-4f90-903d-46ce11cbd006, f1753e28-9ef7-4763-afe0-5201e43c9190, f511f8c6-ec01-4774-a198-bb176e2df028, fc633abe-b08a-44c8-b482-5fc73132dda2, fd4fc9b4-4eae-45b0-9692-9dbb6854ca02, ff0b95e6-f30f-4f51-ac48-cd36097a64dd, ff4c0fa8-4a05-4f5c-8ad4-f50d2f9098fa]
205633 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=200}, partitionStat={2016/03/15=WorkloadStat {numInserts=0, numUpdates=75}, 2015/03/16=WorkloadStat {numInserts=0, numUpdates=58}, 2015/03/17=WorkloadStat {numInserts=0, numUpdates=67}}}
205656 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 6614
205656 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=UPDATE, fileLoc=6b0837f9-c8b5-4b39-a03f-dd461cca6f2e}, 1=BucketInfo {bucketType=UPDATE, fileLoc=e900712b-1a33-4830-a458-0ed5ffd59f7f}, 2=BucketInfo {bucketType=UPDATE, fileLoc=4eceecc3-309f-46c2-be01-323a266dc2c3}}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{6b0837f9-c8b5-4b39-a03f-dd461cca6f2e=0, e900712b-1a33-4830-a458-0ed5ffd59f7f=1, 4eceecc3-309f-46c2-be01-323a266dc2c3=2}
205674 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 004
205674 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 004
206098 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
206098 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
206230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
206230 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
206323 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
206330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 004 as complete
206330 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 004
206389 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Rolling back commits [003, 004]
206404 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [003, 004]
206404 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [003, 004]
206575 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
206579 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_003.parquet	true
206579 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2016/03/15/e900712b-1a33-4830-a458-0ed5ffd59f7f_1_004.parquet	true
206579 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
206580 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_003.parquet	true
206580 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2015/03/16/4eceecc3-309f-46c2-be01-323a266dc2c3_2_004.parquet	true
206580 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
206580 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_003.parquet	true
206580 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit6303565116810690121/2015/03/17/6b0837f9-c8b5-4b39-a03f-dd461cca6f2e_0_004.parquet	true
206583 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [003, 004]
206583 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [003, 004]
206591 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [003, 004] rollback is complete
206591 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
206735 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160506030611]
206735 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160506030611]
206909 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
206910 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/01/id31_1_20160506030611.parquet	true
206910 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
206910 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/02/id32_1_20160506030611.parquet	true
206910 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
206911 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/06/id33_1_20160506030611.parquet	true
206917 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160506030611]
206917 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160506030611]
206974 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160506030611] rollback is complete
206975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20160502020601]
206975 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20160502020601]
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/01
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/01/id21_1_20160502020601.parquet	true
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/02
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/02/id22_1_20160502020601.parquet	true
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/05/06
207094 [Executor task launch worker-0] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file file:/tmp/junit1042769172365216291/2016/05/06/id23_1_20160502020601.parquet	true
207097 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20160502020601]
207097 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20160502020601]
207105 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20160502020601] rollback is complete
207229 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
207229 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
207325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
207325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished []
207325 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: []
207637 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/c66477ad-3ad2-49ed-80e2-f91820ed4ae3_1_000.parquet	true
207767 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/1089c2e0-d637-4fac-af70-ad884cce533b_1_000.parquet	true
207875 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/83d953ea-d37e-4ffd-84d6-99b513135fe8_1_000.parquet	true
207976 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/7891e66c-8c0a-4e81-8ece-26c677060a37_1_000.parquet	true
208105 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/1fe3d01e-9330-404c-9e1e-5a04cf7284fc_1_000.parquet	true
208205 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/83c29be0-e196-4c49-8ebd-6f53f8124281_1_000.parquet	true
208324 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/351430a3-2a9d-482d-ac1c-1cd524ac9e83_1_000.parquet	true
208436 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/f361d5b5-9f4f-4bd8-ac4a-9765791c3ece_1_000.parquet	true
208578 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/b950847f-4634-49a2-93ba-3457f7fec0b8_1_000.parquet	true
208679 [Executor task launch worker-1] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleting file in temporary folderfile:/tmp/junit3311049026541748485/.hoodie/.temp/2a475104-38b3-4655-a52c-8d68f09d4ce6_1_000.parquet	true
208680 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits []
Tests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 132.89 sec - in com.uber.hoodie.TestHoodieClientOnCopyOnWriteStorage
Running com.uber.hoodie.metrics.TestHoodieMetrics
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.065 sec - in com.uber.hoodie.metrics.TestHoodieMetrics
Running com.uber.hoodie.index.TestHbaseIndex
Formatting using clusterid: testClusterID
209157 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-namenode.properties,hadoop-metrics2.properties
209167 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
209378 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
211259 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
211537 [b-cn1011:46235.activeMasterManager] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
212023 [main] WARN  org.apache.hadoop.hbase.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
213357 [b-cn1011:46235.activeMasterManager] WARN  org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore  - Log directory not found: File hdfs://localhost:43417/user/dginelli/test-data/b17d81ea-5cc9-452c-aacf-3674d0fe193b/MasterProcWALs does not exist.
213552 [RS:0;b-cn1011:40765] WARN  org.apache.hadoop.hbase.regionserver.HRegionServer  - reportForDuty failed; sleeping and then retrying.
216596 [RS:0;b-cn1011:40765] WARN  org.apache.hadoop.hbase.ZNodeClearer  - Environment variable HBASE_ZNODE_FILE not set; znodes will not be cleared on crash by start scripts (Longer MTTR!)
221564 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195512
222152 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=64, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=67, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=69, numUpdates=0}}}
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 64, totalInsertBuckets => 1, recordsPerBucket => 500000
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 67, totalInsertBuckets => 1, recordsPerBucket => 500000
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 69, totalInsertBuckets => 1, recordsPerBucket => 500000
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
222187 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
222198 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 001
222254 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
222296 [pool-1987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
222318 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
222318 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
222414 [pool-1987-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
222480 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
222496 [pool-1988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
222517 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
222517 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
222608 [pool-1988-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
222677 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
222700 [pool-1989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
222705 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
222705 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
222780 [pool-1989-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
223094 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 001
223204 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
223204 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
223212 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
223212 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
223347 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
223423 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 001 as complete
223423 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 001
223945 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
224080 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195515
224292 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=200, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=62, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=72, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=66, numUpdates=0}}}
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 62, totalInsertBuckets => 1, recordsPerBucket => 500000
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 72, totalInsertBuckets => 1, recordsPerBucket => 500000
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 66, totalInsertBuckets => 1, recordsPerBucket => 500000
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
224334 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
224344 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20200319195515
224403 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
224416 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
224416 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
224416 [pool-1990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
224533 [pool-1990-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
224561 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
224575 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
224595 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
224575 [pool-1991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
224700 [pool-1991-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
224750 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
224764 [Executor task launch worker-0] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
224765 [Executor task launch worker-0] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
224764 [pool-1992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
224850 [pool-1992-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
224889 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195515
224978 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
224978 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
224994 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
224994 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
225085 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
225113 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195515 as complete
225113 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195515
225460 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Unpublished [20200319195515]
225460 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Clean out all parquet files generated for commits: [20200319195515]
225510 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/16
225512 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43417/tmp/junit6735148693804478075/2015/03/16/a88be516-d9fe-4329-9226-2f9a48b794c1_1_20200319195515.parquet	true
225512 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2015/03/17
225514 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43417/tmp/junit6735148693804478075/2015/03/17/1c9cc6b0-a267-4f07-8e9b-09e3b73105da_2_20200319195515.parquet	true
225514 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Cleaning path 2016/03/15
225515 [Executor task launch worker-2] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Delete file hdfs://localhost:43417/tmp/junit6735148693804478075/2016/03/15/7f9209de-fd5e-48e3-8be3-f8841127a6ea_0_20200319195515.parquet	true
225518 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Deleted inflight commits [20200319195515]
225518 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Index rolled back for commits [20200319195515]
225578 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commits [20200319195515] rollback is complete
225578 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaning up older rollback meta files
225845 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195517
226020 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
226047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
226047 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
226056 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20200319195517
226149 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195517
226270 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=0, numUpdates=0}, partitionStat={}}
226281 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
226281 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :0, buckets info => {}, 
Partition to insert buckets => {}, 
UpdateLocations mapped to buckets =>{}
226290 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit disabled for 20200319195517
226312 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195517
226394 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
226394 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
226398 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
226398 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Nothing to clean here mom. It is already clean
226398 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195517
226805 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:62224] WARN  org.apache.zookeeper.server.NIOServerCnxn  - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x170f42614af0003, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:748)
227757 [main-EventThread] WARN  org.apache.hadoop.hbase.zookeeper.ZKUtil  - regionserver:40765-0x170f42614af0001, quorum=localhost:62224, baseZNode=/hbase Unable to set watcher on znode /hbase/master
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/master
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1041)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:220)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:365)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeDeleted(ZooKeeperNodeTracker.java:209)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:626)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
227776 [main-EventThread] ERROR org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher  - regionserver:40765-0x170f42614af0001, quorum=localhost:62224, baseZNode=/hbase Received unexpected KeeperException, re-throwing exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/master
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1041)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:220)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:365)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeDeleted(ZooKeeperNodeTracker.java:209)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:626)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
227757 [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:62224] WARN  org.apache.zookeeper.server.NIOServerCnxn  - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x170f42614af0004, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:220)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)
	at java.lang.Thread.run(Thread.java:748)
227776 [main-EventThread] ERROR org.apache.hadoop.hbase.regionserver.HRegionServer  - ABORTING region server b-cn1011.hpc2n.umu.se,40765,1584644102871: Unexpected exception handling nodeDeleted event
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/master
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1041)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:220)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.watchAndCheckExists(ZKUtil.java:365)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeDeleted(ZooKeeperNodeTracker.java:209)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:626)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
227778 [main-EventThread] ERROR org.apache.hadoop.hbase.regionserver.HRegionServer  - RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
227796 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
227846 [806785104@qtp-1950792021-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41911] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
227846 [806785104@qtp-1950792021-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41911] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
227851 [DataNode: [[[DISK]file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/test-data/d00ef4c4-ccdb-424a-98f7-83459221f3cc/dfscluster_c833b91a-5d77-4353-b8d8-0ae31f98a871/dfs/data/data1/, [DISK]file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/test-data/d00ef4c4-ccdb-424a-98f7-83459221f3cc/dfscluster_c833b91a-5d77-4353-b8d8-0ae31f98a871/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43417] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-1882420387-130.239.242.71-1584644100295 (Datanode Uuid db91bd40-425a-4f9c-8578-489b965145d9) service to localhost/127.0.0.1:43417 interrupted
227852 [DataNode: [[[DISK]file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/test-data/d00ef4c4-ccdb-424a-98f7-83459221f3cc/dfscluster_c833b91a-5d77-4353-b8d8-0ae31f98a871/dfs/data/data1/, [DISK]file:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/test-data/d00ef4c4-ccdb-424a-98f7-83459221f3cc/dfscluster_c833b91a-5d77-4353-b8d8-0ae31f98a871/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:43417] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-1882420387-130.239.242.71-1584644100295 (Datanode Uuid db91bd40-425a-4f9c-8578-489b965145d9) service to localhost/127.0.0.1:43417
227866 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@74e96a93] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
227908 [1930490785@qtp-1794772701-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42887] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
227909 [1930490785@qtp-1794772701-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42887] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.236 sec - in com.uber.hoodie.index.TestHbaseIndex
Running com.uber.hoodie.index.bloom.TestHoodieBloomIndex
228063 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
228227 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
228227 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${0}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
228296 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
229556 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 1
229556 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
229649 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8641158095838733600/2016/01/31/e681b7ec-6ad5-48dc-a6e8-64573889f67f_1_20200319195520.parquet
229661 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8641158095838733600/2016/01/31/e681b7ec-6ad5-48dc-a6e8-64573889f67f_1_20200319195520.parquet
229661 [Executor task launch worker-0] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8641158095838733600/2016/01/31/e681b7ec-6ad5-48dc-a6e8-64573889f67f_1_20200319195520.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
229719 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
229939 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
229939 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
230067 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
230143 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
230547 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
232013 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
232589 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
232809 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
232812 [HBase-Metrics2-1] WARN  org.apache.hadoop.metrics2.impl.MetricsConfig  - Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties
232827 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
233358 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
233359 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
233451 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5778881518046447495/2016/01/31/1bc4ef3f-b8c9-4e92-848d-3a7db4ad29f5_1_20200319195523.parquet
233474 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5778881518046447495/2016/01/31/1bc4ef3f-b8c9-4e92-848d-3a7db4ad29f5_1_20200319195523.parquet
233474 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5778881518046447495/2016/01/31/1bc4ef3f-b8c9-4e92-848d-3a7db4ad29f5_1_20200319195523.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
233476 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5778881518046447495/2015/01/31/59e9a86e-5597-4b43-b7cb-c4e1cd2ae6e8_1_20200319195524.parquet
233492 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5778881518046447495/2015/01/31/59e9a86e-5597-4b43-b7cb-c4e1cd2ae6e8_1_20200319195524.parquet
233492 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5778881518046447495/2015/01/31/59e9a86e-5597-4b43-b7cb-c4e1cd2ae6e8_1_20200319195524.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
233494 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit5778881518046447495/2016/01/31/5f87e34f-fdec-4f85-b16d-79296413aadf_1_20200319195522.parquet
233509 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit5778881518046447495/2016/01/31/5f87e34f-fdec-4f85-b16d-79296413aadf_1_20200319195522.parquet
233509 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit5778881518046447495/2016/01/31/5f87e34f-fdec-4f85-b16d-79296413aadf_1_20200319195522.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
233986 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
233987 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
234184 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
234857 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
235187 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
235707 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
236299 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
236660 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
237384 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 3
237384 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${1}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 1
237464 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8795265521934077445/2016/01/31/1a9507aa-1dd5-4e40-be3b-33afcda8873c_1_20200319195527.parquet
237489 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8795265521934077445/2016/01/31/1a9507aa-1dd5-4e40-be3b-33afcda8873c_1_20200319195527.parquet
237489 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8795265521934077445/2016/01/31/1a9507aa-1dd5-4e40-be3b-33afcda8873c_1_20200319195527.parquet => [2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
237491 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #1 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8795265521934077445/2016/01/31/e2d63a65-67c8-49fd-92cb-762d9371a091_1_20200319195526.parquet
237506 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8795265521934077445/2016/01/31/e2d63a65-67c8-49fd-92cb-762d9371a091_1_20200319195526.parquet
237506 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8795265521934077445/2016/01/31/e2d63a65-67c8-49fd-92cb-762d9371a091_1_20200319195526.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0]
237507 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - #2 After bloom filter, the candidate row keys is reduced to 1 for /tmp/junit8795265521934077445/2015/01/31/f6149b75-9498-43de-b665-fa173ea52b13_1_20200319195528.parquet
237523 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 1 row keys from /tmp/junit8795265521934077445/2015/01/31/f6149b75-9498-43de-b665-fa173ea52b13_1_20200319195528.parquet
237523 [Executor task launch worker-1] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 1 results, for file /tmp/junit8795265521934077445/2015/01/31/f6149b75-9498-43de-b665-fa173ea52b13_1_20200319195528.parquet => [4eb5b87c-1fej-4edd-87b4-6ec96dc405a0]
237684 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
237903 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
238455 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
238603 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
239334 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - Loading 2 row keys from /tmp/junit3449344381886407174/2016/01/31/ee7c1107-4294-4a1c-8ad5-30f4d568d6e1_1_20200319195530.parquet
239334 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction  - After checking with row keys, we have 2 results, for file /tmp/junit3449344381886407174/2016/01/31/ee7c1107-4294-4a1c-8ad5-30f4d568d6e1_1_20200319195530.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0, 2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
239869 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
240027 [Executor task launch worker-1] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5450461819514641140/2016/04/01/2_0_20160401010101.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1584644130000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
240061 [Executor task launch worker-1] WARN  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Unable to find range metadata in file :HoodieDataFile {fileStatus=DeprecatedRawLocalFileStatus{path=file:/tmp/junit5450461819514641140/2015/03/12/1_0_20150312101010.parquet; isDirectory=false; length=56802; replication=1; blocksize=33554432; modification_time=1584644130000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}}
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.102 sec - in com.uber.hoodie.index.bloom.TestHoodieBloomIndex
Running com.uber.hoodie.index.TestHoodieIndex
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.uber.hoodie.index.TestHoodieIndex
Running com.uber.hoodie.TestMultiFS
Formatting using clusterid: testClusterID
240224 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
240458 [main] WARN  org.apache.hadoop.http.HttpRequestLog  - Jetty request log can only be enabled using Log4j
240505 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
240607 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
240731 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
241030 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195532
241037 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting commit 20200319195532
241229 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
241229 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
241741 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=39, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=34, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=27, numUpdates=0}}}
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 39, totalInsertBuckets => 1, recordsPerBucket => 500000
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 34, totalInsertBuckets => 1, recordsPerBucket => 500000
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 27, totalInsertBuckets => 1, recordsPerBucket => 500000
241769 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
241770 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
241778 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195532
241778 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195532
241836 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
241862 [pool-2149-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
241882 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
241882 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
241985 [pool-2149-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
241996 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
242004 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
242004 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
242004 [pool-2150-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
242094 [pool-2150-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
242105 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
242111 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
242111 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
242111 [pool-2151-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
242171 [pool-2151-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
242196 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
242196 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
242203 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2015/03/16, 2015/03/17, 2016/03/15], with policy KEEP_LATEST_COMMITS
242203 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
242300 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
242305 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
242324 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195532 as complete
242324 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195532
242460 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
242612 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Generate a new commit time 20200319195533
242636 [main] INFO  com.uber.hoodie.TestMultiFS  - Starting write commit 20200319195533
242641 [main] INFO  com.uber.hoodie.TestMultiFS  - Writing to path: file:///tmp/hoodie/sample-table
242848 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
242869 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - Auto computed parallelism :1, totalComparisons: 0
242869 [main] INFO  com.uber.hoodie.index.bloom.HoodieBloomIndex  - InputParallelism: ${2}, IndexParallelism: ${0}, TotalSubParts: ${1}, Join Parallelism set to : 2
243054 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Workload profile :WorkloadProfile {globalStat=WorkloadStat {numInserts=100, numUpdates=0}, partitionStat={2016/03/15=WorkloadStat {numInserts=30, numUpdates=0}, 2015/03/16=WorkloadStat {numInserts=27, numUpdates=0}, 2015/03/17=WorkloadStat {numInserts=43, numUpdates=0}}}
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - AvgRecordSize => 1024
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2016/03/15 Small Files => []
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 30, totalInsertBuckets => 1, recordsPerBucket => 500000
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2016/03/15 => [WorkloadStat {bucketNumber=0, weight=1.0}]
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/16 Small Files => []
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 27, totalInsertBuckets => 1, recordsPerBucket => 500000
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/16 => [WorkloadStat {bucketNumber=1, weight=1.0}]
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - For partitionPath : 2015/03/17 Small Files => []
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - After small file assignment: unassignedInserts => 43, totalInsertBuckets => 1, recordsPerBucket => 500000
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total insert buckets for partition path 2015/03/17 => [WorkloadStat {bucketNumber=2, weight=1.0}]
243073 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Total Buckets :3, buckets info => {0=BucketInfo {bucketType=INSERT, fileLoc=null}, 1=BucketInfo {bucketType=INSERT, fileLoc=null}, 2=BucketInfo {bucketType=INSERT, fileLoc=null}}, 
Partition to insert buckets => {2016/03/15=[WorkloadStat {bucketNumber=0, weight=1.0}], 2015/03/16=[WorkloadStat {bucketNumber=1, weight=1.0}], 2015/03/17=[WorkloadStat {bucketNumber=2, weight=1.0}]}, 
UpdateLocations mapped to buckets =>{}
243081 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto commit enabled: Committing 20200319195533
243082 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Commiting 20200319195533
243157 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
243163 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
243164 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
243164 [pool-2153-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
243254 [pool-2153-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
243280 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
243286 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
243286 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
243286 [pool-2154-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
243361 [pool-2154-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
243388 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - starting to buffer records
243397 [Executor task launch worker-1] INFO  com.uber.hoodie.func.BufferedIterator  - finished buffering records
243397 [Executor task launch worker-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - waiting for hoodie write to finish
243397 [pool-2155-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - starting hoodie writer thread
243435 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
243466 [pool-2155-thread-1] INFO  com.uber.hoodie.func.LazyInsertIterable  - hoodie write is done; notifying reader thread
243504 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Auto cleaning is enabled. Running cleaner now
243504 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaner started
243606 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
243785 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Partitions to clean up : [2016/03/15, 2015/03/16, 2015/03/17], with policy KEEP_LATEST_COMMITS
243785 [main] INFO  com.uber.hoodie.table.HoodieCopyOnWriteTable  - Using cleanerParallelism: 3
243896 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Cleaned 0 files
243934 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Marked clean started on 20200319195533 as complete
243935 [main] INFO  com.uber.hoodie.HoodieWriteClient  - Committed 20200319195533
243935 [main] INFO  com.uber.hoodie.TestMultiFS  - Reading from path: file:///tmp/hoodie/sample-table
244180 [main] WARN  org.apache.hadoop.hdfs.server.datanode.DirectoryScanner  - DirectoryScanner: shutdown has been called
244218 [602508362@qtp-636012634-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37735] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
244218 [602508362@qtp-636012634-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37735] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
244220 [DataNode: [[[DISK]file:/tmp/1584644131277-0/dfs/data/data1/, [DISK]file:/tmp/1584644131277-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36907] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - BPOfferService for Block pool BP-541861472-130.239.242.71-1584644131309 (Datanode Uuid 9a0b815e-712c-4440-be7b-2e7884a20b2d) service to localhost/127.0.0.1:36907 interrupted
244220 [DataNode: [[[DISK]file:/tmp/1584644131277-0/dfs/data/data1/, [DISK]file:/tmp/1584644131277-0/dfs/data/data2/]]  heartbeating to localhost/127.0.0.1:36907] WARN  org.apache.hadoop.hdfs.server.datanode.DataNode  - Ending block pool service for: Block pool BP-541861472-130.239.242.71-1584644131309 (Datanode Uuid 9a0b815e-712c-4440-be7b-2e7884a20b2d) service to localhost/127.0.0.1:36907
244225 [org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager$Monitor@17dcccfa] WARN  org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager  - Monitor interrupted: java.lang.InterruptedException: sleep interrupted
244253 [523954065@qtp-748726121-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33127] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false. Rechecking.
244253 [523954065@qtp-748726121-1 - Acceptor0 HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33127] WARN  org.apache.hadoop.http.HttpServer2  - HttpServer Acceptor: isRunning is false
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.125 sec - in com.uber.hoodie.TestMultiFS
244264 [Thread-1178] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/.dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244271 [Thread-891] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit6427108733149947416/2016/03/15/.36b45f46-d695-4ff1-9311-9bd371a872a1_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244278 [Thread-1231] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/.5a885240-ee0a-4778-975d-a3808817b4f2_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244279 [Thread-636] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit8920228410676156910/2016/03/15/.4ee8fd52-1c53-48b9-901d-e783fdd83ca3_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244282 [Thread-897] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/16/.527cb7d5-ad22-4b21-a5a3-aab0ad94edc5_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244283 [Thread-1132] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/.5a885240-ee0a-4778-975d-a3808817b4f2_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244283 [Thread-1172] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/16/.5a885240-ee0a-4778-975d-a3808817b4f2_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244285 [Thread-1239] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/.0e87a203-33ec-4ff4-8eba-c78ad81d77fc_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244288 [Thread-885] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit6427108733149947416/2015/03/17/.ecf54227-4e5b-4422-ba03-e4c1ce3fbc22_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244290 [Thread-644] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit8920228410676156910/2015/03/16/.7265a857-134e-4cd8-a382-04fa851713d2_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244290 [Thread-628] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit8920228410676156910/2015/03/17/.b240cff9-67a5-4755-b31f-7e7422314641_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244294 [Thread-1184] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/.0e87a203-33ec-4ff4-8eba-c78ad81d77fc_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244303 [Thread-1138] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/.dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244341 [Thread-1223] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2016/03/15/.dcfdbe3c-20b5-46d7-919a-6b737d5e5d9e_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244350 [Thread-1144] WARN  com.uber.hoodie.common.table.log.HoodieLogFileReader  - unable to close input stream for log file HoodieLogFile {hdfs://localhost:41613/tmp/junit3075402958548666369/2015/03/17/.0e87a203-33ec-4ff4-8eba-c78ad81d77fc_001.log.1}
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:857)
	at org.apache.hadoop.hdfs.DFSInputStream.close(DFSInputStream.java:717)
	at java.io.FilterInputStream.close(FilterInputStream.java:181)
	at com.uber.hoodie.common.table.log.HoodieLogFileReader$1.run(HoodieLogFileReader.java:100)
244485 [Executor task launch worker-0-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0009 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
245132 [b-cn1011:46235.activeMasterManager-SendThread(localhost:62224)] WARN  org.apache.zookeeper.ClientCnxn  - Session 0x170f42614af0005 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)

Results :

Tests in error: 
  TestUpdateMapFunction.testSchemaEvolutionOnUpdate:106 » NullPointer

Tests run: 66, Failures: 0, Errors: 1, Skipped: 1

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Hoodie ............................................. SUCCESS [  0.126 s]
[INFO] hoodie-common ...................................... SUCCESS [ 44.218 s]
[INFO] hoodie-hadoop-mr ................................... SUCCESS [  8.313 s]
[INFO] hoodie-client ...................................... FAILURE [04:09 min]
[INFO] hoodie-spark ....................................... SKIPPED
[INFO] hoodie-hive ........................................ SKIPPED
[INFO] hoodie-utilities ................................... SKIPPED
[INFO] hoodie-cli ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:02 min
[INFO] Finished at: 2020-03-19T19:55:36+01:00
[INFO] Final Memory: 54M/129M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-client: There are test failures.
[ERROR] 
[ERROR] Please refer to /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/surefire-reports for the individual test results.
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hoodie-client: There are test failures.

Please refer to /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/surefire-reports for the individual test results.
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoFailureException: There are test failures.

Please refer to /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/target/surefire-reports for the individual test results.
	at org.apache.maven.plugin.surefire.SurefireHelper.reportExecution(SurefireHelper.java:91)
	at org.apache.maven.plugin.surefire.SurefirePlugin.handleSummary(SurefirePlugin.java:320)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:892)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:755)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)
	... 20 more
[ERROR] 
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hoodie-client
