[INFO ] fr.inria.main.evolution.AstorMain.setupLogging(AstorMain.java:272) - Log file at: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/repairnator.astor.jkali.log
[INFO ] fr.inria.main.AbstractMain.determineSourceFolders(AbstractMain.java:982) - Source folders: [/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/main/java]
[INFO ] fr.inria.main.AbstractMain.determineSourceFolders(AbstractMain.java:1001) - Source Test folders: [/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/test/java]
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.calculateSuspicious(AstorCoreEngine.java:911) - Test retrieved from classes: 14
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildSpoonModel(MutationSupporter.java:236) - Creating model,  Code location from working folder: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/main/java
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildModel(MutationSupporter.java:67) - building model: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/main/java, compliance level: 8
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildModel(MutationSupporter.java:81) - Classpath (Dependencies) for building SpoonModel: [/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro/1.7.6-cdh5.7.2/avro-1.7.6-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/common/objectsize/0.0.12/objectsize-0.0.12.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm/3.1/asm-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.2/hadoop-annotations-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/activation/activation/1.1/activation-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpcore/4.3.2/httpcore-4.3.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.2/hadoop-auth-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.2/zookeeper-3.4.5-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/tukaani/xz/1.0/xz-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-graphite/3.1.1/metrics-graphite-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-core/3.1.1/metrics-core-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/beust/jcommander/1.48/jcommander-1.48.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.2/hadoop-client-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.2/hadoop-mapreduce-client-app-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.2/hadoop-mapreduce-client-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.2/hadoop-yarn-client-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.2/hadoop-yarn-server-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.2/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.2/hadoop-yarn-api-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.2/hadoop-mapreduce-client-core-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.2/hadoop-yarn-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.2/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.2/hadoop-aws-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-avro/1.8.1/parquet-avro-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-column/1.8.1/parquet-column-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-common/1.8.1/parquet-common-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-encoding/1.8.1/parquet-encoding-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-format/2.3.0-incubating/parquet-format-2.3.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/it/unimi/dsi/fastutil/6.5.7/fastutil-6.5.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-hadoop/1.8.1/parquet-hadoop-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-jackson/1.8.1/parquet-jackson-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/xerial/snappy/snappy-java/1.1.1.6/snappy-java-1.1.1.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/guava/guava/15.0/guava-15.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-core_2.11/2.1.0/spark-core_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/objenesis/objenesis/2.1/objenesis-2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-launcher_2.11/2.1.0/spark-launcher_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-common_2.11/2.1.0/spark-network-common_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-shuffle_2.11/2.1.0/spark-network-shuffle_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-unsafe_2.11/2.1.0/spark-unsafe_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/oro/oro/2.0.8/oro-2.0.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-tags_2.11/2.1.0/spark-tags_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sql_2.11/2.1.0/spark-sql_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sketch_2.11/2.1.0/spark-sketch_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-catalyst_2.11/2.1.0/spark-catalyst_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-client/1.2.3/hbase-client-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-protocol/1.2.3/hbase-protocol-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mockito/mockito-all/1.10.19/mockito-all-1.10.19.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-hadoop-mr/0.4.2-SNAPSHOT/hoodie-hadoop-mr-0.4.2-SNAPSHOT.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-jdbc/1.1.0-cdh5.7.2/hive-jdbc-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-service/1.1.0-cdh5.7.2/hive-service-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/jpam/jpam/1.1/jpam-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/mail/mail/1.4.1/mail-1.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-commons/3.1/asm-commons-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-tree/3.1/asm-tree-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-serde/1.1.0-cdh5.7.2/hive-serde-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-hadoop-bundle/1.5.0-cdh5.7.2/parquet-hadoop-bundle-1.5.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-exec/1.1.0-cdh5.7.2/hive-exec-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-ant/1.1.0-cdh5.7.2/hive-ant-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/velocity/velocity/1.5/velocity-1.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-metastore/1.1.0-cdh5.7.2/hive-metastore-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/derby/derby/10.11.1.1/derby-10.11.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/transaction/jta/1.1/jta-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-shims/1.1.0-cdh5.7.2/hive-shims-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-common/1.1.0-cdh5.7.2/hive-shims-common-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-0.23/1.1.0-cdh5.7.2/hive-shims-0.23-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0-cdh5.7.2/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/guice/3.0/guice-3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/inject/javax.inject/1/javax.inject-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0-cdh5.7.2/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0-cdh5.7.2/hadoop-yarn-server-web-proxy-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-scheduler/1.1.0-cdh5.7.2/hive-shims-scheduler-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/cloudera/logredactor/logredactor/1.0.3/logredactor-1.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/antlr/antlr/2.7.7/antlr-2.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant/1.9.1/ant-1.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-core/1.0.0-incubating/calcite-core-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-linq4j/1.0.0-incubating/calcite-linq4j-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-avatica/1.0.0-incubating/calcite-avatica-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/stax/stax-api/1.0.1/stax-api-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/jline/jline/2.12/jline-2.12.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-testing-util/1.2.3/hbase-testing-util-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-procedure/1.2.3/hbase-procedure-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-prefix-tree/1.2.3/hbase-prefix-tree-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math/2.2/commons-math-2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-minicluster/2.5.1/hadoop-minicluster-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.5.1/hadoop-yarn-server-tests-2.5.1-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.5.1/hadoop-yarn-server-nodemanager-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.1/hadoop-mapreduce-client-jobclient-2.5.1-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.5.1/hadoop-mapreduce-client-hs-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/junit/junit/4.11/junit-4.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8316533609974665604 as hoodie dataset /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8316533609974665604/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319195618
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8316533609974665604/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit8316533609974665604
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319195618__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit8316533609974665604/.hoodie/20200319195618.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3507204847511894963 as hoodie dataset /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3507204847511894963/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3507204847511894963/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit3507204847511894963
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit3507204847511894963/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit45153749490761554 as hoodie dataset /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit45153749490761554/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit45153749490761554/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit45153749490761554
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit45153749490761554/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2532997778089368410 as hoodie dataset /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2532997778089368410/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2532997778089368410/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit2532997778089368410
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit2532997778089368410/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1855917008592821325 as hoodie dataset /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1855917008592821325/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1855917008592821325/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit1855917008592821325
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit1855917008592821325/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8941242300459291192 as hoodie dataset /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8941242300459291192/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8941242300459291192/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit8941242300459291192
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit8941242300459291192/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit5335115449555826554 as hoodie dataset /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit5335115449555826554/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit5335115449555826554/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit5335115449555826554
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit5335115449555826554/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8760179062241999094 as hoodie dataset /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8760179062241999094/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319195621
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8760179062241999094/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit8760179062241999094
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319195621__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit8760179062241999094/.hoodie/20200319195621.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit661340530465992710 as hoodie dataset /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit661340530465992710/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319195621
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit661340530465992710/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit661340530465992710
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319195621__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit661340530465992710/.hoodie/20200319195621.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit4956978653394858029 as hoodie dataset /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit4956978653394858029/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit4956978653394858029/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[20160501010101__commit], [20160502020601__commit], [==>20160506030611__commit]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit4956978653394858029/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit4956978653394858029
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[20160501010101__commit], [20160502020601__commit], [==>20160506030611__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:588) - Unpublished [20160506030611]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:591) - Clean out all parquet files generated for commits: [20160506030611]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1756529356777500876 as hoodie dataset /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1756529356777500876/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1756529356777500876/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[20160501010101__commit], [==>20160502020601__commit], [==>20160506030611__commit]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1756529356777500876/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit1756529356777500876
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[20160501010101__commit], [==>20160502020601__commit], [==>20160506030611__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:588) - Unpublished [20160506030611]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:591) - Clean out all parquet files generated for commits: [20160506030611]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit604912829618366890 as hoodie dataset /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit604912829618366890/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit604912829618366890/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit604912829618366890
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit604912829618366890/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit9122731289853403623 as hoodie dataset /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit9122731289853403623/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit9122731289853403623/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit9122731289853403623
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit9122731289853403623/.hoodie/001.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1851189287820534287 as hoodie dataset /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1851189287820534287/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1851189287820534287/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit1851189287820534287
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[000__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:538) - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_FILE_VERSIONS
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.cleanPartitionPaths(HoodieCopyOnWriteTable.java:737) - Using cleanerParallelism: 2
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2467480049265256876 as hoodie dataset /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2467480049265256876/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2467480049265256876 as hoodie dataset /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2467480049265256876/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type MERGE_ON_READ from /tmp/junit2467480049265256876
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:538) - Partitions to clean up : [2016/01/01], with policy KEEP_LATEST_FILE_VERSIONS
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.cleanPartitionPaths(HoodieCopyOnWriteTable.java:737) - Using cleanerParallelism: 1
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8033135861024157492 as hoodie dataset /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8033135861024157492/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8033135861024157492/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit8033135861024157492
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[000__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:538) - Partitions to clean up : [2016/01/01, 2016/02/02], with policy KEEP_LATEST_COMMITS
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.cleanPartitionPaths(HoodieCopyOnWriteTable.java:737) - Using cleanerParallelism: 2
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8578401641572500642 as hoodie dataset /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8578401641572500642/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8578401641572500642/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit8578401641572500642
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[000__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:538) - Partitions to clean up : [], with policy KEEP_LATEST_COMMITS
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:541) - Nothing to clean here mom. It is already clean
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7737695860009333659 as hoodie dataset /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7737695860009333659/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7737695860009333659/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit7737695860009333659
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[000__commit], [001__commit], [002__commit], [003__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.clean(HoodieCopyOnWriteTable.java:538) - Partitions to clean up : [2016/01/01, 2016/02/02, 2016/06/02], with policy KEEP_LATEST_COMMITS
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.cleanPartitionPaths(HoodieCopyOnWriteTable.java:737) - Using cleanerParallelism: 3
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3857097484333702794 as hoodie dataset /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3857097484333702794/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@2fc5c513]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3857097484333702794/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit3857097484333702794
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[000__commit]]
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:588) - Unpublished []
[INFO ] com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:591) - Clean out all parquet files generated for commits: []
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing hdfs://localhost:35401/user/dginelli as hoodie dataset hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-638261957_833, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-638261957_833, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from hdfs://localhost:35401/user/dginelli/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-638261957_833, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319195628
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-638261957_833, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from hdfs://localhost:35401/user/dginelli/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for hdfs://localhost:35401/user/dginelli
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319195628__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: hdfs://localhost:35401/user/dginelli/.hoodie/20200319195628.inflight
[INFO ] com.uber.hoodie.TestMultiFS.readLocalWriteHDFS(TestMultiFS.java:120) - Starting commit 20200319195628
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[ERROR] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:170) - error buffering records
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at com.uber.hoodie.func.BufferedIterator.insertRecord(BufferedIterator.java:127)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:165)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testMemoryLimitForBuffering$1(TestBufferedIterator.java:110)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[ERROR] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:170) - error buffering records
com.uber.hoodie.exception.HoodieException: operation has failed
	at com.uber.hoodie.func.BufferedIterator.throwExceptionIfFailed(BufferedIterator.java:197)
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:164)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$2(TestBufferedIterator.java:155)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.Exception: Failing it :)
	at com.uber.hoodie.func.TestBufferedIterator.testException(TestBufferedIterator.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 1 more
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[ERROR] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:170) - error buffering records
java.lang.RuntimeException: failing record reading
	at com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:165)
	at com.uber.hoodie.func.TestBufferedIterator.lambda$testException$3(TestBufferedIterator.java:185)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8657933201407282988 as hoodie dataset /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8657933201407282988/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8657933201407282988/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8657933201407282988/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8657933201407282988
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 2432b008-75d9-41f6-a188-da61adf2226e as we are done with all the records 3
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7758447329694829078 as hoodie dataset /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:43441], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1009299036_987, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:43441], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1009299036_987, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7758447329694829078/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:43441], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, io.storefile.bloom.block.size=131072, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, hbase.rest.threads.min=2, hbase.rs.cacheblocksonwrite=false, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=[DISK]file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/data/data1,[DISK]file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/data/data2, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, hbase.status.multicast.address.ip=226.1.1.3, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, hfile.block.bloom.cacheonwrite=false, hbase.zookeeper.quorum=localhost, hbase.http.staticuser.user=dr.stack, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, hbase.zookeeper.property.syncLimit=5, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, hbase.rest.filter.classes=org.apache.hadoop.hbase.rest.filter.GzipFilter, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, hfile.block.cache.size=0.4, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, hbase.hregion.memstore.mslab.enabled=true, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, hbase.master.port=16000, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, hbase.regionserver.checksum.verify=true, dfs.namenode.num.extra.edits.retained=1000000, hbase.security.visibility.mutations.checkauths=false, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, hbase.server.versionfile.writeattempts=3, dfs.permissions.superusergroup=supergroup, hbase.zookeeper.useMulti=true, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, hbase.zookeeper.leaderport=3888, dfs.datanode.slow.io.warning.threshold.ms=300, hbase.master.info.port=16010, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hbase.regionserver.dns.nameserver=default, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, hbase.ipc.server.callqueue.read.ratio=0, hbase.cluster.distributed=false, hbase.rootdir=hdfs://localhost:43441/user/dginelli/test-data/16917fd4-fd9f-4c7d-9bc6-3990aecee5b7, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, hbase.auth.token.max.lifetime=604800000, hbase.regionserver.msginterval=3000, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, hbase.ipc.client.fallback-to-simple-auth-allowed=false, fs.s3a.paging.maximum=5000, hbase.rest.threads.max=100, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, hbase.snapshot.enabled=true, hbase.dynamic.jars.dir=${hbase.rootdir}/lib, hbase.defaults.for.version=1.2.3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hbase.regions.slop=0.2, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, zookeeper.znode.rootserver=root-region-server, mapreduce.task.timeout=600000, hbase.client.max.perserver.tasks=5, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, zookeeper.session.timeout=90000, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, hbase.balancer.period=300000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, hbase.hstore.blockingWaitTime=90000, nfs.dump.dir=/tmp/.hdfs-nfs, hbase.client.pause=100, hbase.client.write.buffer=2097152, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, hbase.rpc.timeout=60000, hbase.metrics.exposeOperationTimes=true, dfs.namenode.safemode.min.datanodes=0, hbase.thrift.maxWorkerThreads=1000, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hbase.master.catalog.timeout=600000, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, hbase.region.replica.replication.enabled=false, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, hbase.hstore.bytes.per.checksum=16384, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, hbase.hstore.flusher.count=2, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=true, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, hbase.client.retries.number=35, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, hbase.status.multicast.address.port=16100, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, hbase.hstore.checksum.algorithm=CRC32C, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, hbase.server.thread.wakefrequency=10000, nfs.exports.allowed.hosts=* rw, hbase.lease.recovery.timeout=900000, hbase.coordinated.state.manager.class=org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, hfile.format.version=3, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hbase.replication.source.maxthreads=10, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, hbase.thrift.minWorkerThreads=16, hbase.zookeeper.dns.interface=default, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, hbase.snapshot.restore.failsafe.name=hbase-failsafe-{snapshot.name}-{restore.timestamp}, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/mapred_local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, dfs.datanode.hostname=127.0.0.1, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hbase.hstore.blockingStoreFiles=10, hadoop.security.group.mapping.ldap.search.attr.member=member, hbase.regionserver.hlog.reader.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, hbase.master.info.bindAddress=0.0.0.0, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, hbase.master.wait.on.regionservers.maxtostart=1, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, hbase.status.listener.class=org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/namesecondary1,file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/namesecondary2, hbase.regionserver.dns.interface=default, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hbase.rest.port=8080, hbase.regionserver.handler.count=30, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, hbase.ipc.server.callqueue.scan.ratio=0, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=0.0.0.0:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, hbase.bulkload.retries.number=10, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, hbase.hregion.max.filesize=10737418240, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, hbase.master.loadbalancer.class=org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, zookeeper.znode.acl.parent=acl, hbase.status.publisher.class=org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, hbase.tmp.dir=${java.io.tmpdir}/hbase-${user.name}, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, hbase.http.max.threads=10, mapreduce.map.output.compress=false, hbase.client.localityCheck.threadPoolSize=2, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:43441, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hbase.regionserver.regionSplitLimit=1000, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, hbase.thrift.maxQueuedRequests=1000, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, hfile.block.index.cacheonwrite=false, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, hbase.config.read.zookeeper.config=false, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, hbase.master.wait.on.regionservers.mintostart=1, hbase.column.max.version=1, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, hbase.bucketcache.combinedcache.enabled=true, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, hbase.zookeeper.property.clientPort=49470, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, hbase.rest.readonly=false, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , zookeeper.znode.parent=/hbase, fs.defaultFS=hdfs://localhost:43441, hbase.rpc.shortoperation.timeout=10000, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hfile.index.block.max.size=131072, hadoop.log.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop_logs, hbase.client.scanner.timeout.period=60000, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, hbase.ipc.server.fallback-to-simple-auth-allowed=false, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, hbase.hstore.compactionThreshold=3, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, hbase.master.logcleaner.ttl=600000, mapreduce.job.acl-view-job= , mapreduce.job.working.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-working-dir, dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, hbase.rootdir.perms=700, hbase.hregion.majorcompaction=604800000, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, hbase.hregion.memstore.flush.size=134217728, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, hbase.ipc.client.tcpnodelay=true, hbase.regionserver.optionalcacheflushinterval=3600000, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, hbase.normalizer.period=1800000, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, hbase.regionserver.port=16020, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, hbase.hstore.time.to.purge.deletes=0, hbase.regionserver.logroll.errors.tolerated=2, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, hbase.hstore.compaction.max=10, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, hbase.master.infoserver.redirect=true, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, hbase.hregion.percolumnfamilyflush.size.lower.bound=16777216, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, hbase.zookeeper.property.initLimit=10, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, mapreduce.output.fileoutputformat.outputdir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-output-dir, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/name1,file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/name2, hbase.coprocessor.abortonerror=true, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, hbase.hstore.compaction.kv.max=10, hbase.hregion.preclose.flush.size=5242880, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, test.build.data=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5, hbase.master.hfilecleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner, mapreduce.job.emit-timeline-data=false, hbase.metrics.showTableName=true, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, hbase.lease.recovery.dfs.timeout=64000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hbase.server.scanner.max.result.size=104857600, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-system-dir, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, hbase.zookeeper.dns.nameserver=default, hbase.ipc.server.callqueue.handler.factor=0.1, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hbase.bulkload.staging.dir=${hbase.fs.tmp.dir}, hadoop.tmp.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop_tmp, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, hbase.regionserver.logroll.period=3600000, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, hbase.hregion.memstore.block.multiplier=4, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, hbase.storescanner.parallel.seek.enable=false, hbase.snapshot.master.timeout.millis=300000, hbase.regionserver.storefile.refresh.period=0, hbase.dfs.client.read.shortcircuit.buffer.size=131072, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=0.0.0.0:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, hbase.thrift.htablepool.size.max=1000, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=0.0.0.0:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, hbase.client.scanner.caching=2147483647, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, hbase.snapshot.restore.take.failsafe.snapshot=true, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, hadoop.policy.file=hbase-policy.xml, hbase.fs.tmp.dir=/user/${user.name}/hbase-staging, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, hbase.status.published=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, hbase.client.scanner.max.result.size=2097152, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapreduce-jobtracker-staging-root-dir, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, hbase.regionserver.info.bindAddress=0.0.0.0, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, hbase.master.logcleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner, hbase.data.umask.enable=false, hbase.coprocessor.user.enabled=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, hbase.local.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hbase-local-dir, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, hbase.table.lock.enable=true, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hbase.storescanner.parallel.seek.threads=10, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, hbase.online.schema.update.enable=true, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, hbase.zookeeper.property.dataDir=${hbase.tmp.dir}/zookeeper, dfs.namenode.http-address=localhost:35557, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, hbase.regionserver.handler.abort.on.error.percent=0.5, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, hbase.security.exec.permission.checks=false, hbase.master.normalizer.class=org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, hbase.regionserver.info.port.auto=false, ha.failover-controller.cli-check.rpc-timeout.ms=20000, hbase.auth.key.update.interval=86400000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, hbase.regionserver.thrift.framed=false, hbase.zookeeper.property.maxClientCnxns=300, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, hbase.server.compactchecker.interval.multiplier=1000, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, fs.trash.interval=0, hbase.client.max.perregion.tasks=1, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, hbase.data.umask=000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, test.cache.data=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/cache_data, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, hbase.regionserver.hlog.writer.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, hbase.regionserver.region.split.policy=org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy, hbase.cells.scanned.per.heartbeat.check=10000, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hbase.client.max.total.tasks=100, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hbase.zookeeper.peerport=2888, hadoop.security.crypto.buffer.size=8192, hbase.table.max.rowsize=1073741824, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, hbase.client.operation.timeout=1200000, hbase.regionserver.info.port=16030, fs.s3a.threads.max=256, hbase.coprocessor.enabled=true, hbase.hregion.majorcompaction.jitter=0.50, hbase.snapshot.region.timeout=300000, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hbase.regionserver.thrift.framed.max_frame_size_in_mb=2, hbase.http.filter.initializers=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, hbase.replication.rpc.codec=org.apache.hadoop.hbase.codec.KeyValueCodecWithTags, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, hbase.regionserver.catalog.timeout=600000, hbase.security.authentication=simple, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, hbase.client.keyvalue.maxsize=10485760, hbase.regionserver.thrift.compact=false, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/mapred_temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, hbase.defaults.for.version.skip=false, fs.s3a.attempts.maximum=20, hbase.rest.support.proxyuser=false}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1009299036_987, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319195921
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:43441], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, io.storefile.bloom.block.size=131072, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, hbase.rest.threads.min=2, hbase.rs.cacheblocksonwrite=false, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=[DISK]file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/data/data1,[DISK]file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/data/data2, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, hbase.status.multicast.address.ip=226.1.1.3, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, hfile.block.bloom.cacheonwrite=false, hbase.zookeeper.quorum=localhost, hbase.http.staticuser.user=dr.stack, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, hbase.zookeeper.property.syncLimit=5, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, hbase.rest.filter.classes=org.apache.hadoop.hbase.rest.filter.GzipFilter, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, hfile.block.cache.size=0.4, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, hbase.hregion.memstore.mslab.enabled=true, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, hbase.master.port=16000, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, hbase.regionserver.checksum.verify=true, dfs.namenode.num.extra.edits.retained=1000000, hbase.security.visibility.mutations.checkauths=false, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, hbase.server.versionfile.writeattempts=3, dfs.permissions.superusergroup=supergroup, hbase.zookeeper.useMulti=true, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, hbase.zookeeper.leaderport=3888, dfs.datanode.slow.io.warning.threshold.ms=300, hbase.master.info.port=16010, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hbase.regionserver.dns.nameserver=default, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, hbase.ipc.server.callqueue.read.ratio=0, hbase.cluster.distributed=false, hbase.rootdir=hdfs://localhost:43441/user/dginelli/test-data/16917fd4-fd9f-4c7d-9bc6-3990aecee5b7, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, hbase.auth.token.max.lifetime=604800000, hbase.regionserver.msginterval=3000, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, hbase.ipc.client.fallback-to-simple-auth-allowed=false, fs.s3a.paging.maximum=5000, hbase.rest.threads.max=100, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, hbase.snapshot.enabled=true, hbase.dynamic.jars.dir=${hbase.rootdir}/lib, hbase.defaults.for.version=1.2.3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hbase.regions.slop=0.2, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, zookeeper.znode.rootserver=root-region-server, mapreduce.task.timeout=600000, hbase.client.max.perserver.tasks=5, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, zookeeper.session.timeout=90000, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, hbase.balancer.period=300000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, hbase.hstore.blockingWaitTime=90000, nfs.dump.dir=/tmp/.hdfs-nfs, hbase.client.pause=100, hbase.client.write.buffer=2097152, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, hbase.rpc.timeout=60000, hbase.metrics.exposeOperationTimes=true, dfs.namenode.safemode.min.datanodes=0, hbase.thrift.maxWorkerThreads=1000, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hbase.master.catalog.timeout=600000, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, hbase.region.replica.replication.enabled=false, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, hbase.hstore.bytes.per.checksum=16384, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, hbase.hstore.flusher.count=2, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=true, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, hbase.client.retries.number=35, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, hbase.status.multicast.address.port=16100, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, hbase.hstore.checksum.algorithm=CRC32C, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, hbase.server.thread.wakefrequency=10000, nfs.exports.allowed.hosts=* rw, hbase.lease.recovery.timeout=900000, hbase.coordinated.state.manager.class=org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, hfile.format.version=3, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hbase.replication.source.maxthreads=10, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, hbase.thrift.minWorkerThreads=16, hbase.zookeeper.dns.interface=default, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, hbase.snapshot.restore.failsafe.name=hbase-failsafe-{snapshot.name}-{restore.timestamp}, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/mapred_local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, dfs.datanode.hostname=127.0.0.1, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hbase.hstore.blockingStoreFiles=10, hadoop.security.group.mapping.ldap.search.attr.member=member, hbase.regionserver.hlog.reader.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, hbase.master.info.bindAddress=0.0.0.0, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, hbase.master.wait.on.regionservers.maxtostart=1, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, hbase.status.listener.class=org.apache.hadoop.hbase.client.ClusterStatusListener$MulticastListener, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/namesecondary1,file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/namesecondary2, hbase.regionserver.dns.interface=default, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hbase.rest.port=8080, hbase.regionserver.handler.count=30, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, hbase.ipc.server.callqueue.scan.ratio=0, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=0.0.0.0:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, hbase.bulkload.retries.number=10, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, hbase.hregion.max.filesize=10737418240, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, hbase.master.loadbalancer.class=org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, zookeeper.znode.acl.parent=acl, hbase.status.publisher.class=org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, hbase.tmp.dir=${java.io.tmpdir}/hbase-${user.name}, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, hbase.http.max.threads=10, mapreduce.map.output.compress=false, hbase.client.localityCheck.threadPoolSize=2, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:43441, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hbase.regionserver.regionSplitLimit=1000, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, hbase.thrift.maxQueuedRequests=1000, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, hfile.block.index.cacheonwrite=false, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, hbase.config.read.zookeeper.config=false, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, hbase.master.wait.on.regionservers.mintostart=1, hbase.column.max.version=1, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, hbase.bucketcache.combinedcache.enabled=true, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, hbase.zookeeper.property.clientPort=49470, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, hbase.rest.readonly=false, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , zookeeper.znode.parent=/hbase, fs.defaultFS=hdfs://localhost:43441, hbase.rpc.shortoperation.timeout=10000, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hfile.index.block.max.size=131072, hadoop.log.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop_logs, hbase.client.scanner.timeout.period=60000, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, hbase.ipc.server.fallback-to-simple-auth-allowed=false, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, hbase.hstore.compactionThreshold=3, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, hbase.master.logcleaner.ttl=600000, mapreduce.job.acl-view-job= , mapreduce.job.working.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-working-dir, dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, hbase.rootdir.perms=700, hbase.hregion.majorcompaction=604800000, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, hbase.hregion.memstore.flush.size=134217728, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, hbase.ipc.client.tcpnodelay=true, hbase.regionserver.optionalcacheflushinterval=3600000, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, hbase.normalizer.period=1800000, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, hbase.regionserver.port=16020, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, hbase.hstore.time.to.purge.deletes=0, hbase.regionserver.logroll.errors.tolerated=2, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, hbase.hstore.compaction.max=10, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, hbase.master.infoserver.redirect=true, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, hbase.hregion.percolumnfamilyflush.size.lower.bound=16777216, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, hbase.zookeeper.property.initLimit=10, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, mapreduce.output.fileoutputformat.outputdir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-output-dir, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/name1,file:/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5/dfs/name2, hbase.coprocessor.abortonerror=true, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, hbase.hstore.compaction.kv.max=10, hbase.hregion.preclose.flush.size=5242880, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, test.build.data=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/dfscluster_6c8f78ec-a2f4-4eaf-8e5d-7342dddd9de5, hbase.master.hfilecleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner, mapreduce.job.emit-timeline-data=false, hbase.metrics.showTableName=true, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, hbase.lease.recovery.dfs.timeout=64000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hbase.server.scanner.max.result.size=104857600, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapred-system-dir, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, hbase.zookeeper.dns.nameserver=default, hbase.ipc.server.callqueue.handler.factor=0.1, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hbase.bulkload.staging.dir=${hbase.fs.tmp.dir}, hadoop.tmp.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop_tmp, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, hbase.regionserver.logroll.period=3600000, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, hbase.hregion.memstore.block.multiplier=4, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, hbase.storescanner.parallel.seek.enable=false, hbase.snapshot.master.timeout.millis=300000, hbase.regionserver.storefile.refresh.period=0, hbase.dfs.client.read.shortcircuit.buffer.size=131072, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=0.0.0.0:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, hbase.thrift.htablepool.size.max=1000, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=0.0.0.0:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, hbase.client.scanner.caching=2147483647, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, hbase.snapshot.restore.take.failsafe.snapshot=true, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, hadoop.policy.file=hbase-policy.xml, hbase.fs.tmp.dir=/user/${user.name}/hbase-staging, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, hbase.status.published=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, hbase.client.scanner.max.result.size=2097152, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hadoop/mapreduce-jobtracker-staging-root-dir, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, hbase.regionserver.info.bindAddress=0.0.0.0, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, hbase.master.logcleaner.plugins=org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner, hbase.data.umask.enable=false, hbase.coprocessor.user.enabled=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, hbase.local.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/hbase-local-dir, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, hbase.table.lock.enable=true, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hbase.storescanner.parallel.seek.threads=10, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, hbase.online.schema.update.enable=true, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, hbase.zookeeper.property.dataDir=${hbase.tmp.dir}/zookeeper, dfs.namenode.http-address=localhost:35557, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, hbase.regionserver.handler.abort.on.error.percent=0.5, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, hbase.security.exec.permission.checks=false, hbase.master.normalizer.class=org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, hbase.regionserver.info.port.auto=false, ha.failover-controller.cli-check.rpc-timeout.ms=20000, hbase.auth.key.update.interval=86400000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, hbase.regionserver.thrift.framed=false, hbase.zookeeper.property.maxClientCnxns=300, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, hbase.server.compactchecker.interval.multiplier=1000, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, fs.trash.interval=0, hbase.client.max.perregion.tasks=1, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, hbase.data.umask=000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, test.cache.data=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/cache_data, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, hbase.regionserver.hlog.writer.impl=org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, hbase.regionserver.region.split.policy=org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy, hbase.cells.scanned.per.heartbeat.check=10000, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hbase.client.max.total.tasks=100, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hbase.zookeeper.peerport=2888, hadoop.security.crypto.buffer.size=8192, hbase.table.max.rowsize=1073741824, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, hbase.client.operation.timeout=1200000, hbase.regionserver.info.port=16030, fs.s3a.threads.max=256, hbase.coprocessor.enabled=true, hbase.hregion.majorcompaction.jitter=0.50, hbase.snapshot.region.timeout=300000, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hbase.regionserver.thrift.framed.max_frame_size_in_mb=2, hbase.http.filter.initializers=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, hbase.replication.rpc.codec=org.apache.hadoop.hbase.codec.KeyValueCodecWithTags, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, hbase.regionserver.catalog.timeout=600000, hbase.security.authentication=simple, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, hbase.client.keyvalue.maxsize=10485760, hbase.regionserver.thrift.compact=false, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=/pfs/nobackup/home/d/dginelli/development/target/test-data/ed1c5ed0-80b7-46cd-848b-80d3a7b61f01/mapred_temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, hbase.defaults.for.version.skip=false, fs.s3a.attempts.maximum=20, hbase.rest.support.proxyuser=false}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1009299036_987, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7758447329694829078/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit7758447329694829078
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319195921__commit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit7758447329694829078/.hoodie/20200319195921.inflight
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1839950740106187315 as hoodie dataset /tmp/junit1839950740106187315
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:45215], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1457882641_2306, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1839950740106187315
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:45215], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1457882641_2306, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1839950740106187315/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1839950740106187315
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1839950740106187315
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit824205979482812050 as hoodie dataset /tmp/junit824205979482812050
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit824205979482812050
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit824205979482812050/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit824205979482812050
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit824205979482812050
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3119572696323941481 as hoodie dataset /tmp/junit3119572696323941481
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3119572696323941481
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3119572696323941481/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3119572696323941481
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3119572696323941481
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit442069262732379253 as hoodie dataset /tmp/junit442069262732379253
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit442069262732379253
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit442069262732379253/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit442069262732379253
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit442069262732379253
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1759903676758325687 as hoodie dataset /tmp/junit1759903676758325687
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1759903676758325687
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1759903676758325687/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1759903676758325687
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1759903676758325687
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction.checkCandidatesAgainstFile(HoodieBloomIndexCheckFunction.java:69) - Loading 2 row keys from /tmp/junit1759903676758325687/2016/01/31/f6af366a-1d14-4dbc-831a-1aeabcb813e2_1_20200319201250.parquet
[INFO ] com.uber.hoodie.index.bloom.HoodieBloomIndexCheckFunction.checkCandidatesAgainstFile(HoodieBloomIndexCheckFunction.java:78) - After checking with row keys, we have 2 results, for file /tmp/junit1759903676758325687/2016/01/31/f6af366a-1d14-4dbc-831a-1aeabcb813e2_1_20200319201250.parquet => [1eb5b87a-1feh-4edd-87b4-6ec96dc405a0, 2eb5b87b-1feu-4edd-87b4-6ec96dc405a0]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1633639140977925265 as hoodie dataset /tmp/junit1633639140977925265
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1633639140977925265
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1633639140977925265/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1633639140977925265
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1633639140977925265
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2171887885991064412 as hoodie dataset /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2171887885991064412/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2171887885991064412/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2171887885991064412
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit134078685635734202 as hoodie dataset /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit134078685635734202/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit134078685635734202/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit134078685635734202
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7543118890674781887 as hoodie dataset /tmp/junit7543118890674781887
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7543118890674781887
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7543118890674781887/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7543118890674781887
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7543118890674781887
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1404933555810550468 as hoodie dataset /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1404933555810550468/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1404933555810550468/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1404933555810550468
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:118) - No Instants to archive
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7978032255951560399 as hoodie dataset /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7978032255951560399/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7978032255951560399/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit7978032255951560399
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:118) - No Instants to archive
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8835516191009816853 as hoodie dataset /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8835516191009816853/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8835516191009816853/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8835516191009816853
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [101__commit], [102__commit], [103__commit], [104__commit], [105__commit]]
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:166) - Building HoodieLogFormat Writer
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:183) - Computing the next log version for commits in /tmp/junit8835516191009816853/.hoodie
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:186) - Computed the next log version for commits in /tmp/junit8835516191009816853/.hoodie as 1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:193) - HoodieLogFile on path /tmp/junit8835516191009816853/.hoodie/.commits_.archive.1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormatWriter.<init>(HoodieLogFormatWriter.java:98) - HoodieLogFile {/tmp/junit8835516191009816853/.hoodie/.commits_.archive.1} does not exist. Create a new file
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:114) - Archiving instants [[100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:193) - Wrapper schema {"type":"record","name":"HoodieArchivedMetaEntry","namespace":"com.uber.hoodie.avro.model","fields":[{"name":"hoodieCommitMetadata","type":["null",{"type":"record","name":"HoodieCommitMetadata","fields":[{"name":"partitionToWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieWriteStat","fields":[{"name":"fileId","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"path","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"prevCommit","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"numWrites","type":["null","long"]},{"name":"numDeletes","type":["null","long"]},{"name":"numUpdateWrites","type":["null","long"]},{"name":"totalWriteBytes","type":["null","long"]},{"name":"totalWriteErrors","type":["null","long"]},{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]}]}},"avro.java.string":"String"}]},{"name":"extraMetadata","type":["null",{"type":"map","values":{"type":"string","avro.java.string":"String"},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieCleanMetadata","type":["null",{"type":"record","name":"HoodieCleanMetadata","fields":[{"name":"startCleanTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"earliestCommitToRetain","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieCleanPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"policy","type":{"type":"string","avro.java.string":"String"}},{"name":"deletePathPatterns","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieCompactionMetadata","type":["null",{"type":"record","name":"HoodieCompactionMetadata","fields":[{"name":"partitionToCompactionWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieCompactionWriteStat","fields":[{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]},{"name":"hoodieWriteStat","type":["null","HoodieWriteStat"]}]}},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieRollbackMetadata","type":["null",{"type":"record","name":"HoodieRollbackMetadata","fields":[{"name":"startRollbackTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"commitsRollback","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieRollbackPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieSavePointMetadata","type":["null",{"type":"record","name":"HoodieSavepointMetadata","fields":[{"name":"savepointedBy","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointedAt","type":"long"},{"name":"comments","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieSavepointPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointDataFile","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"commitTime","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"actionType","type":["null",{"type":"string","avro.java.string":"String"}]}]}
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:170) - Deleting instants [[100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit8835516191009816853/.hoodie/100.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit8835516191009816853/.hoodie/101.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit8835516191009816853/.hoodie/102.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit8835516191009816853/.hoodie/103.commit
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[104__commit], [105__commit]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit8695957089572332428 as hoodie dataset /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8695957089572332428/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit8695957089572332428/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit8695957089572332428
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [101__commit], [101__savepoint], [102__commit], [103__commit], [104__commit], [105__commit]]
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:166) - Building HoodieLogFormat Writer
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:183) - Computing the next log version for commits in /tmp/junit8695957089572332428/.hoodie
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:186) - Computed the next log version for commits in /tmp/junit8695957089572332428/.hoodie as 1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:193) - HoodieLogFile on path /tmp/junit8695957089572332428/.hoodie/.commits_.archive.1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormatWriter.<init>(HoodieLogFormatWriter.java:98) - HoodieLogFile {/tmp/junit8695957089572332428/.hoodie/.commits_.archive.1} does not exist. Create a new file
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:114) - Archiving instants [[100__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:193) - Wrapper schema {"type":"record","name":"HoodieArchivedMetaEntry","namespace":"com.uber.hoodie.avro.model","fields":[{"name":"hoodieCommitMetadata","type":["null",{"type":"record","name":"HoodieCommitMetadata","fields":[{"name":"partitionToWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieWriteStat","fields":[{"name":"fileId","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"path","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"prevCommit","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"numWrites","type":["null","long"]},{"name":"numDeletes","type":["null","long"]},{"name":"numUpdateWrites","type":["null","long"]},{"name":"totalWriteBytes","type":["null","long"]},{"name":"totalWriteErrors","type":["null","long"]},{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]}]}},"avro.java.string":"String"}]},{"name":"extraMetadata","type":["null",{"type":"map","values":{"type":"string","avro.java.string":"String"},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieCleanMetadata","type":["null",{"type":"record","name":"HoodieCleanMetadata","fields":[{"name":"startCleanTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"earliestCommitToRetain","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieCleanPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"policy","type":{"type":"string","avro.java.string":"String"}},{"name":"deletePathPatterns","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieCompactionMetadata","type":["null",{"type":"record","name":"HoodieCompactionMetadata","fields":[{"name":"partitionToCompactionWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieCompactionWriteStat","fields":[{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]},{"name":"hoodieWriteStat","type":["null","HoodieWriteStat"]}]}},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieRollbackMetadata","type":["null",{"type":"record","name":"HoodieRollbackMetadata","fields":[{"name":"startRollbackTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"commitsRollback","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieRollbackPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieSavePointMetadata","type":["null",{"type":"record","name":"HoodieSavepointMetadata","fields":[{"name":"savepointedBy","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointedAt","type":"long"},{"name":"comments","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieSavepointPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointDataFile","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"commitTime","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"actionType","type":["null",{"type":"string","avro.java.string":"String"}]}]}
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:170) - Deleting instants [[100__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit8695957089572332428/.hoodie/100.commit
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[101__commit], [101__savepoint], [102__commit], [103__commit], [104__commit], [105__commit]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3983637226067236720 as hoodie dataset /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3983637226067236720/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3983637226067236720 as hoodie dataset /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3983637226067236720/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3983637226067236720/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [101__commit], [102__commit], [103__commit], [104__commit], [105__commit]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [100__clean], [101__commit], [==>101__clean], [101__clean], [102__commit], [102__clean], [103__commit], [103__clean], [104__commit], [104__clean], [105__commit], [105__clean], [==>106__clean], [==>107__clean]]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [100__clean], [101__commit], [==>101__clean], [101__clean], [102__commit], [102__clean], [103__commit], [103__clean], [104__commit], [104__clean], [105__commit], [105__clean], [==>106__clean], [==>107__clean]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3983637226067236720/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit3983637226067236720
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[100__commit], [100__clean], [101__commit], [==>101__clean], [101__clean], [102__commit], [102__clean], [103__commit], [103__clean], [104__commit], [104__clean], [105__commit], [105__clean], [==>106__clean], [==>107__clean]]
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:166) - Building HoodieLogFormat Writer
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:183) - Computing the next log version for commits in /tmp/junit3983637226067236720/.hoodie
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:186) - Computed the next log version for commits in /tmp/junit3983637226067236720/.hoodie as 1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormat$WriterBuilder.build(HoodieLogFormat.java:193) - HoodieLogFile on path /tmp/junit3983637226067236720/.hoodie/.commits_.archive.1
[INFO ] com.uber.hoodie.common.table.log.HoodieLogFormatWriter.<init>(HoodieLogFormatWriter.java:98) - HoodieLogFile {/tmp/junit3983637226067236720/.hoodie/.commits_.archive.1} does not exist. Create a new file
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:114) - Archiving instants [[100__clean], [101__clean], [102__clean], [103__clean], [100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:193) - Wrapper schema {"type":"record","name":"HoodieArchivedMetaEntry","namespace":"com.uber.hoodie.avro.model","fields":[{"name":"hoodieCommitMetadata","type":["null",{"type":"record","name":"HoodieCommitMetadata","fields":[{"name":"partitionToWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieWriteStat","fields":[{"name":"fileId","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"path","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"prevCommit","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"numWrites","type":["null","long"]},{"name":"numDeletes","type":["null","long"]},{"name":"numUpdateWrites","type":["null","long"]},{"name":"totalWriteBytes","type":["null","long"]},{"name":"totalWriteErrors","type":["null","long"]},{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]}]}},"avro.java.string":"String"}]},{"name":"extraMetadata","type":["null",{"type":"map","values":{"type":"string","avro.java.string":"String"},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieCleanMetadata","type":["null",{"type":"record","name":"HoodieCleanMetadata","fields":[{"name":"startCleanTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"earliestCommitToRetain","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieCleanPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"policy","type":{"type":"string","avro.java.string":"String"}},{"name":"deletePathPatterns","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieCompactionMetadata","type":["null",{"type":"record","name":"HoodieCompactionMetadata","fields":[{"name":"partitionToCompactionWriteStats","type":["null",{"type":"map","values":{"type":"array","items":{"type":"record","name":"HoodieCompactionWriteStat","fields":[{"name":"partitionPath","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"totalLogRecords","type":["null","long"]},{"name":"totalLogFiles","type":["null","long"]},{"name":"totalRecordsToBeUpdate","type":["null","long"]},{"name":"hoodieWriteStat","type":["null","HoodieWriteStat"]}]}},"avro.java.string":"String"}]}]}],"default":"null"},{"name":"hoodieRollbackMetadata","type":["null",{"type":"record","name":"HoodieRollbackMetadata","fields":[{"name":"startRollbackTime","type":{"type":"string","avro.java.string":"String"}},{"name":"timeTakenInMillis","type":"long"},{"name":"totalFilesDeleted","type":"int"},{"name":"commitsRollback","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieRollbackPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"successDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}},{"name":"failedDeleteFiles","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"hoodieSavePointMetadata","type":["null",{"type":"record","name":"HoodieSavepointMetadata","fields":[{"name":"savepointedBy","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointedAt","type":"long"},{"name":"comments","type":{"type":"string","avro.java.string":"String"}},{"name":"partitionMetadata","type":{"type":"map","values":{"type":"record","name":"HoodieSavepointPartitionMetadata","fields":[{"name":"partitionPath","type":{"type":"string","avro.java.string":"String"}},{"name":"savepointDataFile","type":{"type":"array","items":{"type":"string","avro.java.string":"String"}}}]},"avro.java.string":"String"}}]}],"default":"null"},{"name":"commitTime","type":["null",{"type":"string","avro.java.string":"String"}]},{"name":"actionType","type":["null",{"type":"string","avro.java.string":"String"}]}]}
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:170) - Deleting instants [[100__clean], [101__clean], [102__clean], [103__clean], [100__commit], [101__commit], [102__commit], [103__commit]]
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/100.clean
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/101.clean
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/102.clean
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/103.clean
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/100.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/101.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/102.commit
[INFO ] com.uber.hoodie.io.HoodieCommitArchiveLog.deleteArchivedInstants(HoodieCommitArchiveLog.java:179) - Archived and deleted instant file /tmp/junit3983637226067236720/.hoodie/103.commit
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[==>101__clean], [104__commit], [104__clean], [105__commit], [105__clean], [==>106__clean], [==>107__clean]]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[==>101__clean], [104__commit], [104__clean], [105__commit], [105__clean], [==>106__clean], [==>107__clean]]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit170093134175933351 as hoodie dataset /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit170093134175933351/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type MERGE_ON_READ from /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 100
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit170093134175933351/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit170093134175933351
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>100__deltacommit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit170093134175933351/.hoodie/100.deltacommit.inflight
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit6378136443466329196 as hoodie dataset /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit6378136443466329196/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type MERGE_ON_READ from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit6378136443466329196 as hoodie dataset /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit6378136443466329196/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit6378136443466329196/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit6378136443466329196
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3881700229947510668 as hoodie dataset /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3881700229947510668/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type MERGE_ON_READ from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3881700229947510668/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 20200319201253
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3881700229947510668/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit3881700229947510668
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>20200319201253__deltacommit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit3881700229947510668/.hoodie/20200319201253.deltacommit.inflight
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2235291648709587919 as hoodie dataset /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2235291648709587919/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2235291648709587919/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2235291648709587919
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit667678217694125963 as hoodie dataset /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit667678217694125963/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit667678217694125963/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit667678217694125963
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 871063b9-7e55-435f-a4b4-80cea5056230 as we are done with all the records 3
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit2332978171249320064 as hoodie dataset /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2332978171249320064/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2332978171249320064/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file dd69b9b7-262a-457b-ac26-46b1eda07644 as we are done with all the records 3
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit2332978171249320064/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit2332978171249320064
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants [[20200319201254__commit]]
[INFO ] com.uber.hoodie.io.HoodieIOHandle.getMaxMemoryAllowedForMerge(HoodieIOHandle.java:120) - MaxMemoryInBytes allowed for compaction => 0
[INFO ] com.uber.hoodie.common.util.collection.DiskBasedMap.initFile(DiskBasedMap.java:186) - Spilling to file location /tmp/543e3f49-5f89-44ba-9abb-4ef714c7e7d9
[INFO ] com.uber.hoodie.io.HoodieMergeHandle.logExtra(HoodieMergeHandle.java:173) - Number of entries in MemoryBasedMap in ExternalSpillableMap => 0
[INFO ] com.uber.hoodie.io.HoodieMergeHandle.logExtra(HoodieMergeHandle.java:175) - Total size in bytes of MemoryBasedMap in ExternalSpillableMap => 0
[INFO ] com.uber.hoodie.io.HoodieMergeHandle.logExtra(HoodieMergeHandle.java:177) - Number of entries in DiskBasedMap in ExternalSpillableMap => 2
[INFO ] com.uber.hoodie.io.HoodieMergeHandle.logExtra(HoodieMergeHandle.java:179) - Size of file spilled to disk => 732
[INFO ] com.uber.hoodie.io.HoodieMergeHandle.init(HoodieMergeHandle.java:127) - Merging new data into oldPath /tmp/junit2332978171249320064/2016/01/31/dd69b9b7-262a-457b-ac26-46b1eda07644_0_20200319201254.parquet, as newPath /tmp/junit2332978171249320064/2016/01/31/dd69b9b7-262a-457b-ac26-46b1eda07644_0_20200319201255.parquet
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1112617352826978657 as hoodie dataset /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1112617352826978657/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1112617352826978657/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1112617352826978657
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/05/04
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7247643079691398211 as hoodie dataset /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7247643079691398211/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7247643079691398211/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7247643079691398211
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file e920224f-0fcd-4136-a961-4c89e2ec010a as we are done with all the records 10
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/02/01
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 40c3bce7-a861-4595-b96f-3b198b31bc93 as we are done with all the records 1
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 1 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 1 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 21c3ee34-14ea-440c-ad2f-ab2a4db94b53 as we are done with all the records 1
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/02/01
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 4325abc5-605a-4f93-be67-594605863338 as we are done with all the records 5
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/02/02
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file c7ba5a48-7bb5-443e-9cd9-aa7531d41b22 as we are done with all the records 1
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit7746829793417726653 as hoodie dataset /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7746829793417726653/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit7746829793417726653/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit7746829793417726653
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file e98cda01-6ca9-4104-8ba6-68c5a96d2e4d as we are done with all the records 840
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 17542d9d-887a-4096-bde1-b4e9a14ab7cf as we are done with all the records 837
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 4e70acc2-c104-4df9-ac9a-f50443122adc as we are done with all the records 323
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit3742968447968708752 as hoodie dataset /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3742968447968708752/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit3742968447968708752/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit3742968447968708752
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 0 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file e9546227-78e7-45e4-8a59-d67e81627274 as we are done with all the records 10
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/02/01
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file 7fdba935-98e8-4ec7-b81b-8c5b92e3c2de as we are done with all the records 1
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:160) - starting to buffer records
[INFO ] com.uber.hoodie.func.BufferedIterator.startBuffering(BufferedIterator.java:175) - finished buffering records
[INFO ] com.uber.hoodie.func.LazyInsertIterable.computeNext(LazyInsertIterable.java:103) - waiting for hoodie write to finish
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:85) - starting hoodie writer thread
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 1 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/01/31
[INFO ] com.uber.hoodie.io.HoodieIOHandle.cleanupTmpFilesFromCurrentCommit(HoodieIOHandle.java:94) - Deleting 1 files generated by previous failed attempts.
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file e667f225-fc02-4647-990e-641d0c6fbf33 as we are done with all the records 10
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.<init>(HoodieCreateHandle.java:80) - New InsertHandle for partition :2016/02/01
[INFO ] com.uber.hoodie.io.HoodieCreateHandle.close(HoodieCreateHandle.java:130) - Closing the file c16407b2-34d6-44f3-b9d4-efa077ec27b0 as we are done with all the records 1
[INFO ] com.uber.hoodie.func.LazyInsertIterable.lambda$computeNext$0(LazyInsertIterable.java:92) - hoodie write is done; notifying reader thread
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit1484127724335320366 as hoodie dataset /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1484127724335320366/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type COPY_ON_WRITE from /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [file:///], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [org.apache.hadoop.fs.LocalFileSystem@3fe66350]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit1484127724335320366/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type COPY_ON_WRITE from /tmp/junit1484127724335320366
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:215) - Initializing /tmp/junit4710742046013839900 as hoodie dataset /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:46031], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, dfs.namenode.num.extra.edits.retained=1000000, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, dfs.permissions.superusergroup=supergroup, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, dfs.datanode.slow.io.warning.threshold.ms=300, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, fs.s3a.paging.maximum=5000, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, mapreduce.task.timeout=600000, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, nfs.dump.dir=/tmp/.hdfs-nfs, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, hadoop.proxyuser.dginelli.hosts=*, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, dfs.namenode.safemode.min.datanodes=0, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, nfs.exports.allowed.hosts=* rw, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hadoop.security.group.mapping.ldap.search.attr.member=member, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/tmp/1584645178362-0/dfs/namesecondary1,file:/tmp/1584645178362-0/dfs/namesecondary2, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=127.0.0.1:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, mapreduce.map.output.compress=false, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:46031, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , fs.defaultFS=hdfs://localhost:46031, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, mapreduce.job.acl-view-job= , dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/tmp/1584645178362-0/dfs/name1,file:/tmp/1584645178362-0/dfs/name2, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, mapreduce.job.emit-timeline-data=false, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hadoop.tmp.dir=/tmp/hadoop-${user.name}, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=127.0.0.1:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=127.0.0.1:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, dfs.namenode.http-address=localhost:34981, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, ha.failover-controller.cli-check.rpc-timeout.ms=20000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, hdfs.minidfs.basedir=/tmp/1584645178362-0/dfs, fs.trash.interval=0, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, hadoop.proxyuser.dginelli.groups=*, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hadoop.security.crypto.buffer.size=8192, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, fs.s3a.threads.max=256, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, fs.s3a.attempts.maximum=20}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1101332296_3557, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:46031], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, dfs.namenode.num.extra.edits.retained=1000000, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, dfs.permissions.superusergroup=supergroup, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, dfs.datanode.slow.io.warning.threshold.ms=300, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, fs.s3a.paging.maximum=5000, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, mapreduce.task.timeout=600000, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, nfs.dump.dir=/tmp/.hdfs-nfs, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, hadoop.proxyuser.dginelli.hosts=*, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, dfs.namenode.safemode.min.datanodes=0, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, nfs.exports.allowed.hosts=* rw, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hadoop.security.group.mapping.ldap.search.attr.member=member, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/tmp/1584645178362-0/dfs/namesecondary1,file:/tmp/1584645178362-0/dfs/namesecondary2, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=127.0.0.1:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, mapreduce.map.output.compress=false, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:46031, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , fs.defaultFS=hdfs://localhost:46031, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, mapreduce.job.acl-view-job= , dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/tmp/1584645178362-0/dfs/name1,file:/tmp/1584645178362-0/dfs/name2, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, mapreduce.job.emit-timeline-data=false, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hadoop.tmp.dir=/tmp/hadoop-${user.name}, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=127.0.0.1:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=127.0.0.1:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, dfs.namenode.http-address=localhost:34981, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, ha.failover-controller.cli-check.rpc-timeout.ms=20000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, hdfs.minidfs.basedir=/tmp/1584645178362-0/dfs, fs.trash.interval=0, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, hadoop.proxyuser.dginelli.groups=*, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hadoop.security.crypto.buffer.size=8192, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, fs.s3a.threads.max=256, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, fs.s3a.attempts.maximum=20}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1101332296_3557, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit4710742046013839900/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.initializePathAsHoodieDataset(HoodieTableMetaClient.java:246) - Finished initializing Table of type MERGE_ON_READ from /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:46031], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, dfs.namenode.num.extra.edits.retained=1000000, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, dfs.permissions.superusergroup=supergroup, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, dfs.datanode.slow.io.warning.threshold.ms=300, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, fs.s3a.paging.maximum=5000, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, mapreduce.task.timeout=600000, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, nfs.dump.dir=/tmp/.hdfs-nfs, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, hadoop.proxyuser.dginelli.hosts=*, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, dfs.namenode.safemode.min.datanodes=0, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, nfs.exports.allowed.hosts=* rw, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hadoop.security.group.mapping.ldap.search.attr.member=member, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/tmp/1584645178362-0/dfs/namesecondary1,file:/tmp/1584645178362-0/dfs/namesecondary2, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=127.0.0.1:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, mapreduce.map.output.compress=false, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:46031, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , fs.defaultFS=hdfs://localhost:46031, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, mapreduce.job.acl-view-job= , dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/tmp/1584645178362-0/dfs/name1,file:/tmp/1584645178362-0/dfs/name2, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, mapreduce.job.emit-timeline-data=false, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hadoop.tmp.dir=/tmp/hadoop-${user.name}, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=127.0.0.1:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=127.0.0.1:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, dfs.namenode.http-address=localhost:34981, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, ha.failover-controller.cli-check.rpc-timeout.ms=20000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, hdfs.minidfs.basedir=/tmp/1584645178362-0/dfs, fs.trash.interval=0, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, hadoop.proxyuser.dginelli.groups=*, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hadoop.security.crypto.buffer.size=8192, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, fs.s3a.threads.max=256, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, fs.s3a.attempts.maximum=20}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1101332296_3557, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:920) - Generate a new commit time 001
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:73) - Loading HoodieTableMetaClient from /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.util.FSUtils.getFs(FSUtils.java:87) - Hadoop Configuration: fs.defaultFS: [hdfs://localhost:46031], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, {dfs.journalnode.rpc-address=0.0.0.0:8485, yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=false, dfs.client.mmap.cache.size=256, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=file://${hadoop.tmp.dir}/dfs/data, mapreduce.jobhistory.max-age-ms=604800000, dfs.namenode.lazypersist.file.scrub.interval.sec=300, mapreduce.job.ubertask.enable=false, dfs.namenode.delegation.token.renew-interval=86400000, yarn.nodemanager.log-aggregation.compression-type=none, dfs.namenode.replication.considerLoad=true, mapreduce.job.complete.cancel.delegation.tokens=true, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, dfs.namenode.retrycache.heap.percent=0.03f, dfs.namenode.top.window.num.buckets=10, yarn.resourcemanager.scheduler.address=${yarn.resourcemanager.hostname}:8030, fs.s3a.fast.buffer.size=1048576, dfs.client.file-block-storage-locations.num-threads=10, yarn.resourcemanager.proxy-user-privileges.enabled=false, dfs.datanode.balance.bandwidthPerSec=1048576, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, dfs.secondary.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.nodemanager.delete.debug-delay-sec=0, dfs.client.read.shortcircuit.streams.cache.size=256, dfs.image.transfer.bandwidthPerSec=0, yarn.scheduler.maximum-allocation-vcores=4, dfs.namenode.service.handler.count=10, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, dfs.namenode.fs-limits.min-block-size=1048576, ftp.stream-buffer-size=4096, dfs.client.use.legacy.blockreader.local=false, dfs.short.circuit.shared.memory.watcher.interrupt.check.ms=60000, dfs.datanode.directoryscan.threads=1, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, dfs.namenode.edits.noeditlogchannelflush=false, s3native.bytes-per-checksum=512, yarn.client.failover-retries-on-socket-timeouts=0, dfs.namenode.startup.delay.block.deletion.sec=0, dfs.webhdfs.user.provider.user.pattern=^[A-Za-z_][A-Za-z0-9._-]*[$]?$, mapreduce.tasktracker.tasks.sleeptimebeforesigkill=5000, yarn.timeline-service.client.retry-interval-ms=1000, dfs.encrypt.data.transfer.cipher.key.bitlength=128, hadoop.http.authentication.type=simple, dfs.namenode.path.based.cache.refresh.interval.ms=30000, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, dfs.namenode.max.full.block.report.leases=6, dfs.datanode.cache.revocation.timeout.ms=900000, ipc.client.connection.maxidletime=10000, dfs.namenode.safemode.threshold-pct=0.999f, fs.s3a.multipart.purge.age=86400, dfs.namenode.num.checkpoints.retained=2, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.ubertask.maxmaps=9, dfs.namenode.stale.datanode.interval=30000, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.tasktracker.http.address=0.0.0.0:50060, mapreduce.ifile.readahead.bytes=4194304, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, dfs.block.access.token.lifetime=600, yarn.app.mapreduce.am.resource.cpu-vcores=1, mapreduce.input.lineinputformat.linespermap=1, dfs.namenode.num.extra.edits.retained=1000000, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, dfs.client.context=default, mapreduce.map.maxattempts=4, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.datanode.drop.cache.behind.reads=false, dfs.permissions.superusergroup=supergroup, fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:mapred@hdfs@, dfs.namenode.list.cache.pools.num.responses=100, dfs.datanode.slow.io.warning.threshold.ms=300, dfs.namenode.fs-limits.max-blocks-per-file=1048576, yarn.nodemanager.vmem-check-enabled=false, hadoop.security.authentication=simple, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, fs.s3.sleepTimeSeconds=10, yarn.timeline-service.ttl-ms=604800000, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, mapreduce.jobtracker.heartbeats.in.second=100, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hadoop.ssl.require.client.cert=false, dfs.journalnode.http-address=0.0.0.0:8480, mapreduce.output.fileoutputformat.compress=false, dfs.ha.automatic-failover.enabled=false, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, dfs.namenode.invalidate.work.pct.per.iteration=0.32f, s3native.client-write-packet-size=65536, dfs.client.block.write.replace-datanode-on-failure.policy=DEFAULT, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME, dfs.namenode.acls.enabled=false, dfs.namenode.secondary.http-address=0.0.0.0:50090, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, mapreduce.task.tmp.dir=./tmp, yarn.nodemanager.linux-container-executor.cgroups.mount=false, mapreduce.tasktracker.http.threads=40, mapreduce.jobhistory.http.policy=HTTP_ONLY, fs.s3a.paging.maximum=5000, fs.s3.buffer.dir=${hadoop.tmp.dir}/s3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hadoop.registry.zk.retry.interval.ms=1000, fs.s3a.threads.core=15, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, dfs.namenode.avoid.write.stale.datanode=false, dfs.namenode.checkpoint.txns=1000000, hadoop.ssl.hostname.verifier=DEFAULT, mapreduce.task.timeout=600000, yarn.nodemanager.disk-health-checker.interval-ms=120000, dfs.journalnode.https-address=0.0.0.0:8481, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, dfs.datanode.sync.behind.writes=false, dfs.namenode.full.block.report.lease.length.ms=300000, rpc.engine.org.apache.hadoop.tracing.TraceAdminProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, dfs.client.https.keystore.resource=ssl-client.xml, dfs.namenode.list.encryption.zones.num.responses=100, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, mapreduce.jobtracker.retiredjobs.cache.size=1000, dfs.ha.tail-edits.period=60, dfs.datanode.drop.cache.behind.writes=false, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, nfs.server.port=2049, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, mapreduce.task.profile.reduces=0-2, yarn.timeline-service.client.max-retries=30, yarn.resourcemanager.am.max-attempts=2, nfs.dump.dir=/tmp/.hdfs-nfs, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=104857600, hadoop.proxyuser.dginelli.hosts=*, yarn.app.mapreduce.am.command-opts=-Xmx1024m, yarn.nodemanager.process-kill-wait.ms=2000, dfs.namenode.safemode.min.datanodes=0, mapreduce.job.speculative.minimum-allowed-tasks=10, dfs.namenode.write.stale.datanode.ratio=0.5f, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, mapreduce.tasktracker.dns.nameserver=default, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, dfs.webhdfs.enabled=true, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=4, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=5, fs.automatic.close=true, mapreduce.task.profile=false, dfs.namenode.edit.log.autoroll.multiplier.threshold=2.0, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=0.75f, dfs.namenode.backup.address=0.0.0.0:50100, dfs.client.https.need-auth=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, dfs.user.home.dir.prefix=/user, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, dfs.namenode.xattrs.enabled=true, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=600000, mapreduce.jobtracker.restart.recover=false, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.image.transfer.chunksize=65536, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, dfs.namenode.name.dir.restore=false, dfs.namenode.resource.checked.volumes.minimum=1, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.namenode.list.cache.directives.num.responses=100, dfs.image.transfer-bootstrap-standby.bandwidthPerSec=0, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, dfs.datanode.dns.nameserver=default, yarn.nodemanager.resource.memory-mb=8192, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec,org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.nodemanager.container-monitor.interval-ms=3000, dfs.replication.max=512, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=8, mapreduce.jobhistory.recovery.enable=false, nfs.exports.allowed.hosts=* rw, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, mapreduce.job.jvm.numtasks=1, dfs.datanode.fsdatasetcache.max.threads.per.volume=4, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, hadoop.fuse.timer.period=5, mapreduce.job.reduces=1, yarn.app.mapreduce.am.job.task.listener.thread-count=30, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=2, fs.permissions.umask-mode=022, mapreduce.cluster.local.dir=${hadoop.tmp.dir}/mapred/local, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, dfs.client.failover.connection.retries.on.timeouts=0, mapreduce.jobtracker.instrumentation=org.apache.hadoop.mapred.JobTrackerMetricsInst, ftp.replication=3, hadoop.security.group.mapping.ldap.search.attr.member=member, fs.s3a.max.total.tasks=1000, dfs.namenode.replication.work.multiplier.per.iteration=2, yarn.resourcemanager.resource-tracker.address=${yarn.resourcemanager.hostname}:8031, mapreduce.tasktracker.outofband.heartbeat=false, dfs.namenode.edits.dir=${dfs.namenode.name.dir}, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, dfs.client.read.shortcircuit.streams.cache.expiry.ms=300000, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, mapreduce.tasktracker.healthchecker.script.timeout=600000, hadoop.shell.missing.defaultFs.warning=true, dfs.namenode.fs-limits.max-directory-items=1048576, mapreduce.tasktracker.taskcontroller=org.apache.hadoop.mapred.DefaultTaskController, dfs.namenode.path.based.cache.block.map.allocation.percent=0.25, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, dfs.namenode.checkpoint.dir=file:/tmp/1584645178362-0/dfs/namesecondary1,file:/tmp/1584645178362-0/dfs/namesecondary2, yarn.nodemanager.remote-app-log-dir=/tmp/logs, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, dfs.client.block.write.replace-datanode-on-failure.enable=true, dfs.namenode.replication.interval=3, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, dfs.client.socket.send.buffer.size=131072, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:0, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, dfs.datanode.ipc.address=127.0.0.1:50020, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, dfs.namenode.kerberos.principal.pattern=*, yarn.timeline-service.enabled=false, dfs.client.cached.conn.retry=3, dfs.namenode.backup.http-address=0.0.0.0:50105, mapreduce.tasktracker.report.address=127.0.0.1:0, dfs.namenode.checkpoint.period=3600, mapreduce.job.heap.memory-mb.ratio=0.8, dfs.datanode.shared.file.descriptor.paths=/dev/shm,/tmp, dfs.http.policy=HTTP_ONLY, hadoop.security.groups.cache.warn.after.ms=5000, dfs.datanode.directoryscan.throttle.limit.ms.per.sec=1000, dfs.namenode.fs-limits.max-xattrs-per-inode=32, yarn.resourcemanager.zk-acl=world:anyone:rwcda, dfs.datanode.transfer.socket.send.buffer.size=131072, dfs.namenode.support.allow.format=true, dfs.namenode.checkpoint.max-retries=3, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, dfs.namenode.decommission.nodes.per.interval=5, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, dfs.datanode.cache.revocation.polling.ms=500, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, mapreduce.map.output.compress=false, mapreduce.jobhistory.cleaner.enable=true, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, dfs.blockreport.split.threshold=1000000, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, yarn.nodemanager.logaggregation.threadpool-size-max=100, dfs.datanode.scan.period.hours=504, dfs.namenode.rpc-address=localhost:46031, ipc.client.connect.max.retries=10, io.seqfile.lazydecompress=true, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, io.file.buffer.size=4096, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, ha.zookeeper.parent-znode=/hadoop-ha, mapreduce.tasktracker.indexcache.mb=10, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.acl.enable=false, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.job.token.tracking.ids.enabled=false, dfs.datanode.block-pinning.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, s3.replication=3, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, yarn.timeline-service.http-authentication.type=simple, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, dfs.ha.zkfc.nn.http.timeout.ms=20000, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, yarn.timeline-service.handler-thread-count=10, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, dfs.namenode.reject-unresolved-dn-topology-mapping=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, dfs.client.slow.io.warning.threshold.ms=30000, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, mapreduce.tasktracker.dns.interface=default, mapreduce.jobtracker.handler.count=10, dfs.blockreport.initialDelay=0, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, dfs.namenode.top.enabled=true, dfs.namenode.retrycache.expirytime.millis=600000, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, dfs.client.failover.sleep.max.millis=15000, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, dfs.namenode.blocks.per.postponedblocks.rescan=10000, yarn.resourcemanager.max-completed-applications=10000, yarn.nodemanager.log-dirs=${yarn.log.dir}/userlogs, dfs.client.failover.sleep.base.millis=500, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.default.chunk.view.size=32768, dfs.client.read.shortcircuit=false, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , fs.defaultFS=hdfs://localhost:46031, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, yarn.resourcemanager.connect.max-wait.ms=900000, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.max.extra.edits.segments.retained=10000, dfs.namenode.https-address=0.0.0.0:50470, dfs.block.scanner.volume.bytes.per.second=1048576, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, ipc.client.kill.max=10, rpc.engine.org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=900000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, dfs.namenode.path.based.cache.retry.interval.ms=30000, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, hadoop.registry.zk.session.timeout.ms=60000, yarn.nodemanager.local-cache.max-files-per-directory=8192, dfs.https.server.keystore.resource=ssl-server.xml, mapreduce.jobtracker.taskcache.levels=2, dfs.webhdfs.ugi.expire.after.access=600000, dfs.datanode.handler.count=10, s3native.blocksize=67108864, mapreduce.client.completion.pollinterval=5000, rpc.engine.org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.stream-buffer-size=4096, dfs.namenode.delegation.key.update-interval=86400000, mapreduce.job.maps=2, mapreduce.job.acl-view-job= , dfs.namenode.enable.retrycache=true, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, dfs.namenode.decommission.interval=3, mapreduce.shuffle.max.connections=0, yarn.log-aggregation-enable=false, dfs.client-write-packet-size=65536, dfs.client.file-block-storage-locations.timeout.millis=1000, mapreduce.jobtracker.expire.trackers.interval=600000, dfs.client.block.write.retries=3, mapreduce.task.io.sort.factor=10, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=5000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, dfs.datanode.transfer.socket.recv.buffer.size=131072, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, yarn.resourcemanager.scheduler.client.thread-count=50, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.safemode.extension=0, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, ipc.client.idlethreshold=4000, dfs.namenode.accesstime.precision=3600000, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, dfs.datanode.hdfs-blocks-metadata.enabled=false, yarn.scheduler.minimum-allocation-mb=1024, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, fs.s3a.connection.ssl.enabled=true, dfs.datanode.directoryscan.interval=21600, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, mapreduce.jobtracker.persist.jobstatus.dir=/jobtracker/jobsInfo, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=false, dfs.namenode.name.dir=file:/tmp/1584645178362-0/dfs/name1,file:/tmp/1584645178362-0/dfs/name2, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, ftp.bytes-per-checksum=512, dfs.namenode.max.objects=0, hadoop.http.logs.enabled=true, mapreduce.job.emit-timeline-data=false, mapreduce.map.memory.mb=-1, yarn.client.nodemanager-connect.retry-interval-ms=10000, dfs.namenode.edits.journal-plugin.qjournal=org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager, mapreduce.tasktracker.healthchecker.interval=60000, nfs.wtmax=1048576, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, dfs.namenode.edekcacheloader.initial.delay.ms=3000, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=${yarn.resourcemanager.hostname}:8032, ipc.client.rpc-timeout.ms=0, dfs.cachereport.intervalMsec=10000, mapreduce.task.skip.start.attempts=2, yarn.resourcemanager.zk-timeout-ms=10000, dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}, hadoop.hdfs.configuration.version=1, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, nfs.allow.insecure.ports=true, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, yarn.timeline-service.hostname=0.0.0.0, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=2.1, yarn.nodemanager.container-manager.thread-count=20, dfs.encrypt.data.transfer=false, dfs.block.access.key.update.interval=600, hadoop.tmp.dir=/tmp/hadoop-${user.name}, dfs.namenode.audit.loggers=default, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.http.policy=HTTP_ONLY, dfs.client.short.circuit.replica.stale.threshold.ms=1800000, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, mapreduce.jobtracker.persist.jobstatus.hours=1, tfile.fs.output.buffer.size=262144, dfs.namenode.checkpoint.check.period=60, dfs.datanode.dns.interface=default, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=100, dfs.namenode.inotify.max.events.per.rpc=1000, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, dfs.namenode.avoid.read.stale.datanode=false, mapreduce.output.fileoutputformat.compress.type=RECORD, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=24, dfs.datanode.http.address=127.0.0.1:50075, dfs.image.compress=false, ha.health-monitor.check-interval.ms=1000, dfs.permissions.enabled=true, yarn.resourcemanager.resource-tracker.client.thread-count=50, dfs.client.domain.socket.data.traffic=false, dfs.image.compression.codec=org.apache.hadoop.io.compress.DefaultCodec, dfs.datanode.address=127.0.0.1:50010, dfs.block.access.token.enable=false, mapreduce.reduce.input.buffer.percent=0.0, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, mapreduce.tasktracker.local.dir.minspacestart=0, dfs.blockreport.intervalMsec=21600000, ha.health-monitor.rpc-timeout.ms=45000, dfs.datanode.bp-ready.timeout=20, dfs.client.failover.connection.retries=0, dfs.namenode.kerberos.internal.spnego.principal=${dfs.web.authentication.kerberos.principal}, yarn.scheduler.maximum-allocation-mb=8192, mapreduce.task.files.preserve.failedtasks=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, rpc.engine.org.apache.hadoop.ha.protocolPB.HAServiceProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, mapreduce.jobtracker.tasktracker.maxblacklists=4, io.seqfile.compress.blocksize=1000000, dfs.blocksize=134217728, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, mapreduce.jobtracker.http.address=0.0.0.0:50030, dfs.client.mmap.cache.timeout.ms=3600000, dfs.namenode.edekcacheloader.interval.ms=1000, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, mapreduce.tasktracker.local.dir.minspacekill=0, nfs.mountd.port=4242, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, mapreduce.tasktracker.taskmemorymanager.monitoringinterval=5000, dfs.namenode.resource.du.reserved=104857600, mapreduce.job.end-notification.retry.interval=1000, dfs.data.transfer.server.tcpnodelay=true, mapreduce.jobhistory.loadedjobs.cache.size=5, dfs.client.datanode-restart.timeout=30, yarn.nodemanager.local-dirs=${hadoop.tmp.dir}/nm-local-dir, dfs.datanode.block.id.layout.upgrade.threads=12, mapreduce.task.exit.timeout.check-interval-ms=20000, hadoop.registry.jaas.context=Client, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, mapreduce.jobhistory.address=0.0.0.0:10020, mapreduce.jobtracker.persist.jobstatus.active=true, file.blocksize=67108864, dfs.datanode.readahead.bytes=4193404, dfs.namenode.http-address=localhost:34981, ipc.client.ping=true, hadoop.work.around.non.threadsafe.getpwuid=false, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=false, yarn.resourcemanager.hostname=0.0.0.0, yarn.am.blacklisting.enabled=true, fs.s3n.multipart.uploads.enabled=false, dfs.namenode.startup=REGULAR, dfs.namenode.fs-limits.max-component-length=255, ha.failover-controller.cli-check.rpc-timeout.ms=20000, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=5, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=-1, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, dfs.namenode.secondary.https-address=0.0.0.0:50091, mapreduce.jobhistory.jhist.format=json, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, dfs.namenode.fs-limits.max-xattr-size=16384, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, mapreduce.jobtracker.jobhistory.task.numberprogresssplits=12, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.jobhistory.client.thread-count=10, hdfs.minidfs.basedir=/tmp/1584645178362-0/dfs, fs.trash.interval=0, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, dfs.namenode.top.windows.minutes=1,5,25, mapreduce.reduce.memory.mb=-1, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=0, hadoop.proxyuser.dginelli.groups=*, dfs.namenode.resource.check.interval=5000, mapreduce.client.progressmonitor.pollinterval=1000, yarn.nodemanager.default-container-executor.log-dirs.permissions=710, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, dfs.ha.log-roll.period=120, yarn.scheduler.minimum-allocation-vcores=1, dfs.client.block.write.replace-datanode-on-failure.best-effort=false, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, dfs.storage.policy.enabled=true, dfs.datanode.https.address=0.0.0.0:50475, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, dfs.namenode.replication.min=1, mapreduce.shuffle.connection-keep-alive.enable=false, dfs.namenode.top.num.users=10, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, dfs.datanode.max.locked.memory=0, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, dfs.image.transfer.timeout=60000, dfs.client.read.shortcircuit.skip.checksum=false, nfs.rtmax=1048576, dfs.namenode.edit.log.autoroll.check.interval.ms=300000, mapreduce.reduce.shuffle.connect.timeout=180000, dfs.datanode.failed.volumes.tolerated=0, mapreduce.jobhistory.webapp.address=0.0.0.0:19888, fs.s3a.connection.timeout=200000, dfs.client.mmap.retry.timeout.ms=300000, dfs.datanode.data.dir.perm=700, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=45, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, yarn.admin.acl=*, dfs.namenode.delegation.token.max-lifetime=604800000, mapreduce.reduce.merge.inmem.threshold=1000, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, dfs.datanode.use.datanode.hostname=false, dfs.heartbeat.interval=3, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler, io.map.index.skip=0, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, dfs.namenode.handler.count=10, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, mapreduce.jobtracker.jobhistory.block.size=3145728, hadoop.security.crypto.buffer.size=8192, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, mapreduce.cluster.acls.enabled=false, fs.s3a.threads.max=256, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=2, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, mapreduce.jobtracker.jobhistory.lru.cache.size=5, dfs.client.failover.max.attempts=15, dfs.client.use.datanode.hostname=false, ha.zookeeper.acl=world:anyone:rwcda, mapreduce.jobtracker.maxtasks.perjob=-1, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.am.blacklisting.disable-failure-threshold=0.8f, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, yarn.resourcemanager.nodemanager.minimum.version=NONE, hadoop.fuse.connection.timeout=300, mapreduce.tasktracker.instrumentation=org.apache.hadoop.mapred.TaskTrackerMetricsInst, io.seqfile.sorter.recordlimit=1000000, yarn.app.mapreduce.am.resource.mb=1536, mapreduce.framework.name=local, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=50, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, dfs.client.mmap.enabled=true, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, fs.s3a.attempts.maximum=20}], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1101332296_3557, ugi=dginelli (auth:SIMPLE)]]]
[INFO ] com.uber.hoodie.common.table.HoodieTableConfig.<init>(HoodieTableConfig.java:67) - Loading dataset properties from /tmp/junit4710742046013839900/.hoodie/hoodie.properties
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:83) - Finished Loading Table of type MERGE_ON_READ from /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.table.HoodieTableMetaClient.<init>(HoodieTableMetaClient.java:85) - Loading Active commit timeline for /tmp/junit4710742046013839900
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.<init>(HoodieActiveTimeline.java:82) - Loaded instants []
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createInflight(HoodieActiveTimeline.java:192) - Creating a new in-flight instant [==>001__deltacommit]
[INFO ] com.uber.hoodie.common.table.timeline.HoodieActiveTimeline.createFileInMetaPath(HoodieActiveTimeline.java:275) - Created a new file in meta path: /tmp/junit4710742046013839900/.hoodie/001.deltacommit.inflight
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.calculateSuspicious(AstorCoreEngine.java:925) - Setting up the max to 10310580 milliseconds (10310 sec)
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.initPopulation(AstorCoreEngine.java:720) - 
---- Creating spoon model
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildSpoonModel(MutationSupporter.java:236) - Creating model,  Code location from working folder: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/main/java
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildModel(MutationSupporter.java:67) - building model: /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client/src/main/java, compliance level: 8
[INFO ] fr.inria.astor.core.manipulation.MutationSupporter.buildModel(MutationSupporter.java:81) - Classpath (Dependencies) for building SpoonModel: [/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro/1.7.6-cdh5.7.2/avro-1.7.6-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/common/objectsize/0.0.12/objectsize-0.0.12.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm/3.1/asm-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.2/hadoop-annotations-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/activation/activation/1.1/activation-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpcore/4.3.2/httpcore-4.3.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.2/hadoop-auth-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.2/zookeeper-3.4.5-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/tukaani/xz/1.0/xz-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-graphite/3.1.1/metrics-graphite-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-core/3.1.1/metrics-core-3.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/beust/jcommander/1.48/jcommander-1.48.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.2/hadoop-client-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.2/hadoop-mapreduce-client-app-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.2/hadoop-mapreduce-client-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.2/hadoop-yarn-client-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.2/hadoop-yarn-server-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.2/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.2/hadoop-yarn-api-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.2/hadoop-mapreduce-client-core-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.2/hadoop-yarn-common-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.2/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.2/hadoop-aws-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-avro/1.8.1/parquet-avro-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-column/1.8.1/parquet-column-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-common/1.8.1/parquet-common-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-encoding/1.8.1/parquet-encoding-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-format/2.3.0-incubating/parquet-format-2.3.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/it/unimi/dsi/fastutil/6.5.7/fastutil-6.5.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-hadoop/1.8.1/parquet-hadoop-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-jackson/1.8.1/parquet-jackson-1.8.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/xerial/snappy/snappy-java/1.1.1.6/snappy-java-1.1.1.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/guava/guava/15.0/guava-15.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-core_2.11/2.1.0/spark-core_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/objenesis/objenesis/2.1/objenesis-2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-launcher_2.11/2.1.0/spark-launcher_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-common_2.11/2.1.0/spark-network-common_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-shuffle_2.11/2.1.0/spark-network-shuffle_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-unsafe_2.11/2.1.0/spark-unsafe_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/oro/oro/2.0.8/oro-2.0.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-tags_2.11/2.1.0/spark-tags_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sql_2.11/2.1.0/spark-sql_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sketch_2.11/2.1.0/spark-sketch_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-catalyst_2.11/2.1.0/spark-catalyst_2.11-2.1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-client/1.2.3/hbase-client-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-protocol/1.2.3/hbase-protocol-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mockito/mockito-all/1.10.19/mockito-all-1.10.19.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-hadoop-mr/0.4.2-SNAPSHOT/hoodie-hadoop-mr-0.4.2-SNAPSHOT.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-jdbc/1.1.0-cdh5.7.2/hive-jdbc-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-service/1.1.0-cdh5.7.2/hive-service-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/jpam/jpam/1.1/jpam-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/mail/mail/1.4.1/mail-1.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-commons/3.1/asm-commons-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-tree/3.1/asm-tree-3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-serde/1.1.0-cdh5.7.2/hive-serde-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-hadoop-bundle/1.5.0-cdh5.7.2/parquet-hadoop-bundle-1.5.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-exec/1.1.0-cdh5.7.2/hive-exec-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-ant/1.1.0-cdh5.7.2/hive-ant-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/velocity/velocity/1.5/velocity-1.5.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-metastore/1.1.0-cdh5.7.2/hive-metastore-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/derby/derby/10.11.1.1/derby-10.11.1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/transaction/jta/1.1/jta-1.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-shims/1.1.0-cdh5.7.2/hive-shims-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-common/1.1.0-cdh5.7.2/hive-shims-common-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-0.23/1.1.0-cdh5.7.2/hive-shims-0.23-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0-cdh5.7.2/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/guice/3.0/guice-3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/inject/javax.inject/1/javax.inject-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0-cdh5.7.2/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0-cdh5.7.2/hadoop-yarn-server-web-proxy-2.6.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-scheduler/1.1.0-cdh5.7.2/hive-shims-scheduler-1.1.0-cdh5.7.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/cloudera/logredactor/logredactor/1.0.3/logredactor-1.0.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/antlr/antlr/2.7.7/antlr-2.7.7.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant/1.9.1/ant-1.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-core/1.0.0-incubating/calcite-core-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-linq4j/1.0.0-incubating/calcite-linq4j-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-avatica/1.0.0-incubating/calcite-avatica-1.0.0-incubating.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/stax/stax-api/1.0.1/stax-api-1.0.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/jline/jline/2.12/jline-2.12.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-testing-util/1.2.3/hbase-testing-util-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-procedure/1.2.3/hbase-procedure-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-prefix-tree/1.2.3/hbase-prefix-tree-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math/2.2/commons-math-2.2.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-minicluster/2.5.1/hadoop-minicluster-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.5.1/hadoop-yarn-server-tests-2.5.1-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.5.1/hadoop-yarn-server-nodemanager-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.1/hadoop-mapreduce-client-jobclient-2.5.1-tests.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.5.1/hadoop-mapreduce-client-hs-2.5.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/junit/junit/4.11/junit-4.11.jar, /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar]
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.initModel(AstorCoreEngine.java:790) - Number of CtTypes created: 62
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.initPopulation(AstorCoreEngine.java:724) - 
---- Initial suspicious size: 507
[INFO ] fr.inria.astor.core.solutionsearch.population.ProgramVariantFactory.createProgramInstance(ProgramVariantFactory.java:134) - Total suspicious from FL: 507,  454
[INFO ] fr.inria.astor.core.solutionsearch.population.ProgramVariantFactory.createProgramInstance(ProgramVariantFactory.java:143) - Total ModPoint created: 454
[INFO ] fr.inria.astor.core.solutionsearch.population.ProgramVariantFactory.createInitialPopulation(ProgramVariantFactory.java:82) - Creating program variant #1, [Variant id: 1, #gens: 454, #ops: 0, parent:-]
[INFO ] fr.inria.astor.core.solutionsearch.AstorCoreEngine.setFitnessOfPopulation(AstorCoreEngine.java:765) - The original fitness is : 1.0
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:66) - ----------------------------
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:67) - ---Configuration properties:---Execution values
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:lastJUnitVersion= ./examples/libs/junit-4.11.jar
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:alternativecompliancelevel= 8
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:ignoredTestCases= 
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:workingDirectory= /proj/nobackup/snic2020-10-10/dginelli/astor
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:manipulatesuper= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:validation= process
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:jvm4testexecution= /cvmfs/ebsw.hpc2n.umu.se/amd64_ubuntu1604_common/software/Core/Java/1.8.0_202/bin
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:operatorspace= suppression
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:disablelog= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:binjavafolder= /target/classes
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:synthesis_depth= 3
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:gzoltartestpackagetoexclude= junit.framework
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:skipfitnessinitialpopulation= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evosuiteresultfolder= evosuite
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:flthreshold= 0.1
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:regressionforfaultlocalization= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:collectonlyusedmethod= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:preservelinenumbers= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:tmax2= 10310580
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:tmax1= 10000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:probagenmutation= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:nomodificationconvergence= 100
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:diff_type= relative
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:targetelementprocessor= statements
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:srctestfolder= src/test/java
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:forceExecuteRegression= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:clusteringfilename= clustering.csv
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:logtestexecution= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evo_buggy_class= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:numberExecutions= 1
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxnumbersolutions= 3
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evo_affected_by_op= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:population= 1
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:considerzerovaluesusp= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxCombinationVariableLimit= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:loglevel= INFO
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:savesolution= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:javacompliancelevel= 8
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:uniqueoptogen= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:gzoltarpackagetonotinstrument= junit.framework
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:projectIdentifier= AstorJKali-repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:modificationpointnavigation= weight
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:stopfirst= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:multipointmodification= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:elementsToMutate= 10
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:jsonoutputname= astor_output
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evoDSE= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:bintestfolder= /target/test-classes
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:testbystep= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:version-location= ./math-version/
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:reintroduce= PARENTS:ORIGINAL
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:executorjar= ./lib/jtestex7.jar
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxGeneration= 200
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:max_synthesis_step= 10000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:projectinfocommand= com.github.tdurieux:project-config-maven-plugin:info
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxVarCombination= 1000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxtime= 100
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evosuitejar= ./lib/evosuite-master-1.0.4-SNAPSHOT.jar
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:jvmversion= 1.8.0_202
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:commandTrunk= 50000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:faultlocalization= CoCoSpoon
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:resetmodel= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxsuspcandidates= 1000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:mode= jkali
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:learningdir= 
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:jvm4evosuitetestexecution= /cvmfs/ebsw.hpc2n.umu.se/amd64_ubuntu1604_common/software/Core/Java/1.8.0_202/bin
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:filterfaultlocalization= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:mutationrate= 1 
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:pathToMVNRepository= 
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:resourcesfolder= /src/main/resources:/src/test/resources:
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:runjava7code= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:timezone= Europe/Paris
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evoRunOnBuggyClass= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:compiler= fr.inria.astor.core.manipulation.bytecode.compiler.SpoonClassCompiler
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:limitbysuspicious= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:logsattemps= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:dependenciespath= /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro/1.7.6-cdh5.7.2/avro-1.7.6-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/common/objectsize/0.0.12/objectsize-0.0.12.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm/3.1/asm-3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-io/commons-io/2.4/commons-io-2.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty/3.6.2.Final/netty-3.6.2.Final.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/netty/netty-all/4.0.23.Final/netty-all-4.0.23.Final.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/xml-apis/xml-apis/1.3.04/xml-apis-1.3.04.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core4/4.0.1-incubating/htrace-core4-4.0.1-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-annotations/2.6.0-cdh5.7.2/hadoop-annotations-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-net/commons-net/3.1/commons-net-3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/activation/activation/1.1/activation-1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-el/commons-el/1.0/commons-el-1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/httpcomponents/httpcore/4.3.2/httpcore-4.3.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-auth/2.6.0-cdh5.7.2/hadoop-auth-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-client/2.7.1/curator-client-2.7.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-recipes/2.7.1/curator-recipes-2.7.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/zookeeper/zookeeper/3.4.5-cdh5.7.2/zookeeper-3.4.5-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/tukaani/xz/1.0/xz-1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-common/0.4.2-SNAPSHOT/hoodie-common-0.4.2-SNAPSHOT-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-graphite/3.1.1/metrics-graphite-3.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-core/3.1.1/metrics-core-3.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/beust/jcommander/1.48/jcommander-1.48.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/log4j/1.2.17/log4j-1.2.17.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-client/2.6.0-cdh5.7.2/hadoop-client-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-common/2.6.0-cdh5.7.2/hadoop-common-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-hdfs/2.6.0-cdh5.7.2/hadoop-hdfs-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-app/2.6.0-cdh5.7.2/hadoop-mapreduce-client-app-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-common/2.6.0-cdh5.7.2/hadoop-mapreduce-client-common-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-client/2.6.0-cdh5.7.2/hadoop-yarn-client-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-common/2.6.0-cdh5.7.2/hadoop-yarn-server-common-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.6.0-cdh5.7.2/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-api/2.6.0-cdh5.7.2/hadoop-yarn-api-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-core/2.6.0-cdh5.7.2/hadoop-mapreduce-client-core-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-common/2.6.0-cdh5.7.2/hadoop-yarn-common-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.6.0-cdh5.7.2/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-aws/2.6.0-cdh5.7.2/hadoop-aws-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-s3/1.10.6/aws-java-sdk-s3-1.10.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-kms/1.10.6/aws-java-sdk-kms-1.10.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/amazonaws/aws-java-sdk-core/1.10.6/aws-java-sdk-core-1.10.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-avro/1.8.1/parquet-avro-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-column/1.8.1/parquet-column-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-common/1.8.1/parquet-common-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-encoding/1.8.1/parquet-encoding-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-format/2.3.0-incubating/parquet-format-2.3.0-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/it/unimi/dsi/fastutil/6.5.7/fastutil-6.5.7.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-hadoop/1.8.1/parquet-hadoop-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/parquet/parquet-jackson/1.8.1/parquet-jackson-1.8.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/xerial/snappy/snappy-java/1.1.1.6/snappy-java-1.1.1.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/guava/guava/15.0/guava-15.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-core_2.11/2.1.0/spark-core_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill_2.11/0.8.0/chill_2.11-0.8.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/kryo-shaded/3.0.3/kryo-shaded-3.0.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/objenesis/objenesis/2.1/objenesis-2.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/chill-java/0.8.0/chill-java-0.8.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-launcher_2.11/2.1.0/spark-launcher_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-common_2.11/2.1.0/spark-network-common_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-network-shuffle_2.11/2.1.0/spark-network-shuffle_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-unsafe_2.11/2.1.0/spark-unsafe_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-library/2.11.8/scala-library-2.11.8.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-jackson_2.11/3.2.11/json4s-jackson_2.11-3.2.11.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-core_2.11/3.2.11/json4s-core_2.11-3.2.11.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/json4s/json4s-ast_2.11/3.2.11/json4s-ast_2.11-3.2.11.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scalap/2.11.0/scalap-2.11.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-compiler/2.11.0/scala-compiler-2.11.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.1/scala-parser-combinators_2.11-1.0.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/ws/rs/javax.ws.rs-api/2.0.1/javax.ws.rs-api-2.0.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-api/2.4.0-b34/hk2-api-2.4.0-b34.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-utils/2.4.0-b34/hk2-utils-2.4.0-b34.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/aopalliance-repackaged/2.4.0-b34/aopalliance-repackaged-2.4.0-b34.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/external/javax.inject/2.4.0-b34/javax.inject-2.4.0-b34.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/hk2-locator/2.4.0-b34/hk2-locator-2.4.0-b34.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/annotation/javax.annotation-api/1.2/javax.annotation-api-1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/bundles/repackaged/jersey-guava/2.22.2/jersey-guava-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/hk2/osgi-resource-locator/1.0.1/osgi-resource-locator-1.0.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/media/jersey-media-jaxb/2.22.2/jersey-media-jaxb-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/validation/validation-api/1.1.0.Final/validation-api-1.1.0.Final.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-databind/2.6.5/jackson-databind-2.6.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/core/jackson-core/2.6.5/jackson-core-2.6.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-scala_2.11/2.6.5/jackson-module-scala_2.11-2.6.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/fasterxml/jackson/module/jackson-module-paranamer/2.6.5/jackson-module-paranamer-2.6.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/oro/oro/2.0.8/oro-2.0.8.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/razorvine/pyrolite/4.13/pyrolite-4.13.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/py4j/py4j/0.10.4/py4j-0.10.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-tags_2.11/2.1.0/spark-tags_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scalatest/scalatest_2.11/2.2.6/scalatest_2.11-2.2.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sql_2.11/2.1.0/spark-sql_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/univocity/univocity-parsers/2.2.1/univocity-parsers-2.2.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-sketch_2.11/2.1.0/spark-sketch_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/spark/spark-catalyst_2.11/2.1.0/spark-catalyst_2.11-2.1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/janino/3.0.0/janino-3.0.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/janino/commons-compiler/3.0.0/commons-compiler-3.0.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr4-runtime/4.5.3/antlr4-runtime-4.5.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-client/1.2.3/hbase-client-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-protocol/1.2.3/hbase-protocol-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mockito/mockito-all/1.10.19/mockito-all-1.10.19.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/uber/hoodie/hoodie-hadoop-mr/0.4.2-SNAPSHOT/hoodie-hadoop-mr-0.4.2-SNAPSHOT.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-jdbc/1.1.0-cdh5.7.2/hive-jdbc-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-service/1.1.0-cdh5.7.2/hive-service-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/jpam/jpam/1.1/jpam-1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/eclipse/jetty/aggregate/jetty-all/7.6.0.v20120127/jetty-all-7.6.0.v20120127.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jta_1.1_spec/1.1.1/geronimo-jta_1.1_spec-1.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/mail/mail/1.4.1/mail-1.4.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-jaspic_1.0_spec/1.0/geronimo-jaspic_1.0_spec-1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/geronimo/specs/geronimo-annotation_1.0_spec/1.1.1/geronimo-annotation_1.0_spec-1.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-commons/3.1/asm-commons-3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/asm/asm-tree/3.1/asm-tree-3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-serde/1.1.0-cdh5.7.2/hive-serde-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/twitter/parquet-hadoop-bundle/1.5.0-cdh5.7.2/parquet-hadoop-bundle-1.5.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/jersey-servlet/1.14/jersey-servlet-1.14.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-exec/1.1.0-cdh5.7.2/hive-exec-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-ant/1.1.0-cdh5.7.2/hive-ant-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/velocity/velocity/1.5/velocity-1.5.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-metastore/1.1.0-cdh5.7.2/hive-metastore-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/derby/derby/10.11.1.1/derby-10.11.1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/transaction/jta/1.1/jta-1.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/hive-shims/1.1.0-cdh5.7.2/hive-shims-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-common/1.1.0-cdh5.7.2/hive-shims-common-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-0.23/1.1.0-cdh5.7.2/hive-shims-0.23-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.6.0-cdh5.7.2/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/guice/3.0/guice-3.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/javax/inject/javax.inject/1/javax.inject-1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.6.0-cdh5.7.2/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.6.0-cdh5.7.2/hadoop-yarn-server-web-proxy-2.6.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hive/shims/hive-shims-scheduler/1.1.0-cdh5.7.2/hive-shims-scheduler-1.1.0-cdh5.7.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/cloudera/logredactor/logredactor/1.0.3/logredactor-1.0.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/antlr/antlr/2.7.7/antlr-2.7.7.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant/1.9.1/ant-1.9.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/ant/ant-launcher/1.9.1/ant-launcher-1.9.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/curator/curator-framework/2.6.0/curator-framework-2.6.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-core/1.0.0-incubating/calcite-core-1.0.0-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-linq4j/1.0.0-incubating/calcite-linq4j-1.0.0-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/pentaho/pentaho-aggdesigner-algorithm/5.1.5-jhyde/pentaho-aggdesigner-algorithm-5.1.5-jhyde.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/eigenbase/eigenbase-properties/1.1.4/eigenbase-properties-1.1.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/calcite/calcite-avatica/1.0.0-incubating/calcite-avatica-1.0.0-incubating.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/jline/jline/2.12/jline-2.12.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-testing-util/1.2.3/hbase-testing-util-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-common/1.2.3/hbase-common-1.2.3-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-annotations/1.2.3/hbase-annotations-1.2.3-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-procedure/1.2.3/hbase-procedure-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-prefix-tree/1.2.3/hbase-prefix-tree-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/jamon/jamon-runtime/2.4.1/jamon-runtime-2.4.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-server/1.2.3/hbase-server-1.2.3-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop-compat/1.2.3/hbase-hadoop-compat-1.2.3-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hbase/hbase-hadoop2-compat/1.2.3/hbase-hadoop2-compat-1.2.3-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-minicluster/2.5.1/hadoop-minicluster-2.5.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-tests/2.5.1/hadoop-yarn-server-tests-2.5.1-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.5.1/hadoop-yarn-server-nodemanager-2.5.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.5.1/hadoop-mapreduce-client-jobclient-2.5.1-tests.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/apache/hadoop/hadoop-mapreduce-client-hs/2.5.1/hadoop-mapreduce-client-hs-2.5.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/com/google/code/gson/gson/2.3.1/gson-2.3.1.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/junit/junit/4.11/junit-4.11.jar:/scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/.m2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:logpatternlayout= [%-5p] %l - %m%n
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:packageToInstrument= 
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:skipfaultlocalization= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:scope= package
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:transformingredient= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:fitnessfunction= fr.inria.astor.core.solutionsearch.population.TestCaseFitnessFunction
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxnumvariablesperingredient= 10
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:logfilepath= /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/repairnator.astor.jkali.log
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:parsesourcefromoriginal= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:pvariantfoldername= variant-
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:savespoonmodelondisk= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:srcjavafolder= src/main/java/
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:populationcontroller= fr.inria.astor.core.solutionsearch.population.TestCaseBasedFitnessPopulationController
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:ignoreflakyinfl= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:cleantemplates= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:applyCrossover= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxmodificationpoints= 1000
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:duplicateingredientsinspace= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:metid= 0
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:continuewhenmodelfail= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:saveall= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:seed= 1
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:savecompletepatched= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:resetoperations= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:processoutputinfile= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:location= /scratch/dginelli/workspace/repairnator-repairnator-experiments-uber-hudi-354203634-20180316-092307-firstCommit/hoodie-client
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:probabilistictransformation= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:overridemaxtime= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:outputjsonresult= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:allpoints= false
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:bugId= 280
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:evosuitetimeout= 120
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:maxtimefactor= 10
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:nrPlaceholders= 1
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:69) - p:forcesubprocesskilling= true
[INFO ] fr.inria.astor.core.setup.ConfigurationProperties.print(ConfigurationProperties.java:71) - ----------------------------
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieReadClient line: 113, pointed element: CtInvocationImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtInvocationImpl) `conf.set("spark.sql.hive.convertMetastoreParquet", "false") ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.index.HoodieIndex line: 53, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.jsc = jsc ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.index.HoodieIndex line: 52, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.config = config ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.table.HoodieTable line: 59, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.config = config ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.table.HoodieTable line: 60, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.metaClient = metaClient ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.table.HoodieTable line: 217, pointed element: CtSwitchImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtSwitchImpl) `switch (metaClient.getTableType()) { 	case COPY_ON_WRITE : 		return com.uber.hoodie.common.table.tim[...] ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.table.HoodieTable line: 241, pointed element: CtSwitchImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtSwitchImpl) `switch (metaClient.getTableType()) { 	case COPY_ON_WRITE : 		return new com.uber.hoodie.table.Hoodie[...] ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 114, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.fs = com.uber.hoodie.common.util.FSUtils.getFs(clientConfig.getBasePath(), jsc.hadoopConfigurat[...] ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 120, pointed element: CtIfImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
ReplaceIfBooleanOp:(spoon.support.reflect.code.CtIfImpl) `if (rollbackInFlight) { 	rollbackInflightCommits(); } ` -topatch--> `if (true) { 	rollbackInflightCommits(); }` (spoon.support.reflect.code.CtIfImpl) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 120, pointed element: CtIfImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
ReplaceIfBooleanOp:(spoon.support.reflect.code.CtIfImpl) `if (rollbackInFlight) { 	rollbackInflightCommits(); } ` -topatch--> `if (false) { 	rollbackInflightCommits(); }` (spoon.support.reflect.code.CtIfImpl) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 120, pointed element: CtIfImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtIfImpl) `if (rollbackInFlight) { 	rollbackInflightCommits(); } ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 118, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.metrics = new com.uber.hoodie.metrics.HoodieMetrics(config, config.getTableName()) ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 117, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.index = com.uber.hoodie.index.HoodieIndex.createIndex(config, jsc) ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 116, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.config = clientConfig ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.HoodieWriteClient line: 115, pointed element: CtAssignmentImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtAssignmentImpl) `this.jsc = jsc ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.config.HoodieWriteConfig line: 399, pointed element: CtInvocationImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtInvocationImpl) `props.setProperty(com.uber.hoodie.config.HoodieWriteConfig.BASE_PATH_PROP, basePath) ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.config.HoodieWriteConfig line: 543, pointed element: CtInvocationImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtInvocationImpl) `com.uber.hoodie.config.DefaultHoodieConfig.setDefaultOnCondition(props, !isMetricsConfigSet,  com.ub[...] ` -topatch--> `-` (null) 
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:59) - mod_point MP=com.uber.hoodie.config.HoodieWriteConfig line: 541, pointed element: CtInvocationImpl
[INFO ] fr.inria.astor.core.solutionsearch.ExhaustiveSearchEngine.startEvolution(ExhaustiveSearchEngine.java:60) - -->op: OP_INSTANCE:
RemoveOp:(spoon.support.reflect.code.CtInvocationImpl) `com.uber.hoodie.config.DefaultHoodieConfig.setDefaultOnCondition(props, !isCompactionConfigSet,  com[...] ` -topatch--> `-` (null) 
